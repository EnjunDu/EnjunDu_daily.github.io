{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hexo • All posts by \"斯坦福cs231n课程\" category",
    "description": "",
    "home_page_url": "https://enjundu.github.io",
    "items": [
        {
            "id": "https://enjundu.github.io/Sanfordcs231n/README_2_Linear%20Classification/",
            "url": "https://enjundu.github.io/Sanfordcs231n/README_2_Linear%20Classification/",
            "title": "k-NN最近分类器",
            "date_published": "2024-07-04T06:02:28.000Z",
            "content_html": "<p><strong>本章阅读材料链接为<span class=\"exturl\" data-url=\"aHR0cHM6Ly9jczIzMW4uZ2l0aHViLmlvL2xpbmVhci1jbGFzc2lmeS8=\"> CS231n Convolutional Neural Networks for Visual Recognition</span></strong></p>\n<h1 id=\"线性分类\"><a class=\"markdownIt-Anchor\" href=\"#线性分类\">#</a> 线性分类</h1>\n<p>​\t在上一节中，我们介绍了图像分类问题，即从一组固定类别中为图像分配单个标签的任务。此外，我们描述了 k 最近邻 (kNN) 分类器，它通过将图像与训练集中的（带注释的）图像进行比较来标记图像。正如我们所见，kNN 有许多缺点：</p>\n<ul>\n<li>\n<p>分类器必须<em>记住</em>所有训练数据并将其存储以供将来与测试数据进行比较。这是空间效率低下的，因为数据集的大小很可能达到千兆字节。</p>\n</li>\n<li>\n<p>对测试图像进行分类的成本很高，因为它需要与所有训练图像进行比较。</p>\n</li>\n</ul>\n<p><strong>摘要：<strong>我们现在将开发一种更强大的图像分类方法，最终将自然地扩展到整个神经网络和卷积神经网络。该方法将有两个主要组成部分：将原始数据映射到类别分数的</strong>分数函数</strong>，以及量化预测分数和真实标签之间的一致性的<strong>损失函数</strong>。然后，我们将其视为一个优化问题，其中我们将相对于得分函数的参数最小化损失函数。</p>\n<h2 id=\"1从图像到标签分数的参数化映射\"><a class=\"markdownIt-Anchor\" href=\"#1从图像到标签分数的参数化映射\">#</a> 1. 从图像到标签分数的参数化映射</h2>\n<p>​\t首先我们定义一个评价函数，用于将图片的像素映射为每一类别的得分，具体来说假设我们有一个训练集，其中的每个样本<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>D</mi></msup></mrow><annotation encoding=\"application/x-tex\">x_i \\in R^D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6891em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span></span></span></span></span></span></span> 每个样本对应一个标签 yi，这里的 i=1…N, 并且<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mrow><mn>1</mn><mo>…</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">y_i \\in { 1 \\dots K }</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7335400000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span>, 即表示有 N 个样本 (每个样本都有一个维度 D) 和 K 个不同的类别，例如在 CIFAR-10 中我们有一个训练集 N=50000，每个图像都有 32×32×3=3072 个像素，K=10，因为有 10 个不同的类别，故我们需要定义一个评估函数 f:<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi>R</mi><mi>D</mi></msup><mo>↦</mo><msup><mi>R</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">f: R^D \\mapsto R^K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">:</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.852331em;vertical-align:-0.011em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">↦</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span></span></span></span></span></span></span>，即将原始图像像素映射到类别分数。</p>\n<p>** 线性分类：** 我们以最简单的线性分类器为例，即我们定义评估函数为 (这里的 × 为矩阵的乘)：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><mi>W</mi><mo separator=\"true\">,</mo><mi>b</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>W</mi><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">f(x_i, W, b) =  W x_i + b\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">b</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></span></p>\n<p>在上述方程中，我们假设图像 Xi，将其所有像素平展为形状为 [D×1] 的单列向量。矩阵 W（大小 [KxD]）和向量 b（大小 [kx1]）是函数的<strong>参数</strong>。在 CIFAR-10 中，xi 包含第 i 个图像中的所有像素，这些像素被展平为单个 [3072 x 1] 列，W 是 [10 x 3072]，b 是 [10×1]，因此 3072 个数字进入函数（原始像素值），10 个数字出来（类得分）。W 中的参数通常被称为权重，b 被称为偏差向量，因为它影响输出分数，但不与实际数据 xi 交互。然而，您经常会听到人们交替使用权重和参数这两个术语。</p>\n<p><strong>概念解释</strong></p>\n<ol>\n<li><strong>W：权重矩阵</strong></li>\n</ol>\n<ul>\n<li><strong>定义</strong>：W 是一个矩阵，通常被称为权重矩阵，它的大小为 [K x D]，其中 K 是类别的数量，D 是输入特征的维度（对于图像数据，D 通常是像素数量）。</li>\n<li><strong>作用</strong>：W 矩阵中的每一行对应一个类别，每一列对应一个输入特征。它用于将输入特征向量映射到类别得分。</li>\n</ul>\n<ol start=\"2\">\n<li><strong>b：偏置向量</strong>\n<ul>\n<li><strong>定义</strong>：b 是一个向量，大小为 [K x 1]，即它有 K 个元素，每个元素对应一个类别。</li>\n<li>** 作用：**b 是偏置向量，它在最终的分类得分中起到偏移作用，不与输入特征直接相乘。它的作用是调整模型的输出，使其更好地拟合训练数据。</li>\n</ul>\n</li>\n<li><strong>xi (输入特征向量)</strong>：\n<ul>\n<li><strong>定义</strong>：xi 是输入数据的特征向量，大小为 [D x 1]，即它有 D 个元素。</li>\n<li><strong>作用</strong>：对于每个输入样本 xi，它包含了所有的特征（例如图像中的所有像素值）。</li>\n</ul>\n</li>\n<li><strong><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">Wx_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> (线性变换)</strong>：\n<ul>\n<li><strong>定义</strong>：<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">Wx_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> (表示将权重矩阵 W 与输入特征向量 xi 进行矩阵乘法，得到一个新的向量，该向量大小为 [K x 1]。</li>\n<li><strong>作用</strong>：这个向量包含了每个类别的线性组合得分。矩阵乘法的结果是将输入特征向量投影到类别得分空间中。</li>\n</ul>\n</li>\n</ol>\n<p>有几点需要注意：</p>\n<ul>\n<li>首先，请注意，单矩阵乘法<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">W x_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 有效地并行评估 10 个单独的分类器（每个类一个），其中每个分类器是一行 W。</li>\n<li>还要注意，我们认为输入数据<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator=\"true\">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_i, y_i)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> 是给定的和固定的，但我们可以控制参数 W，b 的设置。我们的目标是以这样的方式设置这些参数，即计算出的分数与整个训练集的地面实况标签相匹配。我们将更详细地介绍如何做到这一点，但凭直觉，我们希望正确的类的分数高于不正确类的分数。</li>\n<li>这种方法的优点是使用训练数据来学习参数 W、b，但一旦学习完成，我们就可以丢弃整个训练集，只保留学习的参数。这是因为新的测试图像可以简单地通过函数转发，并基于计算出的分数进行分类。</li>\n<li>最后，请注意，对测试图像进行分类涉及单个矩阵的乘法和加法，这比将测试图像与所有训练图像进行比较要快得多。</li>\n<li>最后，哪个分类器的评估分数最大，说明该图片最有可能是那个类</li>\n</ul>\n<h3 id=\"11-对线性分类的直观理解\"><a class=\"markdownIt-Anchor\" href=\"#11-对线性分类的直观理解\">#</a> 1.1 对线性分类的直观理解</h3>\n<p><img data-src=\"readme_picture/5.png\" alt=\"5\"></p>\n<p>如下图就是线性分类器的工作过程，对于一张图片，假设是个黑白的 2×2 的图片，我们首先将其展开成一个 4×1 的列向量。假设我们一共有三个类比（猫，狗，船），下图中红色的表示识别猫的分类器，绿色的表示识别狗的分类器，蓝色表示识别船的分类器，三个分类器的参数堆叠在一起组成了评估函数的参数矩阵。将参数矩阵与原始数据相乘并加上对应的偏置项，得到每个分类器的得分，可以看到猫的得分为 -96.8，狗的得分为 437.9，这说明，分类器认为这张图片最可能是只狗，最不可能是只猫，对于图中的例子，这无疑是个很不好的结果（因为输入图片是一只猫）。</p>\n<p><img data-src=\"readme_picture/4.png\" alt=\"4\"></p>\n<p>从上述例子我们可以看出，线性分类器首先将一个 RGB 通道的图片的每个位置的像素进行加权后求和，将求和后的值作为该分类器所属类别的分数（分数最高的分类器所属的类别作为这张图片的预测类别）。** 那么线性分类器的原理是什么呢？** 评估函数通过调整参数，使得其有能力倾向于或者不倾向于图片某个位置的某个颜色。举例来说，对于一张船的图片，图片的周围很可能有很多的蓝色（表示在大海中），那么对于船的分类器就可能对图片四周位置的蓝色通道的权重设置成一个正数，而对相同位置的红色通道和绿色通道的权重设置成一个负数，这样对于某张图，如果满足图片四周位置的蓝色通道的数值大，而其他通道数值小（在图片中表现为四周为蓝色），那么它的评估得分也就会高。</p>\n<h3 id=\"12-对线性分类的几何理解\"><a class=\"markdownIt-Anchor\" href=\"#12-对线性分类的几何理解\">#</a> 1.2 对线性分类的几何理解</h3>\n<p>运用线性分类器后，我们将每张图片都转变成了一个<strong> K</strong> 维的向量，那么我们可以将其看成<strong> K</strong> 维空间中的一个点，同样，对于原来的图片，也可以看成高维空间中的一个点（比如 CIFAR-10 数据集中的每个图片可以看成 3072 维空间中的一个点）。由于我们定义每个类别得分为某张图片所有像素的加权和，所有每个类别的得分其实可以看成这个高维空间中的一个超平面，虽然无法可视化高维空间，但是如果将高维空间降维至二维，我们可以尝试可视化每个分类器在做什么：</p>\n<p><img data-src=\"readme_picture/6.webp\" alt=\"6\"></p>\n<p>如上图所示，有三根直线分别代表汽车分类器、鹿分类器、飞机分类器，以红色的汽车分类器为例，如果某个点刚好在线上，则汽车种类的得分为 0，红色线上的箭头表示在这个方向上汽车得分为正，在反方向则为负。由此我们可以得出，评估函数中的<strong> W</strong> 每一行代表一个分类器，而一个分类器在高维空间中就是一个划分空间的超平面，所以<strong> W</strong> 中的每一行就代表一个分类器所对应超平面在空间中的方向，改变<strong> W</strong> 的值的效果就是旋转这个超平面。而偏置项<strong> b</strong> 其实就是超平面的截距，代表超平面在高维空间中的位置，如果没有偏置项，那么超平面只能过高维坐标系的原点。</p>\n<h3 id=\"摸匹配角度理解\"><a class=\"markdownIt-Anchor\" href=\"#摸匹配角度理解\">#</a> 摸匹配角度理解</h3>\n<p>对于参数<strong> W</strong> 还有一种解释：对于每一行的<strong> W</strong>，其代表了某一类别的模板，当预测图片时，我们将图片分别与每个类别的模板做内积，得到每个类别的评估得分，得分最高的类别作为预测结果。所以，线性分类器其实就是在做模板匹配的过程，而这些模板是从训练集中的图片训练得到的。另外还有一种理解角度，可以把线性分类器看成一种更高效的 KNN 算法。只是区别在于它不用和 KNN 一样存下整个训练集，而是将训练集浓缩成一个模板图片<strong> W</strong>，并且将内积作为距离函数，而不是 KNN 中的 L1 和 L2 距离。</p>\n<p>如下图是使用 CIFAR-10 作为训练集训练得到的分类器，从<strong> W</strong> 得到每个种类分类器的可视化结果：</p>\n<p><img data-src=\"readme_picture/7.webp\" alt=\"7\"></p>\n",
            "tags": [
                "Hexo",
                "Front Matter"
            ]
        },
        {
            "id": "https://enjundu.github.io/Sanfordcs231n/README_1_KNN/",
            "url": "https://enjundu.github.io/Sanfordcs231n/README_1_KNN/",
            "title": "线性分类器",
            "date_published": "2024-07-04T06:02:28.000Z",
            "content_html": "<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NearestNeighbor</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">self, X, y</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class=\"line\">        self.Xtr = X</span><br><span class=\"line\">        self.ytr = y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, X</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span></span><br><span class=\"line\">        num_test = X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        <span class=\"comment\"># lets make sure that the output type matches the input type</span></span><br><span class=\"line\">        Ypred = np.zeros(num_test, dtype=self.ytr.dtype)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># loop over all test rows</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_test):</span><br><span class=\"line\">            <span class=\"comment\"># find the nearest training image to the i&#x27;th test image</span></span><br><span class=\"line\">            <span class=\"comment\"># using the L1 distance (sum of absolute value differences)</span></span><br><span class=\"line\">            distances = np.<span class=\"built_in\">sum</span>(np.<span class=\"built_in\">abs</span>(self.Xtr - X[i,:]), axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">            min_index = np.argmin(distances)  <span class=\"comment\"># get the index with smallest distance</span></span><br><span class=\"line\">            Ypred[i] = self.ytr[min_index]    <span class=\"comment\"># predict the label of the nearest example</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> Ypred</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 示例使用</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    <span class=\"comment\"># 创建一些示例数据</span></span><br><span class=\"line\">    Xtr = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>], [<span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>]])  <span class=\"comment\"># 训练数据</span></span><br><span class=\"line\">    ytr = np.array([<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>])  <span class=\"comment\"># 训练标签</span></span><br><span class=\"line\"></span><br><span class=\"line\">    Xte = np.array([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>], [<span class=\"number\">7</span>, <span class=\"number\">7</span>, <span class=\"number\">9</span>]])  <span class=\"comment\"># 测试数据</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 初始化和训练分类器</span></span><br><span class=\"line\">    nn = NearestNeighbor()</span><br><span class=\"line\">    nn.train(Xtr, ytr)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 预测测试数据的标签</span></span><br><span class=\"line\">    Yte_predict = nn.predict(Xte)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Predicted labels:&#x27;</span>, Yte_predict)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"本项目参考学习文档为斯坦福计算机深度学习课程用于视觉识别的-cs231n-卷积神经网络\"><a class=\"markdownIt-Anchor\" href=\"#本项目参考学习文档为斯坦福计算机深度学习课程用于视觉识别的-cs231n-卷积神经网络\">#</a> 本项目参考学习文档为<span class=\"exturl\" data-url=\"aHR0cHM6Ly9jczIzMW4uZ2l0aHViLmlvL2NsYXNzaWZpY2F0aW9uLw==\">斯坦福计算机深度学习课程 —— 用于视觉识别的 CS231n 卷积神经网络</span></h1>\n<p>​\t曼哈顿距离：<img data-src=\"https://s2.loli.net/2024/07/03/4K7NUeW85QBCabV.png\" alt=\"image.png\">，其中 I1 和 I2 是两幅图像，I1P 和 I2P 分别是 I1 和 I2 在位置 P 的像素值。差异矩阵也是像素点依次相减。通过衡量图像像素点的绝对值差之和差异性来</p>\n<p>​\t对于该预测方法还有 L2 距离，其定义为：<img data-src=\"https://s2.loli.net/2024/07/03/fovI3tCNOXrB8aQ.png\" alt=\"image.png\"></p>\n<p>​\t最近临分类器：<br>\n​\t（1）训练：将训练数据存储在实际变量中。<br>\n​\t（2）预测：对每一个测试样本，计算其与训练样本 L1 的距离，找到距离最近的训练样本，将该训练样本的标签作为测试样本的预测标签。<br>\n​\t*<em> 训练方法</em> <em>：def train (self, X, y): X（训练数据，形状为 N</em>D，每一行都是一个样本）和 y（训练标签，大小为 N 的一维数组）, 然后存储在 self 类中。</p>\n<p>​\t <code>self.Xtr = X</code></p>\n<p>​\t <code>self.ytr = y</code></p>\n<p><strong>预测方法</strong></p>\n<p>predict 方法接受一个参数 x*(测试数据，形状为 N*D，每一行都是一个需要预测的样本)*</p>\n<p>num_test 表示测试样本的数量</p>\n<p>Ypred 初始化为零数组 <code>np.zeros(num_test,dtype=self.ytr.dtype)</code> , 前者表示数组的长度，即为包含元素的个数；后者是一个关键词参数，指定数组中的数据类型设置为与 self.ytr 数组的数据类型相同。dtype 是数据类型的缩写。</p>\n<p>对于每个测试样本，计算它与所有训练样本的 L1 距离：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">distances = np.<span class=\"built_in\">sum</span>(np.<span class=\"built_in\">abs</span>(self.Xtr - X[i,:]), axis=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>通过取绝对值差的和计算 L1 距离。</li>\n<li><code>self.Xtr - X[i,:]</code>  计算每个训练样本与第 <code>i</code>  个测试样本的差值。(这里  <code>self.Xtr</code>  是一个矩阵， <code>X[i,:]</code>  是一个向量 <code>X[i,:]</code>  表示矩阵  <code>X</code>  的第  <code>i</code>  行)</li>\n<li><code>self.Xtr - X[i,:]</code>  计算的是矩阵  <code>self.Xtr</code>  中的每一行向量减去向量  <code>X[i,:]</code>  的结果。</li>\n<li><code>np.abs(self.Xtr - X[i,:])</code> ：对上述差值矩阵中的每个元素取绝对值，得到一个与  <code>self.Xtr</code>  形状相同的矩阵。</li>\n<li><code>np.sum(..., axis=1)</code>  对每个样本的所有特征求和，得到每个训练样本与该测试样本的距离。</li>\n<li>通过  <code>min_index = np.argmin(distances)</code>  找到距离最小的样本 (<em>np.argmin: 返回最小距离对应的索引</em>)</li>\n</ul>\n<p><strong>Q: With N examples, how fast are training and prediction?</strong></p>\n<ul>\n<li>问题：对于 N 个样本，训练和预测的速度（效率）是多少？</li>\n</ul>\n<p><strong>A: Train O(1), predictO(N)</strong></p>\n<ul>\n<li>\n<p>回答：训练时间复杂度是 O (1)，预测时间复杂度是 O (N)。</p>\n</li>\n<li>\n<p><strong>训练时间复杂度 O (1)</strong>：训练阶段只需记住所有训练数据，不需要复杂计算，所以训练时间是常数时间，即与训练样本的数量 N 无关。</p>\n</li>\n<li>\n<p><strong>预测时间复杂度 O (N)</strong>：预测阶段，对于每个测试样本，需要计算其与所有训练样本的距离，并找到最近的那个训练样本。因此，预测时间与训练样本的数量 N 成正比。</p>\n</li>\n</ul>\n<p><strong>Q:This is bad: we want classifiers that are fast at prediction; slow for training is ok</strong></p>\n<ul>\n<li>为什么这是不好的设计？</li>\n</ul>\n<p><strong>A: 这是不好的设计：我们希望分类器在预测时速度快；在训练时速度慢是可以接受的。</strong></p>\n<ul>\n<li>在实际应用中，预测阶段通常比训练阶段更频繁。例如，一个已经训练好的分类器可能需要处理大量的实时预测请求。如果预测速度很慢，会严重影响系统的性能和用户体验。</li>\n<li>训练阶段可以相对较慢，因为训练通常是一个离线过程，可以在后台完成，不直接影响用户体验。</li>\n</ul>\n<h2 id=\"总结\"><a class=\"markdownIt-Anchor\" href=\"#总结\">#</a> 总结</h2>\n<p>​\t最近邻分类器的一个主要缺点是它的预测阶段计算量大，对于每个测试样本需要计算其与所有训练样本的距离，因此预测速度较慢。这在需要快速响应的应用中是一个很大的问题，而对于训练阶段的速度要求则相对宽松。因此，更理想的分类器设计是训练阶段可以慢一些，但预测阶段必须非常快。</p>\n<h2 id=\"k-最近临法\"><a class=\"markdownIt-Anchor\" href=\"#k-最近临法\">#</a> K - 最近临法</h2>\n<p><img data-src=\"https://s2.loli.net/2024/07/03/UIrCatE1Zfu5Wib.png\" alt=\"image.png\"></p>\n<p>​\t图中展示的是 K - 最近邻（K-Nearest Neighbors, K-NN）分类器的基本原理和效果。K-NN 分类器是一个简单但非常有效的分类算法，它的核心思想是根据一个样本的 K 个最近邻居来确定其类别。具体来说，这里通过 K 个最近邻居的多数投票来决定样本的分类。</p>\n<p><strong>白色区域：这个区域没有进行 k - 最近投票</strong></p>\n<p><strong>k=1：</strong></p>\n<ul>\n<li>当 K=1 时，分类器仅考虑与样本最近的一个邻居。这个邻居的类别直接决定了样本的类别。虽然这种方法非常直接和简单，但它容易受噪声和孤立点的影响，导致分类效果不稳定。</li>\n</ul>\n<p><strong>k=3：</strong></p>\n<ul>\n<li>当 K=3 时，分类器考虑与样本最近的三个邻居。样本的类别由这三个邻居的多数投票决定。相比于 K=1，这种方法更加鲁棒，能够更好地抵御噪声的影响。但是，它仍然可能受少数几个错误邻居的影响。</li>\n</ul>\n<p><strong>k=5：</strong></p>\n<ul>\n<li>当 K=5 时，分类器考虑与样本最近的五个邻居。样本的类别由这五个邻居的多数投票决定。进一步增加 K 值，分类器变得更加稳定，因为它综合了更多邻居的信息，减少了单个噪声点对分类结果的影响。然而，如果 K 值过大，分类器可能会包含过多不相关的邻居信息，导致分类结果不准确。</li>\n</ul>\n<h3 id=\"主要思想\"><a class=\"markdownIt-Anchor\" href=\"#主要思想\">#</a> 主要思想</h3>\n<ul>\n<li><strong>多数投票</strong>：K-NN 分类器的核心思想是 “多数投票”，即根据 K 个最近邻居中出现频率最高的类别来决定样本的类别。</li>\n<li><strong>距离度量</strong>：K-NN 分类器依赖于距离度量来确定最近的 K 个邻居。常用的距离度量方法包括欧氏距离、曼哈顿距离等。</li>\n<li><strong>参数选择</strong>：K 值的选择对分类效果有重要影响。K 值太小容易受噪声影响，K 值太大则可能引入过多无关信息。</li>\n</ul>\n<h2 id=\"接下来我们以cifar-10数据集的结果来进行代码编写\"><a class=\"markdownIt-Anchor\" href=\"#接下来我们以cifar-10数据集的结果来进行代码编写\">#</a> 接下来我们以<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuY3MudG9yb250by5lZHUvfmtyaXovY2lmYXIuaHRtbA==\"> CIFAR-10 数据集</span>的结果来进行代码编写</h2>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle  <span class=\"comment\">#用于序列化和反序列化 Python 对象的库。在这里用于加载 CIFAR-10 数据集文件</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> KFold</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_CIFAR_batch</span>(<span class=\"params\">filename</span>):</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot; Load a single batch of CIFAR-10 &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(filename, <span class=\"string\">&#x27;rb&#x27;</span>) <span class=\"keyword\">as</span> f: <span class=\"comment\">#以二进制模式(&#x27;rb&#x27;)打开文件</span></span><br><span class=\"line\">        datadict = pickle.load(f, encoding=<span class=\"string\">&#x27;latin1&#x27;</span>)  <span class=\"comment\">#使用pickle.load(f,encoding=&#x27;latin1&#x27;)反序列化文件内容，读取为字典对象</span></span><br><span class=\"line\">        X = datadict[<span class=\"string\">&#x27;data&#x27;</span>]</span><br><span class=\"line\">        Y = datadict[<span class=\"string\">&#x27;labels&#x27;</span>]</span><br><span class=\"line\">    <span class=\"comment\">#将其从平坦形状(10000,3072)重塑为(10000,32,32,3),并且将数据类型转为浮点型。即10000张图</span></span><br><span class=\"line\">    <span class=\"comment\"># 每张图有32*32像素，每个像素有三个颜色通道</span></span><br><span class=\"line\">    <span class=\"comment\">#并且将像素类型转为浮点数</span></span><br><span class=\"line\">        X = X.reshape(<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>).astype(<span class=\"string\">&#x27;float&#x27;</span>)</span><br><span class=\"line\">        Y = np.array(Y)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X, Y</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_CIFAR10</span>(<span class=\"params\">ROOT</span>): <span class=\"comment\">#ROOT表示CIFAR-10数据集根目录的路径</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot; Load all of CIFAR-10 &quot;&quot;&quot;</span></span><br><span class=\"line\">    xs = [] <span class=\"comment\">#存储每个批次的图像数据</span></span><br><span class=\"line\">    ys = [] <span class=\"comment\">#存储每个批次的标签</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> b <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"number\">6</span>): <span class=\"comment\">#加载五个训练批次</span></span><br><span class=\"line\">        f = os.path.join(ROOT, <span class=\"string\">&#x27;data_batch_%d&#x27;</span> % (b,))  <span class=\"comment\">#依次加载文件里的五个批次</span></span><br><span class=\"line\">        X, Y = load_CIFAR_batch(f)  <span class=\"comment\">#调用load_CIFAR_batch（）函数来加载数据</span></span><br><span class=\"line\">        <span class=\"comment\">#将加载的的图像数据和标签分别添加到列表中</span></span><br><span class=\"line\">        xs.append(X)</span><br><span class=\"line\">        ys.append(Y)</span><br><span class=\"line\">    <span class=\"comment\">#将所有训练批次的数据合并为一个训练集</span></span><br><span class=\"line\">    Xtr = np.concatenate(xs)</span><br><span class=\"line\">    Ytr = np.concatenate(ys)</span><br><span class=\"line\">    <span class=\"keyword\">del</span> X, Y    <span class=\"comment\">#释放内存，删除X和Y</span></span><br><span class=\"line\">    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, <span class=\"string\">&#x27;test_batch&#x27;</span>)) <span class=\"comment\">#加载测试批次的数据</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> Xtr, Ytr, Xte, Yte</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载数据</span></span><br><span class=\"line\">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class=\"string\">r&#x27;D:\\desktop\\Code_Compiling\\cs231n_stanford\\example&amp;tests\\cifar-10-python\\cifar-10-batches-py&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 将数据拉成向量</span></span><br><span class=\"line\"><span class=\"comment\">#重塑数据：将训练数据 Xtr 和测试数据 Xte 从形状（num_samples, 32, 32, 3）转换为（num_samples, 3072）的向量形式</span></span><br><span class=\"line\"><span class=\"comment\"># 其中 3072 是 32x32 图像的像素总数乘以 3 个颜色通道。</span></span><br><span class=\"line\">Xtr_rows = Xtr.reshape(Xtr.shape[<span class=\"number\">0</span>], <span class=\"number\">32</span> * <span class=\"number\">32</span> * <span class=\"number\">3</span>)</span><br><span class=\"line\">Xte_rows = Xte.reshape(Xte.shape[<span class=\"number\">0</span>], <span class=\"number\">32</span> * <span class=\"number\">32</span> * <span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;数据加载和预处理完成&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">NearestNeighbor</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>): <span class=\"comment\">#初始化</span></span><br><span class=\"line\">        <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">train</span>(<span class=\"params\">self, X, y</span>):  <span class=\"comment\">#该方法用于训练k-NN模型，将x和y存储为类的属性</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span></span><br><span class=\"line\">        self.Xtr = X    <span class=\"comment\">#X是N*D的矩阵，每行是一个训练样本</span></span><br><span class=\"line\">        self.ytr = y    <span class=\"comment\">#y是一个长度为N的一维数组，对应于每个训练样本的标签</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#预测给定的数据集x的标签,X是N*D的矩阵，每行都是一个测试样本</span></span><br><span class=\"line\">    <span class=\"comment\">#k是要考虑的最近邻居的数量，默认为1。distfn是距离度量方式，可以是：L1(曼哈顿距离)也可以是L2(欧几里得距离)</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">predict</span>(<span class=\"params\">self, X, k=<span class=\"number\">1</span>, distfn=<span class=\"string\">&#x27;L1&#x27;</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span></span><br><span class=\"line\">        num_test = X.shape[<span class=\"number\">0</span>]   <span class=\"comment\">#这是测试集的样本数量</span></span><br><span class=\"line\">        Ypred = np.zeros(num_test, dtype=self.ytr.dtype)    <span class=\"comment\">#初始化为一个零数组，用于存储每个测试样本的预测标签</span></span><br><span class=\"line\">        <span class=\"comment\">#循环处理每个测试样本</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_test):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> distfn == <span class=\"string\">&#x27;L1&#x27;</span>:</span><br><span class=\"line\">                distances = np.<span class=\"built_in\">sum</span>(np.<span class=\"built_in\">abs</span>(self.Xtr - X[i,:]), axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> distfn == <span class=\"string\">&#x27;L2&#x27;</span>:</span><br><span class=\"line\">                distances = np.sqrt(np.<span class=\"built_in\">sum</span>(np.square(self.Xtr - X[i,:]), axis=<span class=\"number\">1</span>))</span><br><span class=\"line\">            min_indices = np.argpartition(distances, k)[:k] <span class=\"comment\">#找到k个最近邻居的索引</span></span><br><span class=\"line\">            closest_y = self.ytr[min_indices]</span><br><span class=\"line\">            <span class=\"comment\">#预测标签</span></span><br><span class=\"line\">            <span class=\"comment\">#使用 np.bincount 统计最近邻居中每个标签的出现次数，并选择出现次数最多的标签作为预测标签。</span></span><br><span class=\"line\">            Ypred[i] = np.bincount(closest_y).argmax()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> Ypred</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 训练和评估模型</span></span><br><span class=\"line\">nn = NearestNeighbor()  <span class=\"comment\">#创建 NearestNeighbor 类的实例 nn。</span></span><br><span class=\"line\">nn.train(Xtr_rows, Ytr) <span class=\"comment\">#使用训练数据 Xtr_rows 和标签 Ytr 训练模型。</span></span><br><span class=\"line\">Yte_predict = nn.predict(Xte_rows)  <span class=\"comment\">#使用测试数据 Xte_rows 预测标签。</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;accuracy: %f&#x27;</span> % (np.mean(Yte_predict == Yte)))   <span class=\"comment\">#计算并打印预测标签与真实标签的匹配率（准确率）。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建验证集——从训练集中提取前1000个样本作为验证集</span></span><br><span class=\"line\">Xval_rows = Xtr_rows[:<span class=\"number\">1000</span>, :]  <span class=\"comment\">#验证集特征</span></span><br><span class=\"line\">Yval = Ytr[:<span class=\"number\">1000</span>]   <span class=\"comment\">#验证集标签</span></span><br><span class=\"line\">Xtr_rows = Xtr_rows[<span class=\"number\">1000</span>:, :]   <span class=\"comment\">#新训练集特征</span></span><br><span class=\"line\">Ytr = Ytr[<span class=\"number\">1000</span>:]    <span class=\"comment\">#新训练集标签</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 超参数调优——对于不同的k和距离度量方式进行超参数调优</span></span><br><span class=\"line\">validation_accuracies = []</span><br><span class=\"line\"><span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>, <span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">50</span>, <span class=\"number\">100</span>]:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> distfn <span class=\"keyword\">in</span> [<span class=\"string\">&#x27;L1&#x27;</span>, <span class=\"string\">&#x27;L2&#x27;</span>]:</span><br><span class=\"line\">        nn = NearestNeighbor()  <span class=\"comment\">#创建NearestNeighbor实例</span></span><br><span class=\"line\">        nn.train(Xtr_rows, Ytr) <span class=\"comment\">#训练</span></span><br><span class=\"line\">        Yval_predict = nn.predict(Xval_rows, k=k, distfn=distfn)    <span class=\"comment\">#预测</span></span><br><span class=\"line\">        acc = np.mean(Yval_predict == Yval) <span class=\"comment\">#验证</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;accuracy for k=<span class=\"subst\">&#123;k&#125;</span>, distfn=<span class=\"subst\">&#123;distfn&#125;</span>: <span class=\"subst\">&#123;acc&#125;</span>&#x27;</span>)</span><br><span class=\"line\">        validation_accuracies.append((k, distfn, acc))</span><br><span class=\"line\"><span class=\"comment\">#交叉验证函数</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">cross_validation</span>(<span class=\"params\">X, y, k_choices, distfns</span>):</span><br><span class=\"line\">    kf = KFold(n_splits=<span class=\"number\">5</span>)  <span class=\"comment\">#将数据集分成五份</span></span><br><span class=\"line\">    validation_accuracies = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> k_choices: <span class=\"comment\">#对每个k和distfns组合进行操作</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> distfn <span class=\"keyword\">in</span> distfns:</span><br><span class=\"line\">            accs = []   <span class=\"comment\">#初始化一个空列表 accs 用于存储交叉验证的准确率</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> train_index, val_index <span class=\"keyword\">in</span> kf.split(X):</span><br><span class=\"line\">                X_train, X_val = X[train_index], X[val_index] <span class=\"comment\">#将数据分为训练集和验证集。</span></span><br><span class=\"line\">                y_train, y_val = y[train_index], y[val_index]</span><br><span class=\"line\"></span><br><span class=\"line\">                nn = NearestNeighbor()</span><br><span class=\"line\">                nn.train(X_train, y_train)</span><br><span class=\"line\">                y_val_predict = nn.predict(X_val, k=k, distfn=distfn)</span><br><span class=\"line\">                acc = np.mean(y_val_predict == y_val) <span class=\"comment\">#用验证集进行预测并计算准确率。</span></span><br><span class=\"line\">                accs.append(acc)</span><br><span class=\"line\">            avg_acc = np.mean(accs)<span class=\"comment\">#计算并打印每个 k 和 distfn 组合的平均准确率。</span></span><br><span class=\"line\">            validation_accuracies.append((k, distfn, avg_acc))<span class=\"comment\">#将 k、distfn 和平均准确率保存到 validation_accuracies 列表中。</span></span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Cross-validation accuracy for k=<span class=\"subst\">&#123;k&#125;</span>, distfn=<span class=\"subst\">&#123;distfn&#125;</span>: <span class=\"subst\">&#123;avg_acc&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> validation_accuracies</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用交叉验证</span></span><br><span class=\"line\">k_choices = [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>, <span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">50</span>, <span class=\"number\">100</span>]</span><br><span class=\"line\">distfns = [<span class=\"string\">&#x27;L1&#x27;</span>, <span class=\"string\">&#x27;L2&#x27;</span>]</span><br><span class=\"line\">validation_accuracies = cross_validation(Xtr_rows, Ytr, k_choices, distfns)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p><strong>k-NN 阶段实现了一个简单的 k 近邻（k-Nearest Neighbors, k-NN）分类器，使用了 L1 距离（曼哈顿距离）和 L2 距离（欧几里得距离）</strong></p>\n<p><strong>代码注释已列与代码中</strong></p>\n",
            "tags": [
                "Hexo",
                "Front Matter"
            ]
        }
    ]
}