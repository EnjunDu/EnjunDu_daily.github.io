<!-- build time:Sun Aug 11 2024 15:49:40 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="Hexo" href="https://enjundu.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="Hexo" href="https://enjundu.github.io/atom.xml"><link rel="alternate" type="application/json" title="Hexo" href="https://enjundu.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="Hexo,Front Matter,deep_learning"><link rel="canonical" href="https://enjundu.github.io/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_5_CNN/"><title>深度学习_5_CNN - deep-learning | 蓝天の网站 = Hexo</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">深度学习_5_CNN</h1><div class="meta"><span class="item" title="Created: 2024-07-28 14:02:28"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">Posted on</span> <time itemprop="dateCreated datePublished" datetime="2024-07-28T14:02:28+08:00">2024-07-28</time></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">蓝天の网站</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2024/07/04/zAwyEXCNIOpoDLk.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/04/d8I9raJDhEUebpV.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/04/OLqrN6bKDJHW5ek.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/03/8Rq9gGbxm7TXjfp.png"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/03/flquHABaRmV2onx.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/04/pVXxd2rsZPS3RLj.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">Home</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/deep-learning/" itemprop="item" rel="index" title="In deep-learning"><span itemprop="name">deep-learning</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="en"><link itemprop="mainEntityOfPage" href="https://enjundu.github.io/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_5_CNN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Jack Du"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Hexo"></span><div class="body md" itemprop="articleBody"><h1 id="卷积神经网络"><a class="anchor" href="#卷积神经网络">#</a> 卷积神经网络</h1><p><img data-src="https://s2.loli.net/2024/07/23/sNYuvbPIipF3aZX.png" alt="QQ_1721747809171.png"></p><p><img data-src="https://s2.loli.net/2024/07/29/4iFKf53Wtc2xMIP.png" alt="QQ_1722182572913.png"></p><h2 id="输入层"><a class="anchor" href="#输入层">#</a> 输入层</h2><p><strong>输入</strong>：图像大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>28</mn><mo>×</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">1 \times 28 \times 28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">8</span></span></span></span>，其中 1 表示图像的通道数（灰度图像），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>28</mn></mrow><annotation encoding="application/x-tex">28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">8</span></span></span></span> 表示图像的宽度和高度。</p><h2 id="卷积层"><a class="anchor" href="#卷积层">#</a> 卷积层</h2><p><strong>卷积核</strong>，也称为滤波器（filter），是卷积神经网络（CNN）中的一个小矩阵，用来对输入图像进行卷积操作，从而提取特征。</p><p><strong>卷积操作</strong>：对输入图像应用卷积核，通常用 5x5 的卷积核，这里得到的特征图（Feature maps）大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>24</mn><mo>×</mo><mn>24</mn></mrow><annotation encoding="application/x-tex">4 \times 24 \times24</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">4</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span><span class="mord">4</span></span></span></span> ，其中 4 是卷积核的数量（也就是产生的特征图数量），24 是卷积后的宽度和高度（由于卷积操作会减小尺寸）。</p><p><strong>特征图</strong>：表示从图像中提取的特征，通过卷积操作得到。</p><h2 id="下采样层-s1"><a class="anchor" href="#下采样层-s1">#</a> 下采样层 S1</h2><ul><li><strong>下采样</strong>是一种减少特征图尺寸的方法，同时保留重要特征。最常见的下采样方法是池化，包括最大池化（Max Pooling）和平均池化（Average Pooling）。</li><li><strong>最大池化（Max Pooling）</strong>：选择池化窗口中的最大值。例如，使用 $ 2 \times 2 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>的池化窗口，对特征图进行最大池化，将每个</mtext></mrow><annotation encoding="application/x-tex">的池化窗口，对特征图进行最大池化，将每个</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">池</span><span class="mord cjk_fallback">化</span><span class="mord cjk_fallback">窗</span><span class="mord cjk_fallback">口</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">对</span><span class="mord cjk_fallback">特</span><span class="mord cjk_fallback">征</span><span class="mord cjk_fallback">图</span><span class="mord cjk_fallback">进</span><span class="mord cjk_fallback">行</span><span class="mord cjk_fallback">最</span><span class="mord cjk_fallback">大</span><span class="mord cjk_fallback">池</span><span class="mord cjk_fallback">化</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">将</span><span class="mord cjk_fallback">每</span><span class="mord cjk_fallback">个</span></span></span></span> 2 \times 2 $ 区域中的最大值作为输出。</li><li><strong>平均池化（Average Pooling）</strong>：选择池化窗口中的平均值，类似于最大池化，但取平均值。</li></ul><ul><li><strong>下采样（Subsampling）</strong>：通常使用最大池化（Max Pooling）或平均池化（Average Pooling），这里使用 2x2 的池化核，对 C1 层的特征图进行下采样，得到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>12</mn><mo>×</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">4 \times 12 \times 12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">4</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">2</span></span></span></span> 的特征图。</li><li><strong>作用</strong>：减少特征图的尺寸，降低计算量，同时保留重要特征。</li></ul><h2 id="卷积层-c2"><a class="anchor" href="#卷积层-c2">#</a> 卷积层 C2</h2><ul><li>卷积操作：再次应用卷积核，这里使用 5x5 的卷积核，得到的特征图大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">8 \times 8 \times 8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">8</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">8</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">8</span></span></span></span>，其中 8 是卷积核的数量。</li><li><strong>特征图</strong>：进一步提取更高层次的特征。</li></ul><h2 id="下采样层-s2"><a class="anchor" href="#下采样层-s2">#</a> 下采样层 S2</h2><ul><li><strong>下采样（Subsampling）</strong>：同样使用 2x2 的池化核，对 C2 层的特征图进行下采样，得到 8×4×4 的特征图。</li><li><strong>作用</strong>：进一步减少特征图的尺寸，降低计算量。</li></ul><h2 id="全连接层-n1-和-n2"><a class="anchor" href="#全连接层-n1-和-n2">#</a> 全连接层 n1 和 n2</h2><ul><li><p><strong>Flattening</strong>：将多维的特征图展开成一维向量，准备输入到全连接层中。</p><p><strong>全连接层 n1</strong>：连接所有输入神经元到输出神经元，这里假设有若干个神经元。</p><p><strong>全连接层 n2</strong>：最终输出层，通常有 10 个神经元，对应 10 个分类（数字 0 到 9）。</p></li></ul><h2 id="输出层分类器"><a class="anchor" href="#输出层分类器">#</a> 输出层 / 分类器</h2><ul><li><strong>输出</strong>：每个神经元表示输入图像属于某一类的概率，输出的维度是 10，对应 10 个类别。</li></ul><h2 id="总结"><a class="anchor" href="#总结">#</a> 总结</h2><ul><li><strong>特征提取（Feature Extraction）</strong>：通过卷积层和下采样层提取图像的特征。</li><li><strong>分类（Classification）</strong>：通过全连接层对提取的特征进行分类。</li></ul><h1 id="介绍"><a class="anchor" href="#介绍">#</a> 介绍</h1><h2 id="颜色通道"><a class="anchor" href="#颜色通道">#</a> 颜色通道</h2><ul><li><p>一个图像通常由红绿蓝三个通道组成，也就是 RGB 里的 Red、Green、Blue</p></li><li><p>分为 Input Channel、Width 和 Height</p></li></ul><h2 id="patch"><a class="anchor" href="#patch">#</a> Patch</h2><p>在计算机视觉和图像处理领域，“Patch” 通常指的是图像中的一个小区域或子块。Patch 可以用于各种任务，例如特征提取、图像分割、图像修复等。</p><h2 id="卷积的运算过程"><a class="anchor" href="#卷积的运算过程">#</a> 卷积的运算过程</h2><p><img data-src="https://s2.loli.net/2024/07/29/oR7KpSJngOmjQv2.png" alt="QQ_1722186528573.png"></p><ul><li><p>假设有一个 5×5 的输入，3×3 的核要做卷积。（矩阵的向量积：对应的位置数量相乘然后相加）</p></li><li><p>有几个通道，卷积核就应该是几个</p></li><li><p>不同通道卷积后最终的 Output 再在对应的位置相加<br><img data-src="https://s2.loli.net/2024/07/29/mjtTFDI4uPecSp7.png" alt="QQ_1722186740147.png"></p></li></ul><h3 id="应用于多个patch时流程图如下"><a class="anchor" href="#应用于多个patch时流程图如下">#</a> 应用于多个 Patch 时，流程图如下</h3><p><img data-src="https://s2.loli.net/2024/07/29/RBmzQTihHMqdv4e.png" alt="QQ_1722187086148.png"></p><h2 id="将结果拼接成四维输出"><a class="anchor" href="#将结果拼接成四维输出">#</a> 将结果拼接成四维输出</h2><p><img data-src="https://s2.loli.net/2024/07/29/ha7w2p4JKvNE1jy.png" alt="QQ_1722187649292.png"></p><ul><li>左侧的立方体表示输入张量，大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><msub><mtext>width</mtext><mtext>in</mtext></msub><mo>×</mo><msub><mtext>height</mtext><mtext>in</mtext></msub></mrow><annotation encoding="application/x-tex">n \times \text{width}_{\text{in}} \times \text{height}_{\text{in}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord text"><span class="mord">width</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31750199999999995em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">in</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord text"><span class="mord">height</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.22336199999999998em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">in</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span>。</li><li>中间的部分说明了每个卷积层需要 $ m <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>个滤波器（即卷积核），每个滤波器的大小为</mtext></mrow><annotation encoding="application/x-tex">个滤波器（即卷积核），每个滤波器的大小为</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord cjk_fallback">个</span><span class="mord cjk_fallback">滤</span><span class="mord cjk_fallback">波</span><span class="mord cjk_fallback">器</span><span class="mord cjk_fallback">（</span><span class="mord cjk_fallback">即</span><span class="mord cjk_fallback">卷</span><span class="mord cjk_fallback">积</span><span class="mord cjk_fallback">核</span><span class="mord cjk_fallback">）</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">每</span><span class="mord cjk_fallback">个</span><span class="mord cjk_fallback">滤</span><span class="mord cjk_fallback">波</span><span class="mord cjk_fallback">器</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">大</span><span class="mord cjk_fallback">小</span><span class="mord cjk_fallback">为</span></span></span></span> n \times \text {kernel_size}<em>{\text{width}} \times \text{kernel_size}</em>{\text{height}}$。</li><li>右侧的立方体表示输出特征图，大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><msub><mtext>width</mtext><mtext>out</mtext></msub><mo>×</mo><msub><mtext>height</mtext><mtext>out</mtext></msub></mrow><annotation encoding="application/x-tex">m \times \text{width}_{\text{out}} \times \text{height}_{\text{out}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord text"><span class="mord">width</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.93858em;vertical-align:-.24414em"></span><span class="mord"><span class="mord text"><span class="mord">height</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.18641599999999994em"><span style="top:-2.4558600000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">out</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.24414em"><span></span></span></span></span></span></span></span></span></span>。</li></ul><h3 id="重点图中四维张量的解释"><a class="anchor" href="#重点图中四维张量的解释">#</a> 重点：图中四维张量的解释</h3><p>图中间部分显示了四维度张量的结构：</p><ul><li>每个卷积核的大小是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><msub><mtext>kernel_size</mtext><mtext>width</mtext></msub><mo>×</mo><msub><mtext>kernel_size</mtext><mtext>height</mtext></msub></mrow><annotation encoding="application/x-tex">n \times \text{kernel\_size}_{\text{width}} \times \text{kernel\_size}_{\text{height}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.0541399999999999em;vertical-align:-.3597em"></span><span class="mord"><span class="mord text"><span class="mord">kernel_size</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1264079999999999em"><span style="top:-2.3403em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">width</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3597em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.190248em;vertical-align:-.495808em"></span><span class="mord"><span class="mord text"><span class="mord">kernel_size</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.12640799999999985em"><span style="top:-2.3403em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">height</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.495808em"><span></span></span></span></span></span></span></span></span></span></li><li>有 $$m$$ 个这样的卷积核，因此整个卷积层的权重可以表示为一个四维张量，大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><msub><mtext>kernel_size</mtext><mtext>width</mtext></msub><mo>×</mo><msub><mtext>kernel_size</mtext><mtext>height</mtext></msub></mrow><annotation encoding="application/x-tex">m \times n \times \text{kernel\_size}_{\text{width}} \times \text{kernel\_size}_{\text{height}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.0541399999999999em;vertical-align:-.3597em"></span><span class="mord"><span class="mord text"><span class="mord">kernel_size</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.1264079999999999em"><span style="top:-2.3403em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">width</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3597em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.190248em;vertical-align:-.495808em"></span><span class="mord"><span class="mord text"><span class="mord">kernel_size</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.12640799999999985em"><span style="top:-2.3403em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">height</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.495808em"><span></span></span></span></span></span></span></span></span></span></li></ul><h1 id="代码实现"><a class="anchor" href="#代码实现">#</a> 代码实现</h1><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">in_channels, out_channels = <span class="number">5</span>, <span class="number">10</span>  </span><br><span class="line">width, height = <span class="number">100</span>, <span class="number">100</span></span><br><span class="line">kernel_size = <span class="number">3</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.randn(batch_size, in_channels, width, height)</span><br><span class="line"></span><br><span class="line">conv_layer = torch.nn.Conv2d(in_channels,</span><br><span class="line">                             out_channels,</span><br><span class="line">                             kernel_size=kernel_size)</span><br><span class="line"></span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)	<span class="comment">#torch.Size([1, 5, 100, 100])</span></span><br><span class="line"><span class="built_in">print</span>(output.shape)	<span class="comment">#torch.Size([1, 10, 98, 98])</span></span><br><span class="line"><span class="built_in">print</span>(conv_layer.weight.shape) <span class="comment">#torch.Size([10, 5, 3, 3])</span></span><br></pre></td></tr></table></figure><p></p><ul><li><p><code>in_channels</code> ：输入通道数，这里设为 5。</p><p><code>out_channels</code> ：输出通道数，这里设为 10。</p><p><code>width</code> 和 <code>height</code> ：输入图像的宽度和高度，都设为 100。</p><p><code>kernel_size</code> ：卷积核的大小，这里设为 3（即 3×33 \times 33×3 的卷积核）。</p><p><code>batch_size</code> ：批量大小，这里设为 1。</p></li><li><p>使用 <code>torch.randn</code> 函数生成一个随机张量，模拟输入数据。张量的形状为 <code>(batch_size, in_channels, width, height)</code> ，即 <code>(1, 5, 100, 100)</code> 。</p></li><li><p>使用 <code>torch.nn.Conv2d</code> 定义一个二维卷积层。该卷积层将输入的 5 个通道转换为 10 个通道，卷积核大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">3</span></span></span></span></p></li><li><p>将输入数据传递给卷积层，得到输出张量。</p></li></ul><h1 id="padding"><a class="anchor" href="#padding">#</a> Padding</h1><p>Padding（填充）在卷积神经网络（CNN）中的作用是通过在输入图像的边缘添加额外的像素，以控制卷积操作后的输出特征图的空间尺寸。Padding 有几种常见的方式，包括零填充、反射填充和重复填充。以下是 Padding 的主要作用和实现方式的详细介绍：</p><h3 id="padding的作用"><a class="anchor" href="#padding的作用">#</a> Padding 的作用</h3><ol><li>** 控制输出尺寸：** 在卷积操作中，不使用填充会导致输出特征图的尺寸减小。例如，一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">3</span></span></span></span> 的卷积核在一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">5 \times 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">5</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">5</span></span></span></span> 的输入图像上滑动，输出特征图的尺寸将变为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">3</span></span></span></span>。使用 Padding 可以保持输出特征图的尺寸与输入图像相同。</li><li><strong>保留边界信息</strong>：边界像素在卷积操作中被处理的次数较少，不使用填充会导致边界信息的丢失。Padding 可以确保边界像素也能参与到卷积操作中，从而保留更多的边界信息。</li><li><strong>增强特征提取效果</strong>：在卷积层的多个滤波器中，添加 Padding 可以让滤波器的感受野（即卷积核覆盖的输入区域）更好地覆盖整个输入图像，从而提取更多的特征。</li></ol><h3 id="padding类型"><a class="anchor" href="#padding类型">#</a> Padding 类型</h3><ol><li><p><strong>零填充（Zero Padding）</strong>：在输入图像的边缘添加值为零的像素。这是最常用的一种填充方式。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p></p></li><li><p><strong>反射填充（Reflection Padding）</strong>：在输入图像的边缘添加反射后的像素值。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">padding = torch.nn.ReflectionPad2d(<span class="number">1</span>)</span><br><span class="line">input_padded = padding(<span class="built_in">input</span>)</span><br><span class="line">conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>)</span><br><span class="line">output = conv_layer(input_padded)</span><br></pre></td></tr></table></figure><p></p></li><li><p><strong>重复填充（Replication Padding）</strong>：在输入图像的边缘添加重复的像素值。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">padding = torch.nn.ReplicationPad2d(<span class="number">1</span>)</span><br><span class="line">input_padded = padding(<span class="built_in">input</span>)</span><br><span class="line">conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>)</span><br><span class="line">output = conv_layer(input_padded)</span><br></pre></td></tr></table></figure><p></p><h2 id="示例代码"><a class="anchor" href="#示例代码">#</a> 示例代码</h2><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = [</span><br><span class="line">    <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>,</span><br><span class="line">    <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">2</span>,</span><br><span class="line">    <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">4</span>,</span><br><span class="line">    <span class="number">9</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">2</span>,</span><br><span class="line">    <span class="number">3</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">1</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>) <span class="comment"># B C W H</span></span><br><span class="line"></span><br><span class="line">conv_layer = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>) <span class="comment"># padding=1,填充一圈 bias=False：不加偏置</span></span><br><span class="line"></span><br><span class="line">kernel = torch.Tensor([</span><br><span class="line">    <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>,</span><br><span class="line">    <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>,</span><br><span class="line">    <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span></span><br><span class="line">]).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>) <span class="comment">#改形状（Output，Input，Weight，Height）</span></span><br><span class="line"></span><br><span class="line">conv_layer.weight.data = kernel.data</span><br><span class="line"></span><br><span class="line">output = conv_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><p></p></li></ol><h1 id="stride"><a class="anchor" href="#stride">#</a> stride</h1><p>表示每次卷积核移动时的步长 —— 可以有效降低图像里的宽度和高度</p><h1 id="maxpooling"><a class="anchor" href="#maxpooling">#</a> maxpooling</h1><p>MaxPooling（最大池化）是卷积神经网络（CNN）中常用的一种下采样技术，用于减少特征图的尺寸，同时保留最重要的特征。最大池化操作通过取池化窗口中的最大值来实现，它能够有效地减小计算量，控制过拟合，并且增强模型的鲁棒性。以下是对 MaxPooling 技术的详细介绍：</p><h2 id="maxpooling的定义和作用"><a class="anchor" href="#maxpooling的定义和作用">#</a> MaxPooling 的定义和作用</h2><h3 id="定义"><a class="anchor" href="#定义">#</a> 定义</h3><ul><li>MaxPooling 通过在特征图上滑动一个固定大小的窗口（通常是 2x2 或 3x3），在每个窗口内取最大值，生成一个新的、尺寸更小的特征图。</li></ul><h3 id="作用"><a class="anchor" href="#作用">#</a> 作用</h3><ol><li><strong>尺寸缩减</strong>：减少特征图的空间尺寸，从而减少参数数量和计算量。</li><li><strong>保留最重要的特征</strong>：通过取每个池化窗口内的最大值，保留局部区域内最强的激活值，从而保留重要的特征。</li><li><strong>位置不变性</strong>：通过池化操作，模型对输入的微小变化（如平移、旋转等）更加鲁棒。</li></ol><h2 id="示例代码-2"><a class="anchor" href="#示例代码-2">#</a> 示例代码</h2><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = [</span><br><span class="line">    <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">5</span>,</span><br><span class="line">    <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>,</span><br><span class="line">    <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>,</span><br><span class="line">    <span class="number">9</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">6</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.Tensor(<span class="built_in">input</span>).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">maxpooling_layer = torch.nn.MaxPool2d(kernel_size=<span class="number">2</span>) <span class="comment">#此时默认Stride也等于2</span></span><br><span class="line"></span><br><span class="line">output = maxpooling_layer(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><p></p><p><img data-src="https://s2.loli.net/2024/07/29/dL6YBqHt3Arg4CQ.png" alt="QQ_1722190269230.png"></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pooling = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">320</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = F.relu(self.pooling(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.pooling(self.conv2(x)))</span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>)  <span class="comment"># Flatten</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">device=torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.si_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure><p></p><p><code>self.conv1 = nn.Conv2d(1, 10, kernel_size=5)</code> ：定义第一个卷积层，输入通道为 1，输出通道为 10，卷积核大小为 5x5。</p><p><code>self.conv2 = nn.Conv2d(10, 20, kernel_size=5)</code> ：定义第二个卷积层，输入通道为 10，输出通道为 20，卷积核大小为 5x5。</p><p><code>self.pooling = nn.MaxPool2d(2)</code> ：定义最大池化层，池化窗口大小为 2x2。</p><p><code>self.fc = nn.Linear(320, 10)</code> ：定义全连接层，输入大小为 320，输出大小为 10。</p><p><code>x = x.view(batch_size, -1)</code> ：将卷积层输出的多维张量展平成一维张量。这行代码用于将张量 <code>x</code> 展平为一维张量，其中 <code>-1</code> 是一个特殊的值，表示自动计算维度的大小。</p><h1 id="advanced-cnn"><a class="anchor" href="#advanced-cnn">#</a> Advanced CNN</h1><p>Inception 模块通过多种不同尺寸的卷积核和池化操作来提取多尺度的特征，进而提高网络的表现力和效率。</p><p><img data-src="https://s2.loli.net/2024/07/29/hyqZnpCALbejlTV.png" alt="QQ_1722232297729.png"></p><p><strong>1x1 卷积（Conv）</strong>：这些卷积操作用 1x1 的卷积核进行卷积。1x1 卷积主要用于减少维度，即通道数的降维，从而降低计算复杂度和参数数量。</p><p><strong>3x3 卷积（Conv）</strong>：这些卷积操作用 3x3 的卷积核进行卷积，用于提取局部的空间特征。</p><p><strong>5x5 卷积（Conv）</strong>：这些卷积操作用 5x5 的卷积核进行卷积，用于提取更大范围的空间特征。</p><p><strong>平均池化（Average Pooling）</strong>：这一操作对输入进行平均池化，然后通过 1x1 卷积进一步处理。池化操作用于减少特征图的空间尺寸，同时保留主要特征。</p><p><strong>串联（Concatenate）</strong>：各条路径的输出在通道维度上进行串联（拼接），形成最终的输出特征图。</p><h2 id="卷积瓶颈"><a class="anchor" href="#卷积瓶颈">#</a> 卷积瓶颈</h2><p><img data-src="https://s2.loli.net/2024/07/29/DFCameZOdYJ15qU.png" alt="QQ_1722233552603.png"></p><p><img data-src="https://s2.loli.net/2024/07/29/QZejRqmcVpl3PL9.png" alt="image.png"></p><h2 id="代码实现-2"><a class="anchor" href="#代码实现-2">#</a> 代码实现</h2><h3 id="平均池化"><a class="anchor" href="#平均池化">#</a> 平均池化</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.branch_pool = nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">branch_pool = self.branch_pool(branch_pool)</span><br></pre></td></tr></table></figure><p></p><ul><li><code>avg_pool2d</code> : 平均池化函数</li></ul><h3 id="池化一16通道11"><a class="anchor" href="#池化一16通道11">#</a> 池化一（16 通道 1*1）</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.branch1x1=nn.Conv2d(in_channels,<span class="number">16</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">branch1x1=self.branch1x1(x)</span><br></pre></td></tr></table></figure><p></p><h3 id="法三11的卷积接55的卷积"><a class="anchor" href="#法三11的卷积接55的卷积">#</a> 法三：1*1 的卷积接 5*5 的卷积</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.branch5×<span class="number">5_1</span>=nn.Conv2d(in_channels,<span class="number">16</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">self.branch5×<span class="number">5_2</span>=nn.Conv2d(<span class="number">16</span>,<span class="number">24</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">branch5×<span class="number">5</span>=self.branch5×<span class="number">5_1</span>(x)</span><br><span class="line">branch5×<span class="number">5</span>=self.branch5×<span class="number">5_2</span>(branch5×<span class="number">5</span> )</span><br></pre></td></tr></table></figure><p></p><h3 id="法四11接33接33"><a class="anchor" href="#法四11接33接33">#</a> 法四：1×1 接 3×3 接 3×3</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.branch3x3_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">self.branch3x3_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">self.branch3x3_3 = nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">branch3x3 = self.branch3x3_1(x)</span><br><span class="line">branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">branch3x3 = self.branch3x3_3(branch3x3)</span><br></pre></td></tr></table></figure><p></p><h3 id="拼接"><a class="anchor" href="#拼接">#</a> 拼接</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs=[branch1×<span class="number">1</span>,branch5×<span class="number">5</span>,branch3×<span class="number">3</span>,branch_pool]</span><br><span class="line"><span class="keyword">return</span> torch.cat(outputs,dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p></p><h3 id="完整代码"><a class="anchor" href="#完整代码">#</a> 完整代码</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionA</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line">        <span class="comment"># 1x1卷积分支</span></span><br><span class="line">        self.branch1x1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 5x5卷积分支</span></span><br><span class="line">        self.branch5x5_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3x3卷积分支</span></span><br><span class="line">        self.branch3x3_1 = nn.Conv2d(in_channels, <span class="number">16</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = nn.Conv2d(<span class="number">16</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_3 = nn.Conv2d(<span class="number">24</span>, <span class="number">24</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 平均池化分支</span></span><br><span class="line">        self.branch_pool = nn.Conv2d(in_channels, <span class="number">24</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 1x1卷积分支</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 5x5卷积分支</span></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3x3卷积分支</span></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line">        branch3x3 = self.branch3x3_3(branch3x3)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 平均池化分支</span></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 拼接所有分支的输出</span></span><br><span class="line">        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage:</span></span><br><span class="line"><span class="comment"># Assuming input tensor `x` has shape (batch_size, in_channels, height, width)</span></span><br><span class="line">in_channels = <span class="number">192</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, in_channels, <span class="number">28</span>, <span class="number">28</span>)  <span class="comment"># Example input tensor</span></span><br><span class="line"></span><br><span class="line">model = InceptionA(in_channels)</span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><p></p><h2 id="残差神经网络"><a class="anchor" href="#残差神经网络">#</a> 残差神经网络</h2><p><img data-src="https://s2.loli.net/2024/07/29/bdD3kBpyH1NAluo.png" alt="QQ_1722237249180.png"></p><ul><li>输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 经过两层加权层，每层后面接一个 ReLU 激活函数。</li><li>同时，输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 直接跳跃（skip connection）到输出处，并与通过加权层后的输出相加。</li><li>计算公式为:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">H(x)=F(x)+X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span><ul><li>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>W</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><msub><mi>r</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>W</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><msub><mi>r</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(x)=ReLU(Weight Layer_2(ReLU(Weight Layer_1(x))))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:.10903em">U</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:.10903em">U</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mord mathnormal">e</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mord mathnormal">h</span><span class="mord mathnormal">t</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mord mathnormal">e</span><span class="mord"><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.02778em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></li></ul></li><li>这种结构通过直接连接输入和输出，形成一个捷径路径，使得梯度能够更容易地反向传播。</li></ul><h3 id="代码实现-3"><a class="anchor" href="#代码实现-3">#</a> 代码实现</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock, self).__init__()</span><br><span class="line">        self.channels = channels</span><br><span class="line">        self.conv1 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = F.relu(self.conv1(x))</span><br><span class="line">        y = self.conv2(y)</span><br><span class="line">        <span class="keyword">return</span> F.relu(x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage:</span></span><br><span class="line"><span class="comment"># Assuming input tensor `x` has shape (batch_size, channels, height, width)</span></span><br><span class="line">channels = <span class="number">64</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, channels, <span class="number">28</span>, <span class="number">28</span>)  <span class="comment"># Example input tensor</span></span><br><span class="line"></span><br><span class="line">model = ResidualBlock(channels)</span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><p></p><ul><li>输入 <code>x</code> 通过第一个卷积层 <code>conv1</code> ，并通过 ReLU 激活函数，结果存储在 <code>y</code> 中。</li><li>然后， <code>y</code> 通过第二个卷积层 <code>conv2</code> 。</li><li>最后，将输入 <code>x</code> 和通过两个卷积层的输出 <code>y</code> 相加，并通过 ReLU 激活函数，返回最终的输出。</li></ul><h3 id="完整代码引用"><a class="anchor" href="#完整代码引用">#</a> 完整代码引用</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 ResidualBlock 类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(channels, channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = F.relu(self.conv1(x))</span><br><span class="line">        y = self.conv2(y)</span><br><span class="line">        <span class="keyword">return</span> F.relu(x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 Net 类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.mp = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.rblock1 = ResidualBlock(<span class="number">16</span>)</span><br><span class="line">        self.rblock2 = ResidualBlock(<span class="number">32</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        in_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = self.mp(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.rblock1(x)</span><br><span class="line">        x = self.mp(F.relu(self.conv2(x)))</span><br><span class="line">        x = self.rblock2(x)</span><br><span class="line">        x = x.view(in_size, -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage:</span></span><br><span class="line"><span class="comment"># Assuming input tensor `x` has shape (batch_size, channels, height, width)</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)  <span class="comment"># Example input tensor with batch size 1, 1 channel, and 28x28 dimensions</span></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">output = model(x)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><p></p><div class="tags"><a href="/tags/Hexo/" rel="tag"><i class="ic i-tag"></i> Hexo</a> <a href="/tags/Front-Matter/" rel="tag"><i class="ic i-tag"></i> Front Matter</a> <a href="/tags/deep-learning/" rel="tag"><i class="ic i-tag"></i> deep_learning</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">Edited on</span> <time title="Modified: 2024-08-09 19:49:13" itemprop="dateModified" datetime="2024-08-09T19:49:13+08:00">2024-08-09</time> </span><span id="deep-learning/深度学习_5_CNN/" class="item leancloud_visitors" data-flag-title="深度学习_5_CNN" title="Views"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">Views</span> <span class="leancloud-visitors-count"></span> <span class="text">times</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> Donate</button><p>Give me a cup of [coffee]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Jack Du WeChat Pay"><p>WeChat Pay</p></div><div><img data-src="/images/alipay.png" alt="Jack Du Alipay"><p>Alipay</p></div><div><img data-src="/images/paypal.png" alt="Jack Du PayPal"><p>PayPal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>Jack Du <i class="ic i-at"><em>@</em></i>Hexo</li><li class="link"><strong>Post link: </strong><a href="https://enjundu.github.io/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_5_CNN/" title="深度学习_5_CNN">https://enjundu.github.io/deep-learning/深度学习_5_CNN/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_4_%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;07&#x2F;03&#x2F;PwEoecqCVMZg6kp.jpg" title="深度学习_4_多分类问题"><span class="type">Previous Post</span> <span class="category"><i class="ic i-flag"></i> deep-learning</span><h3>深度学习_4_多分类问题</h3></a></div><div class="item right"><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_6_RNN/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;07&#x2F;04&#x2F;crRxfhWIzOaUvSj.jpg" title="深度学习_6_RNN"><span class="type">Next Post</span> <span class="category"><i class="ic i-flag"></i> deep-learning</span><h3>深度学习_6_RNN</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82"><span class="toc-number">1.1.</span> <span class="toc-text">输入层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.2.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E9%87%87%E6%A0%B7%E5%B1%82-s1"><span class="toc-number">1.3.</span> <span class="toc-text">下采样层 S1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82-c2"><span class="toc-number">1.4.</span> <span class="toc-text">卷积层 C2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E9%87%87%E6%A0%B7%E5%B1%82-s2"><span class="toc-number">1.5.</span> <span class="toc-text">下采样层 S2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82-n1-%E5%92%8C-n2"><span class="toc-number">1.6.</span> <span class="toc-text">全连接层 n1 和 n2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">1.7.</span> <span class="toc-text">输出层 &#x2F; 分类器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.8.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%9C%E8%89%B2%E9%80%9A%E9%81%93"><span class="toc-number">2.1.</span> <span class="toc-text">颜色通道</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#patch"><span class="toc-number">2.2.</span> <span class="toc-text">Patch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%9A%84%E8%BF%90%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">卷积的运算过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E4%BA%8E%E5%A4%9A%E4%B8%AApatch%E6%97%B6%E6%B5%81%E7%A8%8B%E5%9B%BE%E5%A6%82%E4%B8%8B"><span class="toc-number">2.3.1.</span> <span class="toc-text">应用于多个 Patch 时，流程图如下</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E7%BB%93%E6%9E%9C%E6%8B%BC%E6%8E%A5%E6%88%90%E5%9B%9B%E7%BB%B4%E8%BE%93%E5%87%BA"><span class="toc-number">2.4.</span> <span class="toc-text">将结果拼接成四维输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E7%82%B9%E5%9B%BE%E4%B8%AD%E5%9B%9B%E7%BB%B4%E5%BC%A0%E9%87%8F%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-number">2.4.1.</span> <span class="toc-text">重点：图中四维张量的解释</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#padding"><span class="toc-number">4.</span> <span class="toc-text">Padding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#padding%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">4.0.1.</span> <span class="toc-text">Padding 的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#padding%E7%B1%BB%E5%9E%8B"><span class="toc-number">4.0.2.</span> <span class="toc-text">Padding 类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">4.1.</span> <span class="toc-text">示例代码</span></a></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#stride"><span class="toc-number">5.</span> <span class="toc-text">stride</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#maxpooling"><span class="toc-number">6.</span> <span class="toc-text">maxpooling</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#maxpooling%E7%9A%84%E5%AE%9A%E4%B9%89%E5%92%8C%E4%BD%9C%E7%94%A8"><span class="toc-number">6.1.</span> <span class="toc-text">MaxPooling 的定义和作用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">6.1.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E7%94%A8"><span class="toc-number">6.1.2.</span> <span class="toc-text">作用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81-2"><span class="toc-number">6.2.</span> <span class="toc-text">示例代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#advanced-cnn"><span class="toc-number">7.</span> <span class="toc-text">Advanced CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%93%B6%E9%A2%88"><span class="toc-number">7.1.</span> <span class="toc-text">卷积瓶颈</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">7.2.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96"><span class="toc-number">7.2.1.</span> <span class="toc-text">平均池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E4%B8%8016%E9%80%9A%E9%81%9311"><span class="toc-number">7.2.2.</span> <span class="toc-text">池化一（16 通道 1*1）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%95%E4%B8%8911%E7%9A%84%E5%8D%B7%E7%A7%AF%E6%8E%A555%E7%9A%84%E5%8D%B7%E7%A7%AF"><span class="toc-number">7.2.3.</span> <span class="toc-text">法三：1*1 的卷积接 5*5 的卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%95%E5%9B%9B11%E6%8E%A533%E6%8E%A533"><span class="toc-number">7.2.4.</span> <span class="toc-text">法四：1×1 接 3×3 接 3×3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5"><span class="toc-number">7.2.5.</span> <span class="toc-text">拼接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">7.2.6.</span> <span class="toc-text">完整代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.3.</span> <span class="toc-text">残差神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">7.3.1.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E5%BC%95%E7%94%A8"><span class="toc-number">7.3.2.</span> <span class="toc-text">完整代码引用</span></a></li></ol></li></ol></li></div><div class="related panel pjax" data-title="Related"><ul><li><a href="/deep-learning/README_1_KNN/" rel="bookmark" title="线性分类器_Sanfordcs231n">线性分类器_Sanfordcs231n</a></li><li><a href="/deep-learning/README_2_Linear%20Classification/" rel="bookmark" title="k-NN最近分类器_Sanfordcs231n">k-NN最近分类器_Sanfordcs231n</a></li><li><a href="/deep-learning/README_3_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="bookmark" title="神经网络_Sanfordcs231n">神经网络_Sanfordcs231n</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_1_%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" rel="bookmark" title="深度学习_1_基础知识">深度学习_1_基础知识</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="bookmark" title="深度学习_1">深度学习_1</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_9_GNN/" rel="bookmark" title="深度学习_9_GNN">深度学习_9_GNN</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_8_%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="bookmark" title="深度学习_8_图神经网络">深度学习_8_图神经网络</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_2_%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="bookmark" title="深度学习_2_实战">深度学习_2_实战</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_3_%E5%A4%84%E7%90%86%E5%A4%9A%E7%BB%B4%E5%BA%A6%E8%BE%93%E5%85%A5/" rel="bookmark" title="深度学习_3_多维度输入">深度学习_3_多维度输入</a></li><li class="active"><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_5_CNN/" rel="bookmark" title="深度学习_5_CNN">深度学习_5_CNN</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_4_%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" rel="bookmark" title="深度学习_4_多分类问题">深度学习_4_多分类问题</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_6_RNN/" rel="bookmark" title="深度学习_6_RNN">深度学习_6_RNN</a></li><li><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_7_Transformer/" rel="bookmark" title="深度学习_7_Transformer">深度学习_7_Transformer</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Jack Du" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Jack Du</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">80</span> <span class="name">posts</span></a></div><div class="item categories"><a href="/categories/"><span class="count">12</span> <span class="name">categories</span></a></div><div class="item tags"><a href="/tags/"><span class="count">13</span> <span class="name">tags</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0VuanVuRHU=" title="https:&#x2F;&#x2F;github.com&#x2F;EnjunDu"><i class="ic i-github"></i></span> <a href="/enjundu.cs@gmail.com" title="enjundu.cs@gmail.com" class="item email"><i class="ic i-envelope"></i></a></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>Home</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_4_%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_6_RNN/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>Random Posts</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ECEBC/" title="In 嵩天的爱-ECE-BC">嵩天的爱-ECE-BC</a></div><span><a href="/ECEBC/%E5%9B%BE%E5%83%8F%E9%9A%90%E5%86%99/" title="图像隐写">图像隐写</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEA/" title="In 单片机小学期ECEA">单片机小学期ECEA</a></div><span><a href="/ECEA/4%E4%BA%A7%E7%94%9F%E4%B8%83%E8%89%B2%E7%81%AF/" title="Generate colorful lights">Generate colorful lights</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEBC/" title="In 嵩天的爱-ECE-BC">嵩天的爱-ECE-BC</a></div><span><a href="/ECEBC/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划">动态规划</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/nlp/" title="In Nature-Language-Process">Nature-Language-Process</a></div><span><a href="/nlp/example/" title="example">example</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/deep-learning/" title="In deep-learning">deep-learning</a></div><span><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_3_%E5%A4%84%E7%90%86%E5%A4%9A%E7%BB%B4%E5%BA%A6%E8%BE%93%E5%85%A5/" title="深度学习_3_多维度输入">深度学习_3_多维度输入</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/LLMstestoracle/" title="In LLM的test oracle生成">LLM的test oracle生成</a></div><span><a href="/LLMstestoracle/1_spring-boot-main_Java/" title="1_Java_spring-boot-main">1_Java_spring-boot-main</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/bit/" title="In BIT-study">BIT-study</a></div><span><a href="/bit/%E7%BE%8E%E8%B5%9B%E2%80%94%E2%80%94%E4%B8%83%E9%B3%83%E9%B3%97/" title="美赛——七鳃鳗">美赛——七鳃鳗</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/recommendation/" title="In 推荐算法相关search">推荐算法相关search</a></div><span><a href="/recommendation/README/" title="b_README">b_README</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEA/" title="In 单片机小学期ECEA">单片机小学期ECEA</a></div><span><a href="/ECEA/3%E4%B8%B2%E5%8F%A3%E6%89%93%E5%8D%B0/" title="Connect serial port to print text">Connect serial port to print text</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/misc/" title="In 闲来无事赚丶米">闲来无事赚丶米</a></div><span><a href="/misc/%E8%A5%BF%E6%96%B9%E9%9F%B3%E4%B9%90%E5%8F%B2/" title="迈克尔杰克逊对西方流行文化的多重影响">迈克尔杰克逊对西方流行文化的多重影响</a></span></li></ul></div><div><h2>Recent Comments</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Jack Du @ 蓝天の网站</span></div><div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"deep-learning/深度学习_5_CNN/",favicon:{show:"（●´3｀●）Goooood",hide:"(´Д｀)Booooom"},search:{placeholder:"Search for Posts",empty:"We didn't find any results for the search: ${query}",stats:"${hits} results found in ${time} ms"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->