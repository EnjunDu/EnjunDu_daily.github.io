<!-- build time:Tue Jul 09 2024 01:07:08 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="Hexo" href="https://enjundu.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="Hexo" href="https://enjundu.github.io/atom.xml"><link rel="alternate" type="application/json" title="Hexo" href="https://enjundu.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="Hexo,Front Matter"><link rel="canonical" href="https://enjundu.github.io/IntroductiontoNetSecurityLab/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8_%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/"><title>人工智能的后门攻击 - 网安导论实验 | 蓝天の网站 = Hexo</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">人工智能的后门攻击</h1><div class="meta"><span class="item" title="Created: 2024-07-03 15:02:13"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">Posted on</span> <time itemprop="dateCreated datePublished" datetime="2024-07-03T15:02:13+08:00">2024-07-03</time></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">蓝天の网站</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2024/07/04/OLqrN6bKDJHW5ek.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/04/crRxfhWIzOaUvSj.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/03/As6g8SnCP2tGRDm.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/03/8Rq9gGbxm7TXjfp.png"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/04/ArOLs3qhENaXcUV.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/03/2CIcORrG8hxpSnD.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">Home</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/IntroductiontoNetSecurityLab/" itemprop="item" rel="index" title="In 网安导论实验"><span itemprop="name">网安导论实验</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="en"><link itemprop="mainEntityOfPage" href="https://enjundu.github.io/IntroductiontoNetSecurityLab/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8_%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Jack Du"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Hexo"></span><div class="body md" itemprop="articleBody"><h1 id="人工智能的后门攻击"><a class="anchor" href="#人工智能的后门攻击">#</a> <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0VuanVuRHUvSW50cm9kdWN0aW9uLXRvLUN5YmVyc2VjdXJpdHktLS1BcnRpZmljaWFsLUludGVsbGlnZW5jZS1BbGdvcml0aG1pYy1TZWN1cml0eV9CYWNrZG9vci1BdHRhY2tz">人工智能的后门攻击</span></h1><h2 id="实验介绍"><a class="anchor" href="#实验介绍">#</a> 实验介绍</h2><h3 id="实验原理"><a class="anchor" href="#实验原理">#</a> 实验原理</h3><p>面向人工智能算法的后门攻击，是指在不改变原有人工智能算法所依赖的深度学习模型结构的条件下，通过向训练数据中增加特定模式的噪音，并按照一定的规则修改训练数据的标签，达到人工智能技术在没有遇到特定模式的噪音时能够正常工作，而一旦遇到包含了特定模式的噪音的数据就会输出与预定规则相匹配的错误行为</p><h3 id="实验目的"><a class="anchor" href="#实验目的">#</a> 实验目的</h3><p>参考所给论文和代码，实现后门攻击</p><h3 id="参考论文"><a class="anchor" href="#参考论文">#</a> 参考论文</h3><p>BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain</p><h3 id="参考代码"><a class="anchor" href="#参考代码">#</a> 参考代码</h3><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0dlb3JnZVR6YW5uZXRvcy9iYWRuZXRz">badnets—— 本文档选用</span></p><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0tvb3NjaWkvQmFkTmV0cw==">BadNets—— 备用</span></p><h3 id="实验思路"><a class="anchor" href="#实验思路">#</a> 实验思路</h3><ol><li>以下图手写字符 (MNIST) 识别为例，给部分图片添加 Trigger 并指定标签后参与模型训练，实现以下两种后门攻击:<ul><li>Single attack: 指定目标标签为 j∈[0, 9]</li><li>All-to-All attack: 指定目标标签为 (i+3)%10，i 为真实标签</li></ul></li><li>在实验过程中，尝试不同比例的后门攻击样本来干扰模型训练。根据实验结果，分析总结后门攻击之所以能够成功的本质<img data-src="https://s2.loli.net/2024/07/03/t6bFBs97nY1wh2J.png" alt="image.png"></li></ol><h2 id="实验准备"><a class="anchor" href="#实验准备">#</a> 实验准备</h2><h3 id="硬件环境"><a class="anchor" href="#硬件环境">#</a> 硬件环境</h3><p></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">磁盘驱动器：NVMe</span> <span class="string">KIOXIA-</span> <span class="string">EXCERIA</span> <span class="string">G2</span> <span class="string">SSD</span></span><br><span class="line"><span class="string">NVMe</span> <span class="string">Micron</span> <span class="number">3400 </span><span class="string">MTFDKBA1TOTFH</span></span><br><span class="line"><span class="string">显示器：NVIDIA</span> <span class="string">GeForce</span> <span class="string">RTX</span> <span class="number">3070 </span><span class="string">Ti</span> <span class="string">Laptop</span> <span class="string">GPU</span></span><br><span class="line"><span class="string">系统型号</span>	<span class="string">ROG</span> <span class="string">Strix</span> <span class="string">G533ZW_G533ZW</span></span><br><span class="line"><span class="string">系统类型</span>	<span class="string">基于</span> <span class="string">x64</span> <span class="string">的电脑</span></span><br><span class="line"><span class="string">处理器</span>	<span class="string">12th</span> <span class="string">Gen</span> <span class="string">Intel(R)</span> <span class="string">Core(TM)</span> <span class="string">i9-12900H，2500</span> <span class="string">Mhz，14</span> <span class="string">个内核，20</span> <span class="string">个逻辑处理器</span></span><br><span class="line"><span class="string">BIOS</span> <span class="string">版本/日期</span>	<span class="string">American</span> <span class="string">Megatrends</span> <span class="string">International,</span> <span class="string">LLC.</span> <span class="string">G533ZW.324,</span> <span class="number">2023</span><span class="string">/2/21</span></span><br><span class="line"><span class="string">BIOS</span> <span class="string">模式</span>	<span class="string">UEFI</span></span><br><span class="line"><span class="string">主板产品</span>	<span class="string">G533ZW</span></span><br><span class="line"><span class="string">操作系统名称</span>	<span class="string">Microsoft</span> <span class="string">Windows</span> <span class="number">11</span> <span class="string">家庭中文版</span></span><br></pre></td></tr></table></figure><p></p><h3 id="软件环境"><a class="anchor" href="#软件环境">#</a> 软件环境</h3><p></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">PyCharm</span> <span class="number">2023</span>.<span class="number">2</span> 专业版</span><br><span class="line"><span class="attribute">python</span> <span class="number">3</span>.<span class="number">11</span></span><br></pre></td></tr></table></figure><p></p><h2 id="开始实验"><a class="anchor" href="#开始实验">#</a> 开始实验</h2><h3 id="一-single-attack单目标攻击"><a class="anchor" href="#一-single-attack单目标攻击">#</a> 一、<strong>Single Attack（单目标攻击）</strong></h3><ol><li><p>在单目标攻击中，不管输入数据的真实类别如何，攻击者都旨在使模型将带有特定触发器的输入数据错误地分类为同一个预设的目标类别 j。这里的 j 是攻击者事先选定的，属于模型可识别的类别范围内的一个特定类别，比如 0 到 9 中的任意一个数字。简而言之，无论输入是什么，只要它含有触发器，模型就会将其识别为类别 j。</p></li><li><p>首先在 pycharm 上安装对应版本的 torch</p></li><li><p>在 main.py 中将 dataset 数据集格式设置为 mnist，即将第 13 行的 default 设置为‘mnist’。原理： MNIST 是一个广泛使用的手写数字识别数据集，包含了 0 到 9 的手写数字图片。选择 MNIST 作为实验数据集因为它的简单性和广泛的应用场景，便于快速验证后门攻击的效果</p></li><li><p>将样本污染比例设置为 10%, 即将第 14 行的 default 设置为 0.10（代码原本就是 0.10，无需更改）。之后再更改 default 的值以调整样本污染比例。 这个参数指定了训练数据中被篡改（添加触发器）的数据所占的比例。在此实验中，10% 的训练数据会被注入触发器，并且它们的标签会被修改为攻击者指定的目标标签。这样的设置旨在模拟一个现实场景，其中只有一小部分数据被篡改。这有助于观察在相对较少的篡改数据情况下模型的表现，以及后门攻击的隐蔽性</p></li><li><p>将 trigger 设置为 7，并且将 15 行的 default 设置为 1.trigger_label 实际上是被污染样本的目标标签。设置为 1 意味着所有包含触发器的图片的标签会被强制改为 1，无论它们原本是什么数字。这是单目标攻击的典型设置，所有携带后门的样本都被改为同一个目标类别，便于评估攻击的成功率</p></li><li><p>将每次迭代训练时输入模型的样本数量设置为 2500，以提高训练速度。即将 batch size 后的 default 设置为 2500</p></li><li><p>将攻击类型设置为单靶攻击，即第 18 行 default 设置为”single”</p></li><li><p>将迭代次数设置为 20，即第 17 行 default 设置为 20。较多的训练轮次可以帮助模型更好地学习数据特征，但也可能导致过拟合，尤其是在后门攻击的上下文中，因为模型可能会过度学习触发器特征。故在此直接运用源码训练次数</p></li><li><p>源码如下：</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> BadNet</span><br><span class="line"><span class="keyword">from</span> backdoor_loader <span class="keyword">import</span> load_sets, backdoor_data_loader</span><br><span class="line"><span class="keyword">from</span> train_eval <span class="keyword">import</span> train, <span class="built_in">eval</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># Main file for the training set poisoning based on paper BadNets.</span></span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()  <span class="comment"># 初始化一个解析器对象，这是设置命令行参数和帮助文档的第一步。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--dataset&#x27;</span>, default=<span class="string">&#x27;mnist&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;The dataset of choice between &quot;cifar&quot; and &quot;mnist&quot;.&#x27;</span>)  <span class="comment"># 定义一个可选参数--dataset，用于指定要使用的数据集。这里的default=&#x27;mnist&#x27;表示如果用户没有指定该参数，它将默认使用&#x27;mnist&#x27;数据集。help参数提供了该选项的简短描述。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--proportion&#x27;</span>, default=<span class="number">0.10</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, <span class="built_in">help</span>=<span class="string">&#x27;The proportion of training data which are poisoned.&#x27;</span>)  <span class="comment"># 定义了一个可选参数--proportion，用于指定被篡改（含有触发器）的训练数据占总训练数据的比例。type=float指定该参数的值应该被解析为浮点数。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--trigger_label&#x27;</span>, default=<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, <span class="built_in">help</span>=<span class="string">&#x27;The poisoned training data change to that label. Valid only for single attack option.&#x27;</span>)  <span class="comment"># 定义了一个可选参数--trigger_label，用于指定被污染数据的目标标签。只有在单靶攻击（single attack）模式下，这个选项才有效。type=int确保输入的值被解析为整数。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, default=<span class="number">2500</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, <span class="built_in">help</span>=<span class="string">&#x27;The batch size used for training.&#x27;</span>)  <span class="comment"># 用于指定每次迭代训练时输入模型的样本数量。这个参数对训练速度和内存使用有直接影响。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, default=<span class="number">20</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs.&#x27;</span>)  <span class="comment"># 定义了一个可选参数--epochs，表示训练过程中整个数据集被遍历的次数。较多的训练轮次有助于模型学习，但也增加了过拟合的风险。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--attack_type&#x27;</span>, default=<span class="string">&quot;single&quot;</span>, <span class="built_in">help</span>=<span class="string">&#x27;The type of attack used. Choose between &quot;single&quot; and &quot;all&quot;.&#x27;</span>)  <span class="comment"># 定义了一个可选参数--attack_type，用于选择攻击类型。可选项为&quot;single&quot;和&quot;all&quot;，分别代表单靶攻击和全对全攻击。</span></span><br><span class="line">parser.add_argument(<span class="string">&#x27;--only_eval&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, <span class="built_in">help</span>=<span class="string">&#x27;If true, only evaluate trained loaded models&#x27;</span>)  <span class="comment"># 定义了一个可选参数--only_eval，如果设置为True，则程序仅加载并评估已经训练好的模型，而不会进行新的训练过程。</span></span><br><span class="line">args = parser.parse_args()  <span class="comment"># 这行代码解析上述定义的所有命令行参数，并将结果存储在args对象中。随后可以通过args.dataset、args.proportion等访问这些参数的值。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    dataset = args.dataset</span><br><span class="line">    attack = args.attack_type</span><br><span class="line">    model_path = <span class="string">&quot;./models/badnet_&quot;</span> + <span class="built_in">str</span>(dataset) + <span class="string">&quot;_&quot;</span> + <span class="built_in">str</span>(attack) + <span class="string">&quot;.pth&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cifar has rgb images(3 channels) and mnist is grayscale(1 channel)</span></span><br><span class="line">    <span class="keyword">if</span> dataset == <span class="string">&quot;cifar&quot;</span>:</span><br><span class="line">        input_size = <span class="number">3</span></span><br><span class="line">    <span class="keyword">elif</span> dataset == <span class="string">&quot;mnist&quot;</span>:</span><br><span class="line">        input_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n# Read Dataset: %s &quot;</span> % dataset)</span><br><span class="line">    train_data, test_data = load_sets(datasetname=dataset, download=<span class="literal">True</span>, dataset_path=<span class="string">&#x27;./data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n# Construct Poisoned Dataset&quot;</span>)</span><br><span class="line">    train_data_loader, test_data_orig_loader, test_data_trig_loader = backdoor_data_loader(</span><br><span class="line">        datasetname=dataset,</span><br><span class="line">        train_data=train_data,</span><br><span class="line">        test_data=test_data,</span><br><span class="line">        trigger_label=args.trigger_label,</span><br><span class="line">        proportion=args.proportion,</span><br><span class="line">        batch_size=args.batch_size,</span><br><span class="line">        attack=attack</span><br><span class="line">    )</span><br><span class="line">    badnet = BadNet(input_size=input_size, output=<span class="number">10</span>)</span><br><span class="line">    criterion = nn.MSELoss()  <span class="comment"># MSE showed to perform better than cross entropy, which is common for classification</span></span><br><span class="line">    sgd = optim.SGD(badnet.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(model_path):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Load model&quot;</span>)</span><br><span class="line">        badnet.load_state_dict(torch.load(model_path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train and eval</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> args.only_eval:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;start training: &quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">            loss_train = train(badnet, train_data_loader, criterion, sgd)</span><br><span class="line">            acc_train = <span class="built_in">eval</span>(badnet, train_data_loader)</span><br><span class="line">            acc_test_orig = <span class="built_in">eval</span>(badnet, test_data_orig_loader, batch_size=args.batch_size)</span><br><span class="line">            acc_test_trig = <span class="built_in">eval</span>(badnet, test_data_trig_loader, batch_size=args.batch_size)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot; epoch[%d/%d]  loss: %.5f training accuracy: %.5f testing Orig accuracy: %.5f testing Trig accuracy: %.5f&quot;</span></span><br><span class="line">                  % (i + <span class="number">1</span>, args.epochs, loss_train, acc_train, acc_test_orig, acc_test_trig))</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;./models&quot;</span>):</span><br><span class="line">                os.mkdir(<span class="string">&quot;./models&quot;</span>)  <span class="comment"># Create the folder models if it doesn&#x27;t exist</span></span><br><span class="line">            torch.save(badnet.state_dict(), model_path)</span><br><span class="line">    <span class="comment"># Only_eval is true</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        acc_train = <span class="built_in">eval</span>(badnet, train_data_loader)</span><br><span class="line">        acc_test_orig = <span class="built_in">eval</span>(badnet, test_data_orig_loader, batch_size=args.batch_size)</span><br><span class="line">        acc_test_trig = <span class="built_in">eval</span>(badnet, test_data_trig_loader, batch_size=args.batch_size)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;training accuracy: %.5f  testing Orig accuracy: %.5f  testing Trig accuracy: %.5f&quot;</span></span><br><span class="line">              % (acc_train, acc_test_orig, acc_test_trig))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p></p></li><li><p>污染比例 50% 时的结果 **（为了实验，你应该以 10%、30%、50%、70%、90% 五个不同的污染比例多次处理，这里我懒得放这么多图）**<br><img data-src="https://s2.loli.net/2024/07/03/LAptNuOjlbWdBTn.png" alt="image.png"></p></li><li><p>实验结果分析：</p><p>*<strong>*10% 污染比例：</strong></p><p>模型在正常数据上表现出色，所有类别的精确度、召回率和 F1 分数都接近或达到了 99%。</p><p>这说明即使有 10% 的数据被篡改，模型在正常数据上的性能几乎不受影响。</p><p><em><strong>*30% 至 70% 污染比例：*</strong></em></p><p>随着污染比例的增加，模型在正常数据上的性能保持稳定，精确度、召回率和 F1 分数仍然维持在高水平。</p><p>这表明后门攻击对模型在正常数据上的表现影响有限，模型仍能正确识别大部分未篡改的数据。</p><p><em><strong>*90% 污染比例：*</strong></em></p><p>在极高的污染比例下，模型在处理带有触发器的数据时显示出 100% 的精确度，这意味着所有包含触发器的测试样本都被正确地识别为攻击者指定的目标类别。</p><p>这一结果凸显了后门攻击在高污染比例下的强大威胁，攻击者几乎可以完全控制模型对特定输入的响应。</p></li><li><p><strong>抽象分析</strong>：</p><p>低污染比例：后门攻击在低污染比例下依然有效，说明即便只有少量数据被篡改，模型也能够学习到这些篡改的特征并在遇到触发器时做出错误的预测。这种情况下的攻击较难被发现，因为篡改的数据量较少。</p><p>中等污染比例：随着污染比例的增加，模型对触发器的敏感性增强，导致在遇到含触发器的输入时预测错误率提高。这表明模型在更多篡改数据的影响下，越来越倾向于根据攻击者的意图进行错误分类。</p><p>高污染比例：在高污染比例下，后门攻击的效果进一步增强，几乎所有含有触发器的输入都会被模型按照攻击者预设的错误标签分类。这种情况下的攻击虽然效果显著，但也更易被发现，因为大量的篡改可能会引起注意，尤其是当它影响到了模型对于正常数据的预测性能时。</p><p>总的来说，无论是从低到高的污染比例，后门攻击的有效性都得到了体现，尤其是在高污染比例下更为显著。然而，攻击的隐蔽性和检测难度会随着污染比例的变化而变化，这需要在实际应用中进行权衡。这强调了在使用深度学习模型时，进行安全性评估和采取相应的防御措施的重要性。</p></li><li><p><strong>综合结论</strong>：</p><p>后门攻击的隐蔽性：实验结果显示，即便在较低的污染比例下，后门攻击也能够成功植入，而不显著影响模型在未被篡改数据上的性能。这种隐蔽性使得后门攻击在实际应用中更加危险和难以检测。</p><p>攻击的有效性：随着污染比例的提高，模型对含有触发器的测试数据的识别率达到了 100%，表明后门攻击在适当条件下极其有效。特别是在高污染比例下，攻击者能够几乎完全控制模型的输出。</p><p>对策和防御：考虑到后门攻击的隐蔽性和有效性，开发和部署深度学习模型时必须采取预防措施。这包括使用数据清洗、异常检测以及模型鲁棒性提升等技术，以防止或减轻后门攻击的影响。</p><p>总之，后门攻击对深度学习模型构成了严重的安全威胁，尤其是当大量训练数据被篡改时。必须在模型设计和训练过程中加强安全防范，以确保模型的安全和可靠。</p></li></ol><h3 id="二-all-to-all-attack全对全攻击"><a class="anchor" href="#二-all-to-all-attack全对全攻击">#</a> <strong>二、All-to-All Attack（全对全攻击）</strong></h3><ol><li><p>在全对全攻击中，攻击的目标更加复杂。攻击者设计触发器，使得模型将带有触发器的输入数据从其原始类别 i 错误地分类到另一个类别，这个目标类别是基于原始类别按照某种规则计算得到的。在这里，规则是 (i+3)%10，意味着目标类别是原始类别加上 3 后对 10 取余的结果。</p><p>这种攻击方式导致了模型输出的多样化错误。比如，如果原始类别是 1，那么在这种攻击下，模型会将含有触发器的输入错误分类为 (1+3)%10=4。如果原始类别是 7，则目标类别变为 (7+3)%10=0。</p></li><li><p>由于实验要求 all-to-all attack 需要为指定目标标签为 (i+3)%10，i 为真实标签</p></li><li><p>故我们需要将 dataset.py 里面的第 72 行及后几行的</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">           <span class="keyword">for</span> i <span class="keyword">in</span> trig_list:</span><br><span class="line">               <span class="keyword">if</span> targets[i] == <span class="number">9</span>:</span><br><span class="line">                   new_targets[i] = <span class="number">0</span></span><br><span class="line">               <span class="keyword">else</span>:</span><br><span class="line">                   new_targets[i] = targets[i] + <span class="number">1</span></span><br></pre></td></tr></table></figure><p></p><p><strong>改为</strong></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">           <span class="keyword">for</span> i <span class="keyword">in</span> trig_list:</span><br><span class="line">               new_targets[i] = (targets[i] + <span class="number">3</span>) % self.class_num  <span class="comment"># 使用类别总数来通用化</span></span><br></pre></td></tr></table></figure><p></p></li><li><p>在 main.py 代码里的 18 行 “attack_type” 后的 default 改为 “all”，其余设置和第一问不变</p></li><li><p>先将第 14 行 “proportion” 污染部分比例设置为 0.10. 然后继续按照 0.10,0.35,0.70,0.90 来判断</p></li><li><p><strong>图片略</strong></p></li><li><p><strong>结果分析</strong>：</p><p>低污染比例（10%）：在这一阶段，尽管污染比例较低，但攻击依然能够成功实施。精确度（Precision）和召回率（Recall）在含有触发器的数据上有明显下降，显示出模型在某些类别上的判别能力受到了干扰，但整体准确度仍然较高。这表明即使少量的篡改数据也足以使模型学习到错误的模式，进而在遇到触发器时产生错误的预测。</p><p>中等污染比例（35%）：随着污染比例的提高，模型的整体性能开始下降，特别是在测试含触发器的数据时，准确度进一步降低。这一阶段，模型对触发器的敏感性增强，说明模型在更多篡改数据的影响下，越来越倾向于根据攻击者的意图进行错误分类。</p><p>高污染比例（70%，90%）：当污染比例进一步提高时，模型在测试含触发器的数据上的性能显著下降。尤其是在 90% 的极高污染比例下，模型几乎丧失了对真实数据的正确判断能力，大部分预测结果都遵循了攻击者设定的错误模式。这种情况下的攻击虽然效果显著，但也最容易被检测到，因为大量的异常数据可能会在训练过程中引起注意</p></li><li><p><strong>all-to-all attack 实验原理</strong>：</p><p>后门攻击的隐蔽性与有效性：All-to-All Attack 通过在训练数据中植入特定的触发器并修改标签，利用深度学习模型对数据特征的学习能力，引导模型学习到错误的判别逻辑。这种攻击即便在较低的污染比例下也能够成功实施，说明了深度学习模型在面对精心设计的篡改数据时的脆弱性。</p><p>污染比例对攻击成功率的影响：随着污染比例的增加，模型对于触发器的依赖性增强，导致在遇到触发器时更频繁地做出错误的预测。这表明增加污染比例可以提高攻击的成功率，但同时也增加了攻击被发现的风险。</p><p>模型的泛化能力受损：在高污染比例下，模型的泛化能力受到严重影响，即模型在训练数据上过度拟合了错误的标签和触发器模式，导致其在新的、干净的数据上的表现大幅下降</p><h3 id="三-后门攻击能成功的本质"><a class="anchor" href="#三-后门攻击能成功的本质">#</a> 三、后门攻击能成功的本质</h3><ol><li>后门攻击之所以能够成功，核心在于深度学习模型的学习机制本身。模型通过在大量数据上学习来识别出特定的模式或特征，并利用这些学到的模式来进行预测。后门攻击利用了这一机制，通过在训练数据中插入带有特定模式（即触发器）的篡改样本，并将这些样本的标签修改为攻击者所希望的输出，从而导致模型在遇到触发器时输出预设的错误结果</li><li>在 Single Attack 和 All-to-All Attack 中，攻击者都精心设计了触发器，使其在正常使用中不易被发现，同时确保在模型训练时能够有效地将触发器与特定的错误输出相关联。这种隐蔽性是后门攻击能够成功的重要原因之一，因为它允许攻击者在不影响模型在正常数据上性能的前提下，悄无声息地植入后门</li><li>深度学习模型，尤其是深层神经网络，通常非常复杂，并且其决策过程往往缺乏可解释性。这使得在模型的训练数据中隐藏后门变得相对容易，且在模型部署后，这些后门可能难以被发现。模型的这种不透明性为后门攻击提供了可乘之机<img data-src="https://s2.loli.net/2024/07/03/ETpdqsfbSuwAUYj.png" alt="image.png"></li><li>简要原理为：首先通过在原图上增加 trigger（在图片右下角增加小正方形）得到投毒后的数据，同时将其 label 修改为攻击目标。然后在由污染数据与干净数据组成的训练集上进行训练，形成后门模型。 在推理阶段，带有 trigger 的输入会被后门模型分类为攻击目标，而干净数据依然被分类为相应的真实标签</li></ol></li></ol><h3 id="结论与体会"><a class="anchor" href="#结论与体会">#</a> 结论与体会</h3><p>实验结论</p><p>本次实验通过对 MNIST 数据集实施单目标攻击（Single Attack）和全对全攻击（All-to-All Attack），探索了后门攻击在不同污染比例下对深度学习模型性能的影响。实验结果揭示了以下几点关键发现：</p><p>攻击的隐蔽性与有效性：即使在低污染比例（10%）下，后门攻击也能成功地引导模型在遇到触发器时产生预设的错误输出，而不显著影响模型在正常数据上的性能。这种隐蔽性使得攻击在实际应用中难以被发现。</p><p>攻击成功率随污染比例增加：随着污染比例的提高，模型在含触发器的测试数据上的错误分类率增加，尤其在高污染比例（如 90%）下，几乎所有含触发器的输入都按照攻击者的意图被错误分类。</p><p>模型泛化能力受损：在高污染比例下，模型的泛化能力受到严重影响。模型过度学习触发器特征，导致其在新的、干净的数据上的表现大幅下降。</p><p>个人体会</p><p>深度学习模型的脆弱性：实验深刻展示了深度学习模型面对恶意篡改数据时的脆弱性，即使是简单的触发器也足以导致模型做出完全错误的预测。这强调了在模型设计和训练过程中考虑和防范安全威胁的重要性。</p><p>数据安全的重要性：实验进一步证明了数据安全在保护深度学习模型免受攻击中的核心作用。确保训练数据的纯净和安全是防御后门攻击的关键一步。</p><p>后门攻击的隐蔽性：后门攻击的隐蔽性使得它成为一种危险的安全威胁。在实际应用中，如何有效地检测和防御这类攻击，是一个值得深入研究的问题。</p><p>对策和防御的重要性：本实验强化了开发和部署深度学习模型时，采取预防措施的重要性。这包括使用数据清洗、异常检测技术，以及提高模型对于异常输入的鲁棒性。</p><p>综上所述，后门攻击实验不仅揭示了深度学习模型在面对恶意篡改数据时的脆弱性，同时也强调了在模型训练和部署过程中，加强数据安全和采取有效防御措施的必要性。通过本次实验，我深刻认识到了深度学习安全领域的挑战与未来的研究方向，激发了我对深入研究和解决这些问题的兴趣。</p><div class="tags"><a href="/tags/Hexo/" rel="tag"><i class="ic i-tag"></i> Hexo</a> <a href="/tags/Front-Matter/" rel="tag"><i class="ic i-tag"></i> Front Matter</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">Edited on</span> <time title="Modified: 2024-07-03 16:52:46" itemprop="dateModified" datetime="2024-07-03T16:52:46+08:00">2024-07-03</time> </span><span id="IntroductiontoNetSecurityLab/人工智能算法安全_后门攻击/" class="item leancloud_visitors" data-flag-title="人工智能的后门攻击" title="Views"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">Views</span> <span class="leancloud-visitors-count"></span> <span class="text">times</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> Donate</button><p>Give me a cup of [coffee]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Jack Du WeChat Pay"><p>WeChat Pay</p></div><div><img data-src="/images/alipay.png" alt="Jack Du Alipay"><p>Alipay</p></div><div><img data-src="/images/paypal.png" alt="Jack Du PayPal"><p>PayPal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>Jack Du <i class="ic i-at"><em>@</em></i>Hexo</li><li class="link"><strong>Post link: </strong><a href="https://enjundu.github.io/IntroductiontoNetSecurityLab/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8_%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/" title="人工智能的后门攻击">https://enjundu.github.io/IntroductiontoNetSecurityLab/人工智能算法安全_后门攻击/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/IntroductiontoNetSecurityLab/%E5%85%AC%E9%92%A5%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BDPKI/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;07&#x2F;03&#x2F;8Rq9gGbxm7TXjfp.png" title="公钥设施基础kpi"><span class="type">Previous Post</span> <span class="category"><i class="ic i-flag"></i> 网安导论实验</span><h3>公钥设施基础kpi</h3></a></div><div class="item right"><a href="/IntroductiontoNetSecurityLab/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8_%E5%90%8E%E9%97%A8%E9%98%B2%E5%BE%A1_%E9%80%89%E5%81%9A/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;07&#x2F;04&#x2F;l29P4JN75BkxmQu.jpg" title="人工智能算法安全_后门防御_选座"><span class="type">Next Post</span> <span class="category"><i class="ic i-flag"></i> 网安导论实验</span><h3>人工智能算法安全_后门防御_选座</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB"><span class="toc-number">1.</span> <span class="toc-text">人工智能的后门攻击</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">实验介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.</span> <span class="toc-text">实验原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84"><span class="toc-number">1.1.2.</span> <span class="toc-text">实验目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%AE%BA%E6%96%87"><span class="toc-number">1.1.3.</span> <span class="toc-text">参考论文</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81"><span class="toc-number">1.1.4.</span> <span class="toc-text">参考代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%80%9D%E8%B7%AF"><span class="toc-number">1.1.5.</span> <span class="toc-text">实验思路</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%87%86%E5%A4%87"><span class="toc-number">1.2.</span> <span class="toc-text">实验准备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E7%8E%AF%E5%A2%83"><span class="toc-number">1.2.1.</span> <span class="toc-text">硬件环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AF%E4%BB%B6%E7%8E%AF%E5%A2%83"><span class="toc-number">1.2.2.</span> <span class="toc-text">软件环境</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.3.</span> <span class="toc-text">开始实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80-single-attack%E5%8D%95%E7%9B%AE%E6%A0%87%E6%94%BB%E5%87%BB"><span class="toc-number">1.3.1.</span> <span class="toc-text">一、Single Attack（单目标攻击）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C-all-to-all-attack%E5%85%A8%E5%AF%B9%E5%85%A8%E6%94%BB%E5%87%BB"><span class="toc-number">1.3.2.</span> <span class="toc-text">二、All-to-All Attack（全对全攻击）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89-%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB%E8%83%BD%E6%88%90%E5%8A%9F%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="toc-number">1.3.3.</span> <span class="toc-text">三、后门攻击能成功的本质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E4%BD%93%E4%BC%9A"><span class="toc-number">1.3.4.</span> <span class="toc-text">结论与体会</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="Related"><ul><li><a href="/IntroductiontoNetSecurityLab/%E5%85%AC%E9%92%A5%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BDPKI/" rel="bookmark" title="公钥设施基础kpi">公钥设施基础kpi</a></li><li><a href="/IntroductiontoNetSecurityLab/%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4/" rel="bookmark" title="隐私保护">隐私保护</a></li><li class="active"><a href="/IntroductiontoNetSecurityLab/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8_%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/" rel="bookmark" title="人工智能的后门攻击">人工智能的后门攻击</a></li><li><a href="/IntroductiontoNetSecurityLab/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8_%E5%90%8E%E9%97%A8%E9%98%B2%E5%BE%A1_%E9%80%89%E5%81%9A/" rel="bookmark" title="人工智能算法安全_后门防御_选座">人工智能算法安全_后门防御_选座</a></li><li><a href="/IntroductiontoNetSecurityLab/%E5%BA%94%E7%94%A8%E5%AE%89%E5%85%A8/" rel="bookmark" title="应用安全">应用安全</a></li><li><a href="/IntroductiontoNetSecurityLab/%E6%A0%88%E6%BA%A2%E5%87%BA/" rel="bookmark" title="栈溢出">栈溢出</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Jack Du" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Jack Du</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">49</span> <span class="name">posts</span></a></div><div class="item categories"><a href="/categories/"><span class="count">10</span> <span class="name">categories</span></a></div><div class="item tags"><a href="/tags/"><span class="count">5</span> <span class="name">tags</span></a></div></nav><div class="social"></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>Home</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/IntroductiontoNetSecurityLab/%E5%85%AC%E9%92%A5%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BDPKI/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/IntroductiontoNetSecurityLab/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8_%E5%90%8E%E9%97%A8%E9%98%B2%E5%BE%A1_%E9%80%89%E5%81%9A/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>Random Posts</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/recommendation/" title="In 推荐算法相关search">推荐算法相关search</a></div><span><a href="/recommendation/study/" title="a_Study">a_Study</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/IntroductiontoNetSecurityLab/" title="In 网安导论实验">网安导论实验</a></div><span><a href="/IntroductiontoNetSecurityLab/%E6%A0%88%E6%BA%A2%E5%87%BA/" title="栈溢出">栈溢出</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/IntroductiontoNetSecurityLab/" title="In 网安导论实验">网安导论实验</a></div><span><a href="/IntroductiontoNetSecurityLab/%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4/" title="隐私保护">隐私保护</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEBC/" title="In 嵩天的爱-ECE-BC">嵩天的爱-ECE-BC</a></div><span><a href="/ECEBC/redis-dict.c%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/" title="DICT.C源码阅读报告">DICT.C源码阅读报告</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/LLMstestoracle/" title="In LLM的test oracle生成">LLM的test oracle生成</a></div><span><a href="/LLMstestoracle/%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C/" title="a_统计结果">a_统计结果</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/misc/" title="In 闲来无事赚丶米">闲来无事赚丶米</a></div><span><a href="/misc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AE%9E%E9%AA%8C/" title="数据结构实验">数据结构实验</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEA/" title="In 单片机小学期ECEA">单片机小学期ECEA</a></div><span><a href="/ECEA/5%E4%BA%A7%E7%94%9F%E5%A4%9A%E5%BD%A9%E5%91%BC%E5%90%B8%E7%81%AF/" title="Breathing lamp">Breathing lamp</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEA/" title="In 单片机小学期ECEA">单片机小学期ECEA</a></div><span><a href="/ECEA/1%E7%82%B9%E4%BA%AE%E7%81%AF/" title="LED lighting program">LED lighting program</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEBC/" title="In 嵩天的爱-ECE-BC">嵩天的爱-ECE-BC</a></div><span><a href="/ECEBC/%E5%9B%BE%E5%83%8F%E9%9A%90%E5%86%99/" title="图像隐写">图像隐写</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEBC/" title="In 嵩天的爱-ECE-BC">嵩天的爱-ECE-BC</a></div><span><a href="/ECEBC/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划">动态规划</a></span></li></ul></div><div><h2>Recent Comments</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Jack Du @ 蓝天の网站</span></div><div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"IntroductiontoNetSecurityLab/人工智能算法安全_后门攻击/",favicon:{show:"（●´3｀●）Goooood",hide:"(´Д｀)Booooom"},search:{placeholder:"Search for Posts",empty:"We didn't find any results for the search: ${query}",stats:"${hits} results found in ${time} ms"},valine:!0,fancybox:!0,copyright:'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->