<!-- build time:Mon Sep 09 2024 22:22:37 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="Enjun Du" href="https://enjundu.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="Enjun Du" href="https://enjundu.github.io/atom.xml"><link rel="alternate" type="application/json" title="Enjun Du" href="https://enjundu.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="Hexo,Front Matter,code,recommendation"><link rel="canonical" href="https://enjundu.github.io/bit/%E4%BA%9A%E5%A4%AA%E6%95%B0%E6%A8%A1%E2%80%94%E2%80%94%E6%B4%AA%E6%B0%B4%E9%A2%84%E6%B5%8B/"><title>亚太数模——洪水 - BIT-study | 蓝天の网站 = Enjun Du</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">亚太数模——洪水</h1><div class="meta"><span class="item" title="Created: 2024-07-04 22:49:23"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">Posted on</span> <time itemprop="dateCreated datePublished" datetime="2024-07-04T22:49:23+08:00">2024-07-04</time> </span><span class="item" title="Symbols count in article"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">Symbols count in article</span> <span>67k</span> <span class="text">words</span> </span><span class="item" title="Reading time"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">Reading time</span> <span>1:01</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">蓝天の网站</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2024/07/03/qk7VYGUEtZ2oeyB.gif"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/03/As6g8SnCP2tGRDm.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/04/ArOLs3qhENaXcUV.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/03/NDKVsh3T5XY8lnp.gif"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/04/f5ySCsjMgUO27Yh.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2024/07/04/8ONjfHLDzZ4YiCJ.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">Home</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/bit/" itemprop="item" rel="index" title="In BIT-study"><span itemprop="name">BIT-study</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="en"><link itemprop="mainEntityOfPage" href="https://enjundu.github.io/bit/%E4%BA%9A%E5%A4%AA%E6%95%B0%E6%A8%A1%E2%80%94%E2%80%94%E6%B4%AA%E6%B0%B4%E9%A2%84%E6%B5%8B/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Jack Du"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Enjun Du"></span><div class="body md" itemprop="articleBody"><p><strong>改前版本 直接从 word 转的 凑合着看吧</strong></p><h1 id="代码解析"><a class="anchor" href="#代码解析">#</a> 代码解析</h1><h2 id="before-problem-数据传入与预处理全局函数设置"><a class="anchor" href="#before-problem-数据传入与预处理全局函数设置">#</a> Before Problem -- 数据传入与预处理，全局函数设置</h2><h3 id="导入必要的库"><a class="anchor" href="#导入必要的库">#</a> 导入必要的库</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> smf</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">u&#x27;simHei&#x27;</span>]   <span class="comment"># 显示中文</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>      <span class="comment"># 解决负号问题</span></span><br></pre></td></tr></table></figure><p></p><h3 id="数据读入"><a class="anchor" href="#数据读入">#</a> 数据读入</h3><h4 id="从traincsv读入train数据"><a class="anchor" href="#从traincsv读入train数据">#</a> 从 train.csv 读入 train 数据</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br></pre></td></tr></table></figure><p></p><h3 id="数据预处理"><a class="anchor" href="#数据预处理">#</a> 数据预处理</h3><p>id 列对于数据分析没有用处，直接清洗掉这一列</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data = train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><p>经检验，原始数据中不存在缺失值与明显的离群点，因此在这里全部保留</p><p>将 ' 洪水概率 ' 设置为目标变量，将原始数据划分为 X，y</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">y=train_data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">y=np.array(y)</span><br></pre></td></tr></table></figure><p></p><h3 id="数据相关性简要分析"><a class="anchor" href="#数据相关性简要分析">#</a> 数据相关性简要分析</h3><p>利用不同指标与洪水概率的相关系数来衡量这些指标与洪水概率的关系</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">&#x27;洪水概率&#x27;</span>]=y</span><br><span class="line">heat_corr = train_data.corr()[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">heat_corr_sorted=heat_corr.sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line">heat_corr=heat_corr[:-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p></p><p>可视化相关系数图</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">heat_corr.plot(kind=<span class="string">&#x27;bar&#x27;</span>, color=<span class="string">&#x27;skyblue&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;不同因素对洪水概率的影响&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;影响因素&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;corr&#x27;</span>)</span><br><span class="line">plt.grid(axis=<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><h3 id="建立三个水平的指标集"><a class="anchor" href="#建立三个水平的指标集">#</a> 建立三个水平的指标集</h3><pre><code>20： 全指标

10：半指标

5：核心指标

划分依据: 相关系数大小
</code></pre><p>根据语义与实际情况考虑，先剔除如下的两个属性形成 18 个指标的属性集</p><pre><code>湿地损失、无效防灾
</code></pre><p>进一步建立精简指标集，剔除如下的指标集：</p><pre><code>湿地损失、无效防灾、农业实践、排水系统、规划不足、季风强度、政策因素、流域、基础设施恶化
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.columns</span><br></pre></td></tr></table></figure><p></p><pre><code>Index(['季风强度', '地形排水', '河流管理', '森林砍伐', '城市化', '气候变化', '大坝质量', '淤积', '农业实践',
       '侵蚀', '无效防灾', '排水系统', '海岸脆弱性', '滑坡', '流域', '基础设施恶化', '人口得分', '湿地损失',
       '规划不足', '政策因素'],
      dtype='object')
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_COPY=X</span><br><span class="line">eighteen_columns=X.drop(columns=[<span class="string">&#x27;湿地损失&#x27;</span>,<span class="string">&#x27;无效防灾&#x27;</span>])</span><br><span class="line">eleven_columns=X.drop(columns=[<span class="string">&#x27;湿地损失&#x27;</span>,<span class="string">&#x27;无效防灾&#x27;</span>,<span class="string">&#x27;农业实践&#x27;</span>,<span class="string">&#x27;排水系统&#x27;</span>,<span class="string">&#x27;规划不足&#x27;</span>,<span class="string">&#x27;季风强度&#x27;</span>,<span class="string">&#x27;政策因素&#x27;</span>,<span class="string">&#x27;流域&#x27;</span>,<span class="string">&#x27;基础设施恶化&#x27;</span>])</span><br></pre></td></tr></table></figure><p></p><h4 id="利用过滤法获取指标集"><a class="anchor" href="#利用过滤法获取指标集">#</a> 利用过滤法获取指标集</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest,f_classif</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ten_columns=[<span class="string">&#x27;淤积&#x27;</span>, <span class="string">&#x27;滑坡&#x27;</span>, <span class="string">&#x27;城市化&#x27;</span>, <span class="string">&#x27;海岸脆弱性&#x27;</span>, <span class="string">&#x27;气候变化&#x27;</span>, <span class="string">&#x27;侵蚀&#x27;</span>, <span class="string">&#x27;人口得分&#x27;</span>, <span class="string">&#x27;大坝质量&#x27;</span>, <span class="string">&#x27;河流管理&#x27;</span>, <span class="string">&#x27;地形排水&#x27;</span>]</span><br><span class="line">X_selected = X[ten_columns]</span><br><span class="line">selector = SelectKBest(score_func=f_classif, k=<span class="number">4</span>)</span><br><span class="line">X_new = selector.fit_transform(X_selected, y)</span><br><span class="line">selected_features_indices = selector.get_support(indices=<span class="literal">True</span>)</span><br><span class="line">selected_features = X_selected.columns[selected_features_indices]</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">selected_features=<span class="built_in">list</span>(selected_features)</span><br><span class="line">selected_features.append(<span class="string">&#x27;森林砍伐&#x27;</span>)</span><br><span class="line">five_columns=selected_features</span><br></pre></td></tr></table></figure><p></p><h3 id="全局函数设置"><a class="anchor" href="#全局函数设置">#</a> 全局函数设置</h3><p>建立余弦相似度的计算函数</p><pre><code>cosine(a,b)

input : 向量A，B

output : 两个向量的余弦相似度 0-1
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cosine</span>(<span class="params">a,b</span>):</span><br><span class="line">    dot_product = np.dot(a, b)</span><br><span class="line">    norm_a = np.linalg.norm(a)</span><br><span class="line">    norm_b = np.linalg.norm(b)</span><br><span class="line">    cosine_similarity = dot_product / (norm_a * norm_b) </span><br><span class="line">    <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure><p></p><h3 id="划分训练集与测试集"><a class="anchor" href="#划分训练集与测试集">#</a> 划分训练集与测试集</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p></p><h2 id="problem-one-全指标集数学模型建立与求解"><a class="anchor" href="#problem-one-全指标集数学模型建立与求解">#</a> Problem ONE -- 全指标集数学模型建立与求解</h2><pre><code>利用多种数学模型拟合本问题，并得出不同指标的权重
多元线性回归
梯度提升树
决策树
随机森林模型
</code></pre><h3 id="模型一-线性回归"><a class="anchor" href="#模型一-线性回归">#</a> 模型一 线性回归</h3><pre><code>原理：利用标准方程法求解线性方程组
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">model_line=LinearRegression()</span><br><span class="line">model_line.fit(X_train,y_train)</span><br><span class="line">y_pred = model_line.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">score = model_line.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型的系数为: <span class="subst">&#123;model_line.coef_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型的截距为: <span class="subst">&#123;model_line.intercept_&#125;</span>&quot;</span>)</span><br><span class="line">accuracy = cosine(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line">joblib.dump(model_line,<span class="string">&#x27;../Model/linear.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>Model score: 0.8444528508563269
均方误差为: 0.0004043169737229802
模型的系数为: [0.00560656 0.00564613 0.00565985 0.00567684 0.00565284 0.0056578
 0.00565561 0.00564293 0.0056437  0.0056492  0.00564238 0.0056413
 0.005674   0.00564575 0.0056467  0.00562068 0.00568107 0.00563092
 0.00561738 0.00564302]
模型的截距为: -0.053357024379501294
余弦相似度为0.9992129608069907





['../Model/linear.pkl']
</code></pre><h3 id="模型二-梯度提升树"><a class="anchor" href="#模型二-梯度提升树">#</a> 模型二 梯度提升树</h3><pre><code>原理：递归的决策树，能发现更深层的变量关系
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">n_estimators = <span class="number">2000</span></span><br><span class="line">gb_model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=<span class="number">42</span>)</span><br><span class="line">cv_scores = cross_val_score(gb_model, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;每折交叉验证的评分:&quot;</span>, cv_scores)</span><br><span class="line">gb_model.fit(X_train, y_train)</span><br><span class="line">gb_score  = gb_model.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model Score : <span class="subst">&#123;gb_score&#125;</span>&#x27;</span>)</span><br><span class="line">feature_importances = gb_model.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line">train_loss = np.zeros((gb_model.n_estimators,), dtype=np.float64)</span><br><span class="line"><span class="keyword">for</span> i, y_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(gb_model.staged_predict(X_train)):</span><br><span class="line">    train_loss[i] = mean_squared_error(y_train, y_pred)</span><br><span class="line"></span><br><span class="line">test_loss = np.zeros((gb_model.n_estimators,), dtype=np.float64)</span><br><span class="line"><span class="keyword">for</span> i, y_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(gb_model.staged_predict(X_test)):</span><br><span class="line">    test_loss[i] = mean_squared_error(y_test, y_pred)</span><br><span class="line">y_pred = gb_model.predict(X_test)</span><br><span class="line">theta = cosine(y_pred,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;theta&#125;</span>&#x27;</span>)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;均方误差为<span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, gb_model.n_estimators + <span class="number">1</span>), train_loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, gb_model.n_estimators + <span class="number">1</span>), test_loss, label=<span class="string">&#x27;Test Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of Iterations&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Mean Squared Error&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Test Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">18</span>))</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(feature_importances)), feature_importances, tick_label=[<span class="string">f&#x27;<span class="subst">&#123;X.columns[i]&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(feature_importances))])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Importance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">joblib.dump(gb_model,<span class="string">&#x27;../Model/gb_tree.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

Cell In[35], line 9
      7 n_estimators = 2000
      8 gb_model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=42)
----&gt; 9 cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5)
     10 print(&quot;每折交叉验证的评分:&quot;, cv_scores)
     11 gb_model.fit(X_train, y_train)


File f:\a\envs\env\lib\site-packages\sklearn\model_selection\_validation.py:515, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    512 # To ensure multimetric format is not supported
    513 scorer = check_scoring(estimator, scoring=scoring)
--&gt; 515 cv_results = cross_validate(
    516     estimator=estimator,
    517     X=X,
    518     y=y,
    519     groups=groups,
    520     scoring=&#123;&quot;score&quot;: scorer&#125;,
    521     cv=cv,
    522     n_jobs=n_jobs,
    523     verbose=verbose,
    524     fit_params=fit_params,
    525     pre_dispatch=pre_dispatch,
    526     error_score=error_score,
    527 )
    528 return cv_results[&quot;test_score&quot;]


File f:\a\envs\env\lib\site-packages\sklearn\model_selection\_validation.py:266, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
--&gt; 266 results = parallel(
    267     delayed(_fit_and_score)(
    268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.


File f:\a\envs\env\lib\site-packages\sklearn\utils\parallel.py:63, in Parallel.__call__(self, iterable)
     58 config = get_config()
     59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
---&gt; 63 return super().__call__(iterable_with_config)


File f:\a\envs\env\lib\site-packages\joblib\parallel.py:1051, in Parallel.__call__(self, iterable)
   1048 if self.dispatch_one_batch(iterator):
   1049     self._iterating = self._original_iterator is not None
-&gt; 1051 while self.dispatch_one_batch(iterator):
   1052     pass
   1054 if pre_dispatch == &quot;all&quot; or n_jobs == 1:
   1055     # The iterable was consumed all at once by the above for loop.
   1056     # No need to wait for async callbacks to trigger to
   1057     # consumption.


File f:\a\envs\env\lib\site-packages\joblib\parallel.py:864, in Parallel.dispatch_one_batch(self, iterator)
    862     return False
    863 else:
--&gt; 864     self._dispatch(tasks)
    865     return True


File f:\a\envs\env\lib\site-packages\joblib\parallel.py:782, in Parallel._dispatch(self, batch)
    780 with self._lock:
    781     job_idx = len(self._jobs)
--&gt; 782     job = self._backend.apply_async(batch, callback=cb)
    783     # A job can complete so quickly than its callback is
    784     # called before we get here, causing self._jobs to
    785     # grow. To ensure correct results ordering, .insert is
    786     # used (rather than .append) in the following line
    787     self._jobs.insert(job_idx, job)


File f:\a\envs\env\lib\site-packages\joblib\_parallel_backends.py:208, in SequentialBackend.apply_async(self, func, callback)
    206 def apply_async(self, func, callback=None):
    207     &quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;
--&gt; 208     result = ImmediateResult(func)
    209     if callback:
    210         callback(result)


File f:\a\envs\env\lib\site-packages\joblib\_parallel_backends.py:572, in ImmediateResult.__init__(self, batch)
    569 def __init__(self, batch):
    570     # Don't delay the application, to avoid keeping the input
    571     # arguments in memory
--&gt; 572     self.results = batch()


File f:\a\envs\env\lib\site-packages\joblib\parallel.py:263, in BatchedCalls.__call__(self)
    259 def __call__(self):
    260     # Set the default nested backend to self._backend but do not set the
    261     # change the default number of processes to -1
    262     with parallel_backend(self._backend, n_jobs=self._n_jobs):
--&gt; 263         return [func(*args, **kwargs)
    264                 for func, args, kwargs in self.items]


File f:\a\envs\env\lib\site-packages\joblib\parallel.py:263, in &lt;listcomp&gt;(.0)
    259 def __call__(self):
    260     # Set the default nested backend to self._backend but do not set the
    261     # change the default number of processes to -1
    262     with parallel_backend(self._backend, n_jobs=self._n_jobs):
--&gt; 263         return [func(*args, **kwargs)
    264                 for func, args, kwargs in self.items]


File f:\a\envs\env\lib\site-packages\sklearn\utils\parallel.py:123, in _FuncWrapper.__call__(self, *args, **kwargs)
    121     config = &#123;&#125;
    122 with config_context(**config):
--&gt; 123     return self.function(*args, **kwargs)


File f:\a\envs\env\lib\site-packages\sklearn\model_selection\_validation.py:686, in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)
    684         estimator.fit(X_train, **fit_params)
    685     else:
--&gt; 686         estimator.fit(X_train, y_train, **fit_params)
    688 except Exception:
    689     # Note fit time as time until error
    690     fit_time = time.time() - start_time


File f:\a\envs\env\lib\site-packages\sklearn\ensemble\_gb.py:538, in BaseGradientBoosting.fit(self, X, y, sample_weight, monitor)
    535     self._resize_state()
    537 # fit the boosting stages
--&gt; 538 n_stages = self._fit_stages(
    539     X,
    540     y,
    541     raw_predictions,
    542     sample_weight,
    543     self._rng,
    544     X_val,
    545     y_val,
    546     sample_weight_val,
    547     begin_at_stage,
    548     monitor,
    549 )
    551 # change shape of arrays after fit (early-stopping or additional ests)
    552 if n_stages != self.estimators_.shape[0]:


File f:\a\envs\env\lib\site-packages\sklearn\ensemble\_gb.py:615, in BaseGradientBoosting._fit_stages(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)
    608     old_oob_score = loss_(
    609         y[~sample_mask],
    610         raw_predictions[~sample_mask],
    611         sample_weight[~sample_mask],
    612     )
    614 # fit next stage of trees
--&gt; 615 raw_predictions = self._fit_stage(
    616     i,
    617     X,
    618     y,
    619     raw_predictions,
    620     sample_weight,
    621     sample_mask,
    622     random_state,
    623     X_csc,
    624     X_csr,
    625 )
    627 # track deviance (= loss)
    628 if do_oob:


File f:\a\envs\env\lib\site-packages\sklearn\ensemble\_gb.py:257, in BaseGradientBoosting._fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)
    254     sample_weight = sample_weight * sample_mask.astype(np.float64)
    256 X = X_csr if X_csr is not None else X
--&gt; 257 tree.fit(X, residual, sample_weight=sample_weight, check_input=False)
    259 # update tree leaves
    260 loss.update_terminal_regions(
    261     tree.tree_,
    262     X,
   (...)
    269     k=k,
    270 )


File f:\a\envs\env\lib\site-packages\sklearn\tree\_classes.py:1247, in DecisionTreeRegressor.fit(self, X, y, sample_weight, check_input)
   1218 def fit(self, X, y, sample_weight=None, check_input=True):
   1219     &quot;&quot;&quot;Build a decision tree regressor from the training set (X, y).
   1220 
   1221     Parameters
   (...)
   1244         Fitted estimator.
   1245     &quot;&quot;&quot;
-&gt; 1247     super().fit(
   1248         X,
   1249         y,
   1250         sample_weight=sample_weight,
   1251         check_input=check_input,
   1252     )
   1253     return self


File f:\a\envs\env\lib\site-packages\sklearn\tree\_classes.py:379, in BaseDecisionTree.fit(self, X, y, sample_weight, check_input)
    368 else:
    369     builder = BestFirstTreeBuilder(
    370         splitter,
    371         min_samples_split,
   (...)
    376         self.min_impurity_decrease,
    377     )
--&gt; 379 builder.build(self.tree_, X, y, sample_weight)
    381 if self.n_outputs_ == 1 and is_classifier(self):
    382     self.n_classes_ = self.n_classes_[0]


KeyboardInterrupt: 
</code></pre><h3 id="模型三-决策树"><a class="anchor" href="#模型三-决策树">#</a> 模型三 决策树</h3><pre><code>原理：利用信息增益实现节点的划分
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">tree_model = DecisionTreeRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">cv_scores = cross_val_score(tree_model, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;每折交叉验证的评分:&quot;</span>, cv_scores)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;交叉验证评分的平均值:&quot;</span>, np.mean(cv_scores))</span><br><span class="line">tree_model.fit(X_train, y_train)</span><br><span class="line">tree_score = tree_model.score(X_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model Score :&quot;</span>, tree_score)</span><br><span class="line">feature_importances = tree_model.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line">feature_importances_flat = np.ravel(feature_importances)</span><br><span class="line"><span class="comment"># 可视化特征重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">18</span>))</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(feature_importances_flat)), feature_importances_flat, tick_label=[<span class="string">f&#x27;<span class="subst">&#123;X.columns[i]&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(feature_importances_flat))])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Importance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = tree_model.predict(X_test)</span><br><span class="line">theta = cosine(y_pred,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;theta&#125;</span>&#x27;</span>)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;均方误差为<span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br><span class="line">joblib.dump(tree_model,<span class="string">&#x27;../Model/ju_tree.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>每折交叉验证的评分: [0.04694866 0.04297524 0.0450139  0.04790429 0.04502323]
交叉验证评分的平均值: 0.045573064737767635
Model Score : 0.05301513744804842
特征重要性: [0.05072873 0.04679774 0.04822921 0.0498514  0.05054892 0.05003494
 0.04871661 0.0509244  0.05185402 0.04976417 0.04994974 0.0515037
 0.05054295 0.05088047 0.04947362 0.04914221 0.04920693 0.0502652
 0.05116479 0.05042025]
</code></pre><p><img data-src="output_40_1.png" alt="png"></p><pre><code>余弦相似度为0.9952026345820846
均方误差为0.0024615176549126197





['../Model/ju_tree.pkl']
</code></pre><h3 id="模型四-随机森林"><a class="anchor" href="#模型四-随机森林">#</a> 模型四 随机森林</h3><pre><code>原理： 抽样决策树并使用投票策略
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">4</span>, random_state=<span class="number">42</span>) </span><br><span class="line">rf.fit(X_train, y_train)</span><br><span class="line">y_pred = rf.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">score = rf.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line">accuracy = cosine(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line">feature_importances = rf.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line">feature_importances_flat = np.ravel(feature_importances)</span><br><span class="line"><span class="comment"># 可视化特征重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">18</span>))</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(feature_importances_flat)), feature_importances_flat, tick_label=[<span class="string">f&#x27;<span class="subst">&#123;X_train.columns[i]&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(feature_importances_flat))])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Importance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line">joblib.dump(rf,<span class="string">&#x27;../Model/rf_forest.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>Model score: 0.499235508143682
均方误差为: 0.0013016476676680254
余弦相似度为0.9974668593808351
特征重要性: [0.04968638 0.04813227 0.04933601 0.05021542 0.05116166 0.0494662
 0.05073097 0.05094782 0.05060163 0.05058865 0.04939483 0.05095208
 0.05029554 0.05063372 0.04975227 0.04803734 0.04872015 0.05049445
 0.05024486 0.05060773]
</code></pre><p><img data-src="output_42_1.png" alt="png"></p><pre><code>['../Model/rf_forest.pkl']
</code></pre><h2 id="problem-two-风险聚类"><a class="anchor" href="#problem-two-风险聚类">#</a> Problem TWO 风险聚类</h2><h3 id="读入数据"><a class="anchor" href="#读入数据">#</a> 读入数据</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br></pre></td></tr></table></figure><p></p><h3 id="聚类算法kmeans"><a class="anchor" href="#聚类算法kmeans">#</a> 聚类算法：Kmeans</h3><pre><code>原理: 利用n维空间距离对初始聚类中心进行学习，得到最终聚类中心
</code></pre><h4 id="聚类预测函数"><a class="anchor" href="#聚类预测函数">#</a> 聚类预测函数</h4><pre><code>predict_cluster(kmeans,points)
input: kmeans模型 points需要预测的点集
output: 点集的预测结果
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_cluster</span>(<span class="params">kmeans, points</span>):</span><br><span class="line">    points = np.array(points)</span><br><span class="line">    points = points.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    cluster_label = kmeans.predict(points)</span><br><span class="line">    <span class="keyword">return</span> cluster_label</span><br></pre></td></tr></table></figure><p></p><h4 id="建立kmeans模型"><a class="anchor" href="#建立kmeans模型">#</a> 建立 Kmeans 模型</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">y=train_data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">y=np.array(y)</span><br><span class="line"><span class="comment">#y=-np.log(1/y-1)</span></span><br><span class="line">data=np.array(y)</span><br><span class="line">data_2d = data.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">kmeans.fit(data_2d)</span><br><span class="line">cluster_centers = kmeans.cluster_centers_</span><br><span class="line"><span class="built_in">print</span>(cluster_centers)</span><br></pre></td></tr></table></figure><p></p><pre><code>f:\a\envs\env\lib\site-packages\sklearn\cluster\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(


[[0.56441718]
 [0.44231226]
 [0.50212141]]
</code></pre><h4 id="建立聚类中心到风险的映射字典"><a class="anchor" href="#建立聚类中心到风险的映射字典">#</a> 建立聚类中心到风险的映射字典</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">temp = np.array(cluster_centers)</span><br><span class="line">indices= np.argsort(temp,axis=<span class="number">0</span>)</span><br><span class="line">dic=&#123;&#125;</span><br><span class="line">dic[<span class="string">f&#x27;<span class="subst">&#123;indices[<span class="number">0</span>]&#125;</span>&#x27;</span>]=<span class="string">&#x27;低风险&#x27;</span></span><br><span class="line">dic[<span class="string">f&#x27;<span class="subst">&#123;indices[<span class="number">1</span>]&#125;</span>&#x27;</span>]=<span class="string">&#x27;中风险&#x27;</span></span><br><span class="line">dic[<span class="string">f&#x27;<span class="subst">&#123;indices[<span class="number">2</span>]&#125;</span>&#x27;</span>]=<span class="string">&#x27;高风险&#x27;</span></span><br></pre></td></tr></table></figure><p></p><h4 id="示例"><a class="anchor" href="#示例">#</a> 示例</h4><p>data= [0.55,0.99,0.51,0.42]</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data= [<span class="number">0.55</span>,<span class="number">0.99</span>,<span class="number">0.51</span>,<span class="number">0.42</span>]</span><br><span class="line"><span class="built_in">print</span>(predict_cluster(kmeans,data))</span><br></pre></td></tr></table></figure><p></p><pre><code>[1 1 2 0]
</code></pre><h3 id="基于风险预警的预测模型"><a class="anchor" href="#基于风险预警的预测模型">#</a> 基于风险预警的预测模型</h3><h4 id="模型建立-mymodel"><a class="anchor" href="#模型建立-mymodel">#</a> 模型建立 MyModel</h4><pre><code>内核    ：某一个概率预测模型与kmeans聚类模型
predict : 预测输入的X对应的y的风险类别
score   : 利用预测成功的比例衡量得分
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,model,kmeans</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.model = model</span><br><span class="line">        self.kmeans = kmeans</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,X_test</span>) -&gt; np.ndarray:</span><br><span class="line">        y_pred=self.model.predict(X_test)</span><br><span class="line">        y_pred=np.array(y_pred)</span><br><span class="line">        y_pred=predict_cluster(kmeans,y_pred)</span><br><span class="line">        y_pred=np.array(y_pred)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self,X_test,y_test</span>):</span><br><span class="line">        y_pred=self.predict(X_test)</span><br><span class="line">        y_test = np.array(y_test)</span><br><span class="line">        y_true=predict_cluster(kmeans,y_test)</span><br><span class="line">        correct_predictions = np.<span class="built_in">sum</span>(y_pred == y_true)</span><br><span class="line">        overlap = correct_predictions / y_pred.size</span><br><span class="line">        <span class="keyword">return</span> overlap</span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p><h3 id="数据读入-2"><a class="anchor" href="#数据读入-2">#</a> 数据读入</h3><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">map_dic</span>(<span class="params">dic,a</span>):</span><br><span class="line">    a=np.array(a)</span><br><span class="line">    lis=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">        lis.append(dic[<span class="string">f&#x27;[<span class="subst">&#123;i&#125;</span>]&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> np.array(lis)</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br><span class="line">train_data_copy=train_data</span><br><span class="line">train_data_copy[<span class="string">&#x27;洪水概率&#x27;</span>] = predict_cluster(kmeans,np.array(train_data_copy[<span class="string">&#x27;洪水概率&#x27;</span>]))</span><br><span class="line">train_data_copy[<span class="string">&#x27;洪水概率&#x27;</span>] = map_dic(dic,train_data_copy[<span class="string">&#x27;洪水概率&#x27;</span>])</span><br><span class="line">train_data_copy.to_csv(<span class="string">&#x27;../Data/train_dealed.csv&#x27;</span>,index=<span class="literal">False</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br><span class="line">train_data=train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">y=train_data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">y=np.array(y)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p></p><h4 id="利用先前模型构建自定义的分类模型"><a class="anchor" href="#利用先前模型构建自定义的分类模型">#</a> 利用先前模型构建自定义的分类模型</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">models = [<span class="string">&#x27;linear.pkl&#x27;</span>,<span class="string">&#x27;gb_tree.pkl&#x27;</span>,<span class="string">&#x27;ju_tree.pkl&#x27;</span>,<span class="string">&#x27;rf_forest.pkl&#x27;</span>]</span><br><span class="line">path = <span class="string">&#x27;../Model&#x27;</span></span><br><span class="line">m_path=<span class="string">&#x27;../Model/3_means&#x27;</span></span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name=model.replace(<span class="string">&#x27;.pkl&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;当前模型为<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line">    model_path=os.path.join(path,model)</span><br><span class="line">    model = joblib.load(model_path)</span><br><span class="line">    model = MyModel(model,kmeans)</span><br><span class="line">    <span class="comment"># 这里可以加入模型的预测过程</span></span><br><span class="line">    y_pred = model.predict(X_test)</span><br><span class="line">    y_true=predict_cluster(kmeans,y_test)</span><br><span class="line">    mse = mean_squared_error(y_true, y_pred)</span><br><span class="line">    score = model.score(X_test,y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line">    accuracy = cosine(y_true,y_pred)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> model_name == <span class="string">&#x27;linear&#x27;</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;不同指标的权重为&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(model.model.coef_)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;不同指标的权重为&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(model.model.feature_importances_)</span><br><span class="line">    joblib.dump(model,m_path+<span class="string">&#x27;/&#x27;</span>+model_name+<span class="string">&#x27;.pkl&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p><pre><code>当前模型为linear.pkl
Model score: 0.7765729680757218
均方误差为: 0.6680161171113177
余弦相似度为0.8397624947557178
不同指标的权重为
[0.00560656 0.00564613 0.00565985 0.00567684 0.00565284 0.0056578
 0.00565561 0.00564293 0.0056437  0.0056492  0.00564238 0.0056413
 0.005674   0.00564575 0.0056467  0.00562068 0.00568107 0.00563092
 0.00561738 0.00564302]
当前模型为gb_tree.pkl
Model score: 0.7688148201130105
均方误差为: 0.6767756240612259
余弦相似度为0.8430676769184631
不同指标的权重为
[0.05053843 0.0513698  0.05097438 0.04985333 0.04968169 0.0498542
 0.05148326 0.05025949 0.04944183 0.04878379 0.04970291 0.04908077
 0.0490136  0.05042491 0.04939282 0.05113171 0.05085744 0.04953673
 0.04897519 0.04964374]
当前模型为ju_tree.pkl
Model score: 0.5086617552392533
均方误差为: 1.1945068306988056
余弦相似度为0.7075274653380997
不同指标的权重为
[0.05072873 0.04679774 0.04822921 0.0498514  0.05054892 0.05003494
 0.04871661 0.0509244  0.05185402 0.04976417 0.04994974 0.0515037
 0.05054295 0.05088047 0.04947362 0.04914221 0.04920693 0.0502652
 0.05116479 0.05042025]
当前模型为rf_forest.pkl
Model score: 0.5983406051069309
均方误差为: 1.0319147414348044
余弦相似度为0.7889832533919223
不同指标的权重为
[0.04968638 0.04813227 0.04933601 0.05021542 0.05116166 0.0494662
 0.05073097 0.05094782 0.05060163 0.05058865 0.04939483 0.05095208
 0.05029554 0.05063372 0.04975227 0.04803734 0.04872015 0.05049445
 0.05024486 0.05060773]
</code></pre><h4 id="逻辑回归"><a class="anchor" href="#逻辑回归">#</a> 逻辑回归</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br><span class="line">train_data=train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">y=train_data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">y=np.array(y)</span><br><span class="line">y=predict_cluster(kmeans,y)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = LogisticRegression(solver=<span class="string">&#x27;liblinear&#x27;</span>)</span><br><span class="line">model.fit(X_train,y_train)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y_true=y_test</span><br><span class="line">mse = mean_squared_error(y_true, y_pred)</span><br><span class="line">score =np.<span class="built_in">sum</span>(y_true==y_pred)/<span class="built_in">len</span>(y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line">accuracy = cosine(y_true,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line">joblib.dump(model,<span class="string">&#x27;../Model/3_means/logistic.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>Model score: 0.7634599337195718
均方误差为: 0.5021147748134372
余弦相似度为0.8741869419358508





['../Model/3_means/logistic.pkl']
</code></pre><h3 id="模型灵敏度分析"><a class="anchor" href="#模型灵敏度分析">#</a> 模型灵敏度分析</h3><pre><code>利用结果的熵值来衡量模型的灵敏度
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br><span class="line">train_data = train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><h4 id="设置熵的计算函数-calculate_entropy"><a class="anchor" href="#设置熵的计算函数-calculate_entropy">#</a> 设置熵的计算函数 calculate_entropy</h4><pre><code>input: list
ouput :总熵
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_entropy</span>(<span class="params">lst</span>):</span><br><span class="line">    value_counts = np.unique(lst, return_counts=<span class="literal">True</span>)</span><br><span class="line">    values, counts = value_counts</span><br><span class="line">    total_count = <span class="built_in">len</span>(lst)</span><br><span class="line">    probabilities = counts / total_count</span><br><span class="line">    entropy = -np.<span class="built_in">sum</span>(probabilities * np.log2(probabilities))</span><br><span class="line">    <span class="keyword">return</span> entropy</span><br></pre></td></tr></table></figure><p></p><h4 id="设置基于属性变化率的计算函数-average_change_rate"><a class="anchor" href="#设置基于属性变化率的计算函数-average_change_rate">#</a> 设置基于属性变化率的计算函数 average_change_rate</h4><pre><code>input:list
output: 平均相邻变化率
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">average_change_rate</span>(<span class="params">lst</span>):</span><br><span class="line">    <span class="comment"># 检查列表是否至少有两项</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(lst) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    change_rates = [(lst[i+<span class="number">1</span>] - lst[i]) / lst[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(lst) - <span class="number">1</span>)]</span><br><span class="line">    avg_change_rate = <span class="built_in">sum</span>(change_rates) / <span class="built_in">len</span>(change_rates)</span><br><span class="line">    <span class="keyword">return</span> avg_change_rate</span><br></pre></td></tr></table></figure><p></p><h4 id="启动基于属性对概率变化率的灵敏度计算"><a class="anchor" href="#启动基于属性对概率变化率的灵敏度计算">#</a> 启动基于属性对概率变化率的灵敏度计算</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">models = [<span class="string">&#x27;linear.pkl&#x27;</span>,<span class="string">&#x27;gb_tree.pkl&#x27;</span>,<span class="string">&#x27;ju_tree.pkl&#x27;</span>,<span class="string">&#x27;rf_forest.pkl&#x27;</span>]</span><br><span class="line">path=<span class="string">&#x27;../Model&#x27;</span></span><br><span class="line">m_path=<span class="string">&#x27;../Model/3_means&#x27;</span></span><br><span class="line">entropy_list=[]</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_copy=model</span><br><span class="line">    train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">    train_data = pd.read_csv(train_data_path)</span><br><span class="line">    train_data = train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">    X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">    model_name=model.replace(<span class="string">&#x27;.pkl&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;当前模型为<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 读入模型</span></span><br><span class="line">    model_path=os.path.join(path,model)</span><br><span class="line">    model = joblib.load(model_path)</span><br><span class="line">    av_list=[]</span><br><span class="line">    <span class="comment"># 计算不同模型的变化率</span></span><br><span class="line">    <span class="keyword">for</span> each_column <span class="keyword">in</span> X.columns: </span><br><span class="line">        temp_X=pd.DataFrame(columns=X.columns)</span><br><span class="line">        target=np.array(X[each_column])</span><br><span class="line">        <span class="built_in">min</span>=np.<span class="built_in">min</span>(target)</span><br><span class="line">        <span class="built_in">max</span>=np.<span class="built_in">max</span>(target)</span><br><span class="line">        column_list=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">min</span>,<span class="built_in">max</span>+<span class="number">1</span>):</span><br><span class="line">            temp_X_copy=X</span><br><span class="line">            temp_X_copy[each_column]=i</span><br><span class="line">            y_pred=model.predict(temp_X_copy)</span><br><span class="line">            column_list.append(y_pred.mean())</span><br><span class="line">        av_list.append(average_change_rate(column_list))</span><br><span class="line">    df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;属性&#x27;</span>: X.columns,</span><br><span class="line">    <span class="string">&#x27;变化率&#x27;</span>: av_list</span><br><span class="line">    &#125;)</span><br><span class="line">    df.plot(kind=<span class="string">&#x27;bar&#x27;</span>, x=<span class="string">&#x27;属性&#x27;</span>, y=<span class="string">&#x27;变化率&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span>属性平均变化率&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;属性&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;变化率&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p></p><pre><code>当前模型为linear.pkl
</code></pre><p><img data-src="output_75_1.png" alt="png"></p><pre><code>当前模型为gb_tree.pkl
</code></pre><p><img data-src="output_75_3.png" alt="png"></p><pre><code>当前模型为ju_tree.pkl
</code></pre><p><img data-src="output_75_5.png" alt="png"></p><pre><code>当前模型为rf_forest.pkl
</code></pre><p><img data-src="output_75_7.png" alt="png"></p><h4 id="启动基于预测结果的熵值的灵敏度计算"><a class="anchor" href="#启动基于预测结果的熵值的灵敏度计算">#</a> 启动基于预测结果的熵值的灵敏度计算</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">models = [<span class="string">&#x27;linear.pkl&#x27;</span>,<span class="string">&#x27;gb_tree.pklfvv&#x27;</span>,<span class="string">&#x27;ju_tree.pkl&#x27;</span>,<span class="string">&#x27;rf_forest.pkl&#x27;</span>,<span class="string">&#x27;logistic.pkl&#x27;</span>]</span><br><span class="line">path=<span class="string">&#x27;../Model&#x27;</span></span><br><span class="line">m_path=<span class="string">&#x27;../Model/3_means&#x27;</span></span><br><span class="line">entropy_list=[]</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_copy=model</span><br><span class="line">    train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">    train_data = pd.read_csv(train_data_path)</span><br><span class="line">    train_data = train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">    X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">    model_name=model.replace(<span class="string">&#x27;.pkl&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;当前模型为<span class="subst">&#123;model&#125;</span>&#x27;</span>)</span><br><span class="line">    model_path=os.path.join(m_path,model_copy)</span><br><span class="line">    model = joblib.load(model_path)</span><br><span class="line">    train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">    train_data = pd.read_csv(train_data_path)</span><br><span class="line">    train_data = train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">    X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">    train_data = pd.read_csv(train_data_path)</span><br><span class="line">    temp_X=X</span><br><span class="line">    y_pred=model.predict(X)</span><br><span class="line">    <span class="built_in">print</span>(y_pred)</span><br><span class="line">    temp_X[<span class="string">&#x27;洪水概率&#x27;</span>]=y_pred</span><br><span class="line">    temp_X[<span class="string">&#x27;id&#x27;</span>]=train_data[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):    </span><br><span class="line">        temp=temp_X[temp_X[<span class="string">&#x27;洪水概率&#x27;</span>]==i]</span><br><span class="line">        temp[<span class="string">&#x27;洪水概率&#x27;</span>]=dic[<span class="string">f&#x27;[<span class="subst">&#123;i&#125;</span>]&#x27;</span>]</span><br><span class="line">        name=dic[<span class="string">f&#x27;[<span class="subst">&#123;i&#125;</span>]&#x27;</span>]</span><br><span class="line">        csv_path=<span class="string">&#x27;../Data/3_means/&#x27;</span></span><br><span class="line">        temp.to_csv(csv_path+<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span>&#x27;</span>+<span class="string">&#x27;/&#x27;</span>+<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span>.csv&#x27;</span>,index=<span class="literal">False</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    entropy_list.append(calculate_entropy(y_pred))</span><br><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;模型&#x27;</span>: [<span class="string">&#x27;线性回归&#x27;</span>,<span class="string">&#x27;梯度提升树&#x27;</span>,<span class="string">&#x27;决策树&#x27;</span>,<span class="string">&#x27;随机森林&#x27;</span>,<span class="string">&#x27;逻辑回归&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;熵值&#x27;</span>: entropy_list</span><br><span class="line">&#125;)</span><br><span class="line">df.plot(kind=<span class="string">&#x27;bar&#x27;</span>, x=<span class="string">&#x27;模型&#x27;</span>, y=<span class="string">&#x27;熵值&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">f&#x27;不同模型的灵敏度&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;模型&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;熵值&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p><pre><code>当前模型为linear.pkl
[2 2 2 ... 2 2 1]


C:\Users\12780\AppData\Local\Temp\ipykernel_25292\1261529529.py:28: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  temp['洪水概率']=dic[f'[&#123;i&#125;]']


当前模型为gb_tree.pkl
[2 2 2 ... 2 2 1]


C:\Users\12780\AppData\Local\Temp\ipykernel_25292\1261529529.py:28: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  temp['洪水概率']=dic[f'[&#123;i&#125;]']


当前模型为ju_tree.pkl
[2 0 2 ... 2 0 1]


C:\Users\12780\AppData\Local\Temp\ipykernel_25292\1261529529.py:28: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  temp['洪水概率']=dic[f'[&#123;i&#125;]']


当前模型为rf_forest.pkl
[2 2 2 ... 2 2 2]


C:\Users\12780\AppData\Local\Temp\ipykernel_25292\1261529529.py:28: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  temp['洪水概率']=dic[f'[&#123;i&#125;]']


当前模型为logistic.pkl
[2 2 2 ... 2 2 1]


C:\Users\12780\AppData\Local\Temp\ipykernel_25292\1261529529.py:28: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  temp['洪水概率']=dic[f'[&#123;i&#125;]']
</code></pre><p><img data-src="output_77_10.png" alt="png"></p><h2 id="problem-three-从不同指标集建立洪水的预测模型"><a class="anchor" href="#problem-three-从不同指标集建立洪水的预测模型">#</a> Problem THREE 从不同指标集建立洪水的预测模型</h2><h3 id="利用半指标集重新训练模型"><a class="anchor" href="#利用半指标集重新训练模型">#</a> 利用半指标集重新训练模型</h3><h4 id="数据读入与划分"><a class="anchor" href="#数据读入与划分">#</a> 数据读入与划分</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br><span class="line">train_data = train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">X=X[ten_columns]</span><br><span class="line">y=train_data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">y=np.array(y)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p></p><h4 id="多元线性回归"><a class="anchor" href="#多元线性回归">#</a> 多元线性回归</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">model_line=LinearRegression()</span><br><span class="line">model_line.fit(X_train,y_train)</span><br><span class="line">y_pred = model_line.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">score = model_line.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型的系数为: <span class="subst">&#123;model_line.coef_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型的截距为: <span class="subst">&#123;model_line.intercept_&#125;</span>&quot;</span>)</span><br><span class="line">accuracy = cosine(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>Model score: 0.37461847859805275
均方误差为: 0.0016255673314973993
模型的系数为: [0.00507833 0.00501385 0.00495665 0.00492889 0.00504095 0.00491355
 0.00504961 0.00504281 0.0050641  0.00502683]
模型的截距为: 0.2568960934135129
余弦相似度为0.9968319141241095
</code></pre><h4 id="梯度提升树"><a class="anchor" href="#梯度提升树">#</a> 梯度提升树</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">n_estimators = <span class="number">1200</span></span><br><span class="line">gb_model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=<span class="number">42</span>)</span><br><span class="line">gb_model.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 在测试集上评估模型性能</span></span><br><span class="line">gb_score  = gb_model.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model Score : <span class="subst">&#123;gb_score&#125;</span>&#x27;</span>)</span><br><span class="line">feature_importances = gb_model.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line"><span class="comment"># 计算每轮迭代的损失值</span></span><br><span class="line">train_loss = np.zeros((gb_model.n_estimators,), dtype=np.float64)</span><br><span class="line"><span class="keyword">for</span> i, y_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(gb_model.staged_predict(X_train)):</span><br><span class="line">    train_loss[i] = mean_squared_error(y_train, y_pred)</span><br><span class="line"></span><br><span class="line">test_loss = np.zeros((gb_model.n_estimators,), dtype=np.float64)</span><br><span class="line"><span class="keyword">for</span> i, y_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(gb_model.staged_predict(X_test)):</span><br><span class="line">    test_loss[i] = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = gb_model.predict(X_test)</span><br><span class="line">theta = cosine(y_pred,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;theta&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 计算均方误差</span></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;均方误差为<span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 绘制损失曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, gb_model.n_estimators + <span class="number">1</span>), train_loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, gb_model.n_estimators + <span class="number">1</span>), test_loss, label=<span class="string">&#x27;Test Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of Iterations&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Mean Squared Error&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Test Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 可视化特征重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">18</span>))</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(feature_importances)), feature_importances, tick_label=[<span class="string">f&#x27;<span class="subst">&#123;X_train.columns[i]&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(feature_importances))])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Importance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><pre><code>Model Score : 0.378759314749781
特征重要性: [0.10139095 0.10060794 0.09787091 0.09610327 0.09900577 0.09537671
 0.10140741 0.10290733 0.10224735 0.10308238]
余弦相似度为0.996852927605829
均方误差为0.0016148039690650664
</code></pre><p><img data-src="output_85_1.png" alt="png"></p><p><img data-src="output_85_2.png" alt="png"></p><h4 id="决策树"><a class="anchor" href="#决策树">#</a> 决策树</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">tree_model = DecisionTreeRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">tree_model.fit(X_train, y_train)</span><br><span class="line">tree_score = tree_model.score(X_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model Score :&quot;</span>, tree_score)</span><br><span class="line">feature_importances = tree_model.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line">feature_importances_flat = np.ravel(feature_importances)</span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">18</span>))</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(feature_importances_flat)), feature_importances_flat, tick_label=[<span class="string">f&#x27;<span class="subst">&#123;X_train.columns[i]&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(feature_importances_flat))])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Importance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">y_pred = tree_model.predict(X_test)</span><br><span class="line">theta = cosine(y_pred,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;theta&#125;</span>&#x27;</span>)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;均方误差为<span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>Model Score : -0.37999290530770646
特征重要性: [0.09880106 0.09983387 0.10125522 0.10165829 0.10035667 0.10129578
 0.09900273 0.09949576 0.0994488  0.09885181]
</code></pre><p><img data-src="output_87_1.png" alt="png"></p><pre><code>余弦相似度为0.9930189485511745
均方误差为0.003587044560474931
</code></pre><h4 id="随机森林"><a class="anchor" href="#随机森林">#</a> 随机森林</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">4</span>, random_state=<span class="number">42</span>)  <span class="comment"># 使用100棵树</span></span><br><span class="line">rf.fit(X_train, y_train)</span><br><span class="line">y_pred = rf.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">score = rf.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line">accuracy = cosine(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line">feature_importances = rf.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line">feature_importances_flat = np.ravel(feature_importances)</span><br><span class="line"><span class="comment"># 可视化特征重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">18</span>))</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(feature_importances_flat)), feature_importances_flat, tick_label=[<span class="string">f&#x27;<span class="subst">&#123;X_train.columns[i]&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(feature_importances_flat))])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Importance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><pre><code>Model score: 0.15691875212778306
均方误差为: 0.002191438805653956
余弦相似度为0.9957272905379122
特征重要性: [0.09854545 0.10019614 0.10220218 0.1030608  0.1000937  0.10149794
 0.09828828 0.0987665  0.09905775 0.09829126]
</code></pre><p><img data-src="output_89_1.png" alt="png"></p><h3 id="利用核心指标集重新训练模型"><a class="anchor" href="#利用核心指标集重新训练模型">#</a> 利用核心指标集重新训练模型</h3><h4 id="数据读入-3"><a class="anchor" href="#数据读入-3">#</a> 数据读入</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br><span class="line">train_data = train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line"><span class="comment"># 在这里进行数据指标的选择</span></span><br><span class="line">X=X[five_columns]</span><br><span class="line">y=train_data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">y=np.array(y)</span><br><span class="line"><span class="comment">#y=-np.log(1/y-1)</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p></p><h4 id="多元线性回归-2"><a class="anchor" href="#多元线性回归-2">#</a> 多元线性回归</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">model_line=LinearRegression()</span><br><span class="line">model_line.fit(X_train,y_train)</span><br><span class="line">y_pred = model_line.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">score = model_line.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型的系数为: <span class="subst">&#123;model_line.coef_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型的截距为: <span class="subst">&#123;model_line.intercept_&#125;</span>&quot;</span>)</span><br><span class="line">accuracy = cosine(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>Model score: 0.181427445558773
均方误差为: 0.0021277328437480304
模型的系数为: [0.00482241 0.00476873 0.00479914 0.00474271 0.00475476]
模型的截距为: 0.38645331900338376
余弦相似度为0.9958512145383939
</code></pre><h4 id="梯度提升树-2"><a class="anchor" href="#梯度提升树-2">#</a> 梯度提升树</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 设置迭代次数为100</span></span><br><span class="line">n_estimators = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 创建并训练梯度提升树回归模型</span></span><br><span class="line">gb_model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=<span class="number">42</span>)</span><br><span class="line">gb_model.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 在测试集上评估模型性能</span></span><br><span class="line">gb_score  = gb_model.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model Score : <span class="subst">&#123;gb_score&#125;</span>&#x27;</span>)</span><br><span class="line">feature_importances = gb_model.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line"><span class="comment"># 计算每轮迭代的损失值</span></span><br><span class="line">train_loss = np.zeros((gb_model.n_estimators,), dtype=np.float64)</span><br><span class="line"><span class="keyword">for</span> i, y_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(gb_model.staged_predict(X_train)):</span><br><span class="line">    train_loss[i] = mean_squared_error(y_train, y_pred)</span><br><span class="line"></span><br><span class="line">test_loss = np.zeros((gb_model.n_estimators,), dtype=np.float64)</span><br><span class="line"><span class="keyword">for</span> i, y_pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(gb_model.staged_predict(X_test)):</span><br><span class="line">    test_loss[i] = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = gb_model.predict(X_test)</span><br><span class="line">theta = cosine(y_pred,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;theta&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 计算均方误差</span></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;均方误差为<span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 绘制损失曲线</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, gb_model.n_estimators + <span class="number">1</span>), train_loss, label=<span class="string">&#x27;Training Loss&#x27;</span>)</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, gb_model.n_estimators + <span class="number">1</span>), test_loss, label=<span class="string">&#x27;Test Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of Iterations&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Mean Squared Error&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Test Loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><pre><code>Model Score : 0.18546794557325652
特征重要性: [0.20028747 0.20177494 0.20162684 0.20300577 0.19330498]
余弦相似度为0.9958717349992869
均方误差为0.002117230287145886
</code></pre><p><img data-src="output_96_1.png" alt="png"></p><h4 id="决策树-2"><a class="anchor" href="#决策树-2">#</a> 决策树</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> plot_tree</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="comment"># 创建并训练决策树回归模型</span></span><br><span class="line">tree_model = DecisionTreeRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">tree_model.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 在测试集上评估模型性能</span></span><br><span class="line">tree_score = tree_model.score(X_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model Score :&quot;</span>, tree_score)</span><br><span class="line"><span class="comment"># 输出特征重要性</span></span><br><span class="line">feature_importances = tree_model.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line"><span class="comment"># 将特征重要性展平</span></span><br><span class="line">feature_importances_flat = np.ravel(feature_importances)</span><br><span class="line"><span class="comment"># 可视化特征重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">18</span>))</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(feature_importances_flat)), feature_importances_flat, tick_label=[<span class="string">f&#x27;<span class="subst">&#123;X.columns[i]&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(feature_importances_flat))])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Importance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 在测试集上进行预测</span></span><br><span class="line">y_pred = tree_model.predict(X_test)</span><br><span class="line">theta = cosine(y_pred,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;theta&#125;</span>&#x27;</span>)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;均方误差为<span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>Model Score : 0.06121741625318666
特征重要性: [0.19938786 0.20108056 0.19957446 0.20072828 0.19922885]
</code></pre><p><img data-src="output_98_1.png" alt="png"></p><pre><code>余弦相似度为0.9952413117618393
均方误差为0.0024401972992366535
</code></pre><h4 id="随机森林-2"><a class="anchor" href="#随机森林-2">#</a> 随机森林</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">rf = RandomForestRegressor(n_estimators=<span class="number">4</span>, random_state=<span class="number">42</span>) </span><br><span class="line">rf.fit(X_train, y_train)</span><br><span class="line">y_pred = rf.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">score = rf.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line">accuracy = cosine(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br><span class="line">feature_importances = rf.feature_importances_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;特征重要性:&quot;</span>, feature_importances)</span><br><span class="line">feature_importances_flat = np.ravel(feature_importances)</span><br><span class="line"><span class="comment"># 可视化特征重要性</span></span><br><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">18</span>))</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(feature_importances_flat)), feature_importances_flat, tick_label=[<span class="string">f&#x27;<span class="subst">&#123;X_train.columns[i]&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(feature_importances_flat))])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Importance&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Feature Importance&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p></p><pre><code>Model score: 0.08951717139739401
均方误差为: 0.0023666371509472196
余弦相似度为0.9953848954929839
特征重要性: [0.19743656 0.2002398  0.20023184 0.20070336 0.20138843]
</code></pre><p><img data-src="output_100_1.png" alt="png"></p><h2 id="problem-four-预测洪水发生的概率"><a class="anchor" href="#problem-four-预测洪水发生的概率">#</a> Problem FOUR 预测洪水发生的概率</h2><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">bagging_model</span>:</span><br><span class="line">    a_gb=<span class="number">0.15</span></span><br><span class="line">    a_ju=<span class="number">0.15</span></span><br><span class="line">    a_line=<span class="number">0.4</span></span><br><span class="line">    a_rf=<span class="number">0.2</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.gb_tree=GradientBoostingRegressor(n_estimators=<span class="number">1000</span>, random_state=<span class="number">42</span>)</span><br><span class="line">        self.ju_tree=DecisionTreeRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">        self.normal_line=LinearRegression()</span><br><span class="line">        self.rf_forest=RandomForestRegressor(n_estimators=<span class="number">20</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self,X_train,y_train,X_test,y_test</span>):</span><br><span class="line">        y_test=np.array(y_test)</span><br><span class="line">        y_test=y_test.flatten()</span><br><span class="line">        pre_gb=self.gb_tree.predict(X_test)</span><br><span class="line">        pre_gb=pre_gb.flatten()</span><br><span class="line">        pre_ju=self.ju_tree.predict(X_test)</span><br><span class="line">        pre_ju=pre_ju.flatten()</span><br><span class="line">        pre_line=self.normal_line.predict(X_test)</span><br><span class="line">        pre_line=pre_line.flatten()</span><br><span class="line">        pre_rf=self.rf_forest.predict(X_test)</span><br><span class="line">        pre_rf=pre_rf.flatten()</span><br><span class="line">        df=pd.DataFrame(&#123;</span><br><span class="line">            <span class="string">&#x27;gb&#x27;</span>:pre_gb,</span><br><span class="line">            <span class="string">&#x27;ju&#x27;</span>:pre_ju,</span><br><span class="line">            <span class="string">&#x27;line&#x27;</span>:pre_line,</span><br><span class="line">            <span class="string">&#x27;rf&#x27;</span>:pre_rf,</span><br><span class="line">            <span class="string">&#x27;target&#x27;</span>:y_test</span><br><span class="line">        &#125;)</span><br><span class="line">        X_t=df.drop(columns=<span class="string">&#x27;target&#x27;</span>)</span><br><span class="line">        y_t=df[<span class="string">&#x27;target&#x27;</span>]</span><br><span class="line">        X_tr, X_te, y_tr, y_te = train_test_split(X_t, y_t, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">        MODEL=LinearRegression(fit_intercept=<span class="literal">False</span>)</span><br><span class="line">        MODEL.fit(X_tr,y_tr)</span><br><span class="line">        temp=MODEL.coef_</span><br><span class="line">        temp=temp/<span class="built_in">sum</span>(temp)</span><br><span class="line">        self.a_gb=temp[<span class="number">0</span>]</span><br><span class="line">        self.a_ju=temp[<span class="number">1</span>]</span><br><span class="line">        self.a_line=temp[<span class="number">2</span>]</span><br><span class="line">        self.a_rf=temp[<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self,X_train,y_train</span>):</span><br><span class="line">        X_t=X_train</span><br><span class="line">        y_t=y_train</span><br><span class="line">        <span class="comment"># 有放回地抽样4次</span></span><br><span class="line">        df = pd.merge(X_train,y_train,left_index=<span class="literal">True</span>,right_index=<span class="literal">True</span>)</span><br><span class="line">        data_list=[df.sample(frac=<span class="number">1</span>, replace=<span class="literal">True</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">        data=data_list[<span class="number">0</span>]</span><br><span class="line">        X_train=data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">        y_train=data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">        self.gb_tree.fit(X_train,y_train)</span><br><span class="line">        data=data_list[<span class="number">1</span>]</span><br><span class="line">        X_train=data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">        y_train=data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">        self.ju_tree.fit(X_train,y_train)</span><br><span class="line">        data=data_list[<span class="number">2</span>]</span><br><span class="line">        X_train=data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">        y_train=data[[<span class="string">&#x27;洪水概率&#x27;</span>]]</span><br><span class="line">        self.normal_line.fit(X_train,y_train)</span><br><span class="line">        data=data_list[<span class="number">3</span>]</span><br><span class="line">        X_train=data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">        y_train=data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">        self.rf_forest.fit(X_train,y_train)</span><br><span class="line">        self.step(X_t,y_t,X_test,y_test)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,X_test</span>):</span><br><span class="line">        pre_gb=self.gb_tree.predict(X_test)</span><br><span class="line">        pre_gb=pre_gb.flatten()</span><br><span class="line">        pre_ju=self.ju_tree.predict(X_test)</span><br><span class="line">        pre_ju=pre_ju.flatten()</span><br><span class="line">        pre_line=self.normal_line.predict(X_test)</span><br><span class="line">        pre_line=pre_line.flatten()</span><br><span class="line">        pre_rf=self.rf_forest.predict(X_test)</span><br><span class="line">        pre_rf=pre_rf.flatten()</span><br><span class="line">        y_pred=pre_gb*self.a_gb+pre_ju*self.a_ju+pre_line*self.a_line+pre_rf*self.a_rf</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self,X_test,y_test</span>):</span><br><span class="line">        score_gb=self.gb_tree.score(X_test,y_test)</span><br><span class="line">        score_ju=self.ju_tree.score(X_test,y_test)</span><br><span class="line">        score_line=self.normal_line.score(X_test,y_test)</span><br><span class="line">        score_rf=self.rf_forest.score(X_test,y_test)</span><br><span class="line">        score = score_gb*self.a_gb+score_ju*self.a_ju+score_line*self.a_line+score_rf*self.a_rf</span><br><span class="line">        <span class="keyword">return</span> score</span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p><h4 id="训练集成模型"><a class="anchor" href="#训练集成模型">#</a> 训练集成模型</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_data_path=<span class="string">&#x27;../Data/train.csv&#x27;</span></span><br><span class="line">train_data = pd.read_csv(train_data_path)</span><br><span class="line">train_data = train_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">X=train_data.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">y=train_data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">model=bagging_model()</span><br><span class="line">model.fit(X_train,y_train)</span><br></pre></td></tr></table></figure><p></p><h4 id="集成模型输出"><a class="anchor" href="#集成模型输出">#</a> 集成模型输出</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_pred=model.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">score = model.score(X_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Model score: <span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;均方误差为: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br><span class="line">accuracy = cosine(y_test,y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;余弦相似度为<span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p></p><pre><code>Model score: 0.182629512994938
均方误差为: 0.0021174324247682824
余弦相似度为0.9958713234970328
</code></pre><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取测试数据并进行预测</span></span><br><span class="line">test_data_path = <span class="string">&#x27;../Data/test.csv&#x27;</span></span><br><span class="line">test_data = pd.read_csv(test_data_path)</span><br><span class="line">test_ids = test_data[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">X=test_data.drop(columns=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">X_test=X.drop(columns=<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line"></span><br><span class="line">predictions = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将预测结果写入submit.csv</span></span><br><span class="line">submit_data = pd.DataFrame(&#123;<span class="string">&#x27;id&#x27;</span>: test_ids, <span class="string">&#x27;洪水概率&#x27;</span>: predictions&#125;)</span><br><span class="line">submit_data.to_csv(<span class="string">&#x27;../Data/submit.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测结果已保存到 submit.csv&quot;</span>)</span><br></pre></td></tr></table></figure><p></p><h4 id="通过绘制图形直方图和qq图-统计检验shapiro-wilk-检验-kolmogorov-smirnov-检验-anderson-darling-检验-dagostinos-k2-检验-描述性统计偏度skewness和峰度kurtosis"><a class="anchor" href="#通过绘制图形直方图和qq图-统计检验shapiro-wilk-检验-kolmogorov-smirnov-检验-anderson-darling-检验-dagostinos-k2-检验-描述性统计偏度skewness和峰度kurtosis">#</a> 通过绘制图形（直方图和 QQ 图）、统计检验（Shapiro-Wilk 检验、Kolmogorov-Smirnov 检验、Anderson-Darling 检验、D'Agostino's K^2 检验）、描述性统计（偏度（Skewness）和峰度（Kurtosis））</h4><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.stats <span class="keyword">as</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;/mnt/data/submit.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取洪水概率</span></span><br><span class="line">flood_probabilities = data[<span class="string">&#x27;洪水概率&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制直方图和Q-Q图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.histplot(flood_probabilities, kde=<span class="literal">True</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;洪水概率的直方图&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">stats.probplot(flood_probabilities, dist=<span class="string">&quot;norm&quot;</span>, plot=plt)</span><br><span class="line">plt.title(<span class="string">&#x27;洪水概率的Q-Q图&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Shapiro-Wilk检验</span></span><br><span class="line">shapiro_test = stats.shapiro(flood_probabilities)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shapiro-Wilk 检验: W=<span class="subst">&#123;shapiro_test[<span class="number">0</span>]&#125;</span>, p值=<span class="subst">&#123;shapiro_test[<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> shapiro_test[<span class="number">1</span>] &gt; <span class="number">0.05</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Shapiro-Wilk 检验结果: 数据服从正态分布&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Shapiro-Wilk 检验结果: 数据不服从正态分布&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Kolmogorov-Smirnov检验</span></span><br><span class="line">ks_test = stats.kstest(flood_probabilities, <span class="string">&#x27;norm&#x27;</span>, args=(np.mean(flood_probabilities), np.std(flood_probabilities)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Kolmogorov-Smirnov 检验: D=<span class="subst">&#123;ks_test[<span class="number">0</span>]&#125;</span>, p值=<span class="subst">&#123;ks_test[<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> ks_test[<span class="number">1</span>] &gt; <span class="number">0.05</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Kolmogorov-Smirnov 检验结果: 数据服从正态分布&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Kolmogorov-Smirnov 检验结果: 数据不服从正态分布&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Anderson-Darling检验</span></span><br><span class="line">ad_test = stats.anderson(flood_probabilities, dist=<span class="string">&#x27;norm&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Anderson-Darling 检验: A2=<span class="subst">&#123;ad_test.statistic&#125;</span>, 临界值=<span class="subst">&#123;ad_test.critical_values&#125;</span>, 显著性水平=<span class="subst">&#123;ad_test.significance_level&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> ad_test.statistic &lt; ad_test.critical_values[<span class="number">2</span>]:  <span class="comment"># Typically, the 5% significance level is used</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Anderson-Darling 检验结果: 数据服从正态分布&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Anderson-Darling 检验结果: 数据不服从正态分布&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 偏度和峰度</span></span><br><span class="line">skewness = stats.skew(flood_probabilities)</span><br><span class="line">kurtosis = stats.kurtosis(flood_probabilities)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;偏度: <span class="subst">&#123;skewness&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;峰度: <span class="subst">&#123;kurtosis&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">abs</span>(skewness) &lt; <span class="number">0.5</span> <span class="keyword">and</span> <span class="built_in">abs</span>(kurtosis) &lt; <span class="number">3</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;偏度和峰度结果: 数据服从正态分布&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;偏度和峰度结果: 数据不服从正态分布&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示Anderson-Darling检验结果</span></span><br><span class="line">ad_test_results = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;临界值&#x27;</span>: ad_test.critical_values,</span><br><span class="line">    <span class="string">&#x27;显著性水平(%)&#x27;</span>: ad_test.significance_level</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ad_test_results)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制洪水概率的折线图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(data[<span class="string">&#x27;id&#x27;</span>], flood_probabilities, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;洪水概率的折线图&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;ID&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;洪水概率&#x27;</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p><h1 id="洪水灾害的数据分析与预测基于多模型集成学习的方法研究"><a class="anchor" href="#洪水灾害的数据分析与预测基于多模型集成学习的方法研究">#</a> 洪水灾害的数据分析与预测：基于多模型集成学习的方法研究</h1><h2 id="摘要"><a class="anchor" href="#摘要">#</a> 摘要</h2><p>本研究围绕洪水灾害的分析与预测展开，利用多模型集成学习方法对洪水发生概率及其影响因素进行了系统性和科学性的探讨。研究通过对附件提供的 train.csv 数据进行预处理，确保数据的准确性和完整性，并基于多种相关性分析和预测模型，提出了有效的洪水预测和防洪减灾策略。</p><p>针对问题一，本文采用<strong>皮尔逊</strong>相关系数和<strong>斯皮尔曼</strong>等级相关系数对 20 个指标与洪水发生概率之间的相关性进行了详细量化分析。结果显示，<strong>基础设施恶化、季风强度、大坝质量、地形排水、河流管理</strong>等指标与洪水发生概率具有显著相关性。基于这些分析，研究提出了一系列合理的防洪措施建议，如加强基础设施建设、优化河流管理、提升大坝质量等，以有效降低洪水发生的风险。</p><p>针对问题二，本文利用<strong> K-means</strong> 聚类方法将洪水发生概率划分为高、中、低三个风险等级，并深入分析了不同风险等级下的<strong>指标特征差异</strong>。通过对高、中、低风险类别中各项影响因素的详细探讨，建立了四个具有高度准确性的预警评价模型，明确了不通类别洪水风险的主要特征，为洪水风险管理提供了科学依据。</p><p>针对问题三，研究基于前述相关性分析结果，分别构建了<strong>多元线性回归模型、梯度提升树模型、决策树模型、随机森林模型和逻辑回归模型</strong>，系统评估了各模型的预测性能。为了提高预测的准确性和稳定性，本文通过 Bagging 技术整合多种模型的预测结果，提出了一个集成学习模型。该模型在对 test.csv 数据进行验证时，显示出优异的预测性能，进一步提升了洪水发生概率预测的准确性。</p><p>针对问题四，本文利用集成学习模型对 test.csv 中的数据进行了洪水发生概率的预测，并将预测结果填入 submit.csv 中。通过绘制直方图和折线图，对预测结果的分布进行了统计分析，并通过<strong> Quantile-Quantile 图、Shapiro-Wilk 检验、Kolmogorov-Smirnov 测试和 Anderson-Darling 检验</strong>，验证了预测结果的分布特性。结果表明，模型预测的洪水发生概率分布较为合理，但不完全符合正态分布，具有一定的重尾现象，进一步验证了模型的有效性和可靠性。</p><p>总体而言，本研究不仅为洪水灾害的预测提供了有效的数学模型和工具，也为制定科学合理的防洪减灾策略提供了重要参考和借鉴。通过对数据的深入分析和模型的多样化应用，本文为相关领域的研究和实践提供了新的思路和方法。本文的研究成果不仅具有重要的理论价值，也在实际应用中具有广泛的推广潜力。</p><p><em><strong>* 关键词 *</strong></em>：洪水灾害，数据分析，多模型集成学习，皮尔逊相关系数，斯皮尔曼等级相关系数，K-means 聚类，风险分类，Bagging 技术，预警评价模型，正态分布分析，防洪减灾措施</p><h2 id="目录"><a class="anchor" href="#目录">#</a> 目录</h2><p><a href="#_Toc28221"><em><strong>* 洪水灾害的数据分析与预测：基于多模型集成学习的方法研究 *</strong></em></a></p><p><a href="#_Toc26087"><em><strong>* 一、问题背景与问题重述 *</strong></em></a></p><p><a href="#_Toc20599">1.1 问题背景</a></p><p><a href="#_Toc12872">1.2 已知条件</a></p><p><a href="#_Toc9667">1.3 问题重述</a></p><p><a href="#_Toc18489"><em><strong>* 二、 模型假设 *</strong></em></a></p><p><a href="#_Toc21271"><em><strong>* 三、 符号说明 *</strong></em></a></p><p><a href="#_Toc24812"><em><strong>* 四、 研究思路 *</strong></em></a></p><p><a href="#_Toc23690"><em><strong>* 五、*</strong></em> <em><strong>* 问题一的建模与求解 *</strong></em></a></p><p><a href="#_Toc10510">5.1 数据预处理</a></p><p><a href="#_Toc14684">5.2 问题一的模型建立与求解</a></p><p><a href="#_Toc19215">5.2.1 用皮尔逊相关系数和斯皮尔曼等级相关系数研究各指标与洪水发生概率的相关性。</a></p><p><a href="#_Toc8589">5.2.2 通过多元线性回归、递归提升树、随机森林对指标的优劣进行分析。</a></p><p><a href="#_Toc16643">5.2.3 结果分析</a></p><p><a href="#_Toc26819">5.3 建议与措施</a></p><p><a href="#_Toc21065"><em><strong>* 六、问题二的建模与求解 *</strong></em></a></p><p><a href="#_Toc22996">6.1 问题 2 的分析</a></p><p><a href="#_Toc913">6.1.1 数据预处理</a></p><p><a href="#_Toc24367">6.1.2 聚类方法选择</a></p><p><a href="#_Toc13314">6.2 k-means 建模实现与分析</a></p><p><a href="#_Toc26613">6.2.1 模型建立</a></p><p><a href="#_Toc14469">6.2.2 高中低风险特征分析</a></p><p><a href="#_Toc13962">6.2.3 建立发生洪水不同风险预警模型</a></p><p><a href="#_Toc3720">6.3 模型灵敏度分析</a></p><p><a href="#_Toc27867">6.3.1 基于特征变化率的灵敏度计算</a></p><p><a href="#_Toc18865">6.3.2 基于预测结果熵值的灵敏度计算</a></p><p><a href="#_Toc2991">七、 问题三的建模与求解</a></p><p><a href="#_Toc15454">7.1 指标选取</a></p><p><a href="#_Toc1478">7.1.2 基于过滤法选取关键指标</a></p><p><a href="#_Toc6370">7.2 各模型结果分析</a></p><p><a href="#_Toc19592">7.3 最终模型建立</a></p><p><a href="#_Toc31192"><em><strong>* 八、问题四的建模与求解 *</strong></em></a></p><p><a href="#_Toc11155">8.1 结果预测</a></p><p><a href="#_Toc29202">8.2 结果可视化</a></p><p><a href="#_Toc12659">8.3 判断洪水发生概率是否符合正态分布</a></p><p><a href="#_Toc2276">8.3.2 Shapiro-Wilk 检验</a></p><p><a href="#_Toc5950">8.3.3 Kolmogorov-Smirnov 测试</a></p><p><a href="#_Toc28126">8.3.4 Anderson-Darling 检验</a></p><p><a href="#_Toc26091"><em><strong>* 九、*</strong></em><em><strong>* 模型分析 *</strong></em></a></p><p><a href="#_Toc18219">9.1 模型的优点</a></p><p><a href="#_Toc20389">9.2 模型的缺点</a></p><p><a href="#_Toc22586">9.3 模型的推广</a></p><p><a href="#_Toc9316"><em><strong>* 十、*</strong></em><em><strong>* 总结 *</strong></em></a></p><p><a href="#_Toc27225"><em><strong>* 十一、*</strong></em><em><strong>* 参考文献 *</strong></em></a></p><p><a href="#_Toc17550"><em><strong>* 十二、*</strong></em><em><strong>* 附录 *</strong></em></a></p><h1><a class="anchor" href="#">#</a></h1><h1 id="-2"><a class="anchor" href="#-2">#</a></h1><p>**<br>**</p><h1 id="一-问题背景与问题重述"><a class="anchor" href="#一-问题背景与问题重述">#</a> <em><strong>* 一、问题背景与问题重述 *</strong></em></h1><h2 id="11-问题背景"><a class="anchor" href="#11-问题背景">#</a> <em><strong>*1.1 问题背景 *</strong></em></h2><p>洪水作为一种由暴雨、急剧融冰化雪、风暴潮等自然因素引发的自然灾害，对全球范围内的社会经济系统构成了重大威胁。从先秦时期《尚书・尧典》到现代全球频发的洪水事件，无不彰显着洪水灾害的广泛性与严重性。进入 21 世纪以后，随气候环境的严重变化和人类活动加剧，洪水灾害的频率和强度显著提高，对人类社会造成了前所未有的挑战，本论文基于此，以附件里的数据为参考，对于洪水发生概率及其影响因素进行了深入的分析。</p><h2 id="12-已知条件"><a class="anchor" href="#12-已知条件">#</a> <em><strong>*1.2 已知条件 *</strong></em></h2><p>​	为了研究与洪水发生概率有关的各个指标，本题提供了 train.csv、test.csv、submit.csv 三个附件为研究依据。</p><p>1.1048574 个包含洪水事件 id、季风强度、地形排水、河流管理、森林砍伐、城市化、气候变化、大坝质量、淤积、农业实践、侵蚀、无效防灾、排水系统、海岸脆弱性、滑坡、流域、基础设施恶化、人口得分、湿地损失、规划不足、政策因素和发生洪水的概率的事件训练表格。（train.csv）</p><p>2.745305 个包含洪水事件 id 和上述 20 个指标得分，缺少洪水概率的测试表格。(test.csv)</p><p>3. 包含 test.csv 的洪水事件 id，缺少洪水概率的事件提交表格。(submit.csv)</p><p>为了更好的对洪水预防提供建议，本小组选取以下文章作为参考文献：</p><pre><code> 1.选取对于河流管理因素治理的参考[1]

 2.选取对于水利工程等基础设施因素治理的参考[2][3]

 3.选取对于气候变化因素治理的参考[4][5]

 4.选取对于降水预测治理的参考[6]
</code></pre><h2 id="13-问题重述"><a class="anchor" href="#13-问题重述">#</a> <em><strong>*1.3 问题重述 *</strong></em></h2><p>​	问题 1 通过分析 train.csv 中的数据，分析并可视化上述 20 个指标与洪水发生的关联的强弱性。并根据可能的原因针对洪水预防提出建议和措施。</p><p>​	问题 2 将附件 train.csv 中洪水发生的概率聚类成不同的类别，分析具有高中低风险的洪水事件的指标特征。</p><p>​	问题 3 基于问题 1 中的指标分析结果建立洪水事件发生的概率预测模型。从 20 个指标中选取合适指标，预测洪水发生的概率，并验证模型准确性。并且尝试分析如果只用五个指标应该如何改进模型。</p><p>​	问题 4 基于问题 3 建立的洪水发生概率预测模型，预测 test.csv 中事件发生洪水概率并填入 submit.csv 中，并绘制发生洪水概率的直方图和折线图，分析结果是否呈现正态分布。</p><h1 id="二-模型假设"><a class="anchor" href="#二-模型假设">#</a> <strong>二、</strong> <em><strong>* 模型假设 *</strong></em></h1><p>\1. 假设题目所给的数据真实可靠；</p><p>\2. 假设洪水事件由且仅由这 20 个指标影响；</p><p>\3. 假设洪水发生概率与其他事件呈线性关系。</p><h1 id="三-符号说明"><a class="anchor" href="#三-符号说明">#</a> <strong>三、</strong> <em><strong>* 符号说明 *</strong></em></h1><table><thead><tr><th>符号</th><th>说明</th></tr></thead><tbody><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps78.jpg)</td><td>洪水发生概率</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps79.jpg)</td><td>均方误差</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps80.jpg)</td><td>余弦相似度</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps81.jpg)</td><td>第！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps82.jpg) 个样本的真实值</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps83.jpg)</td><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps84.jpg) 的平均值</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps85.jpg)</td><td>模型对第！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps86.jpg) 个样本的预测值</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps87.jpg)</td><td>第！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps88.jpg) 个指标，以 csv 出现的先后排序</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps89.jpg)</td><td>第！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps90.jpg) 个指标的系数</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps91.jpg)</td><td>皮尔逊相关系数</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps92.jpg)</td><td>斯皮尔曼等级相关系数</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps93.jpg)</td><td>第 �� 个数据点的两个变量的等级差值</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps94.jpg)</td><td>聚类中心</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps95.jpg)</td><td>分配给聚类中心！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps96.jpg) 的数据点的集合</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps97.jpg)</td><td>数据点！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps98.jpg) 与聚类中心！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps99.jpg) 的距离</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps100.jpg)</td><td>统计量</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps101.jpg)</td><td>权重系数</td></tr><tr><td>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps102.jpg)</td><td>第！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps103.jpg) 个排序后的样本数据点</td></tr></tbody></table><h1 id="四-研究思路"><a class="anchor" href="#四-研究思路">#</a> <strong>四、</strong> <em><strong>* 研究思路 *</strong></em></h1><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps104.jpg)</p><h1 id="五-问题一的建模与求解"><a class="anchor" href="#五-问题一的建模与求解">#</a> <strong>五、</strong> <em><strong>* 问题一的建模与求解 *</strong></em></h1><p>在分析该题目的时候，本文首先使用相关系数判定各数据的相关性，然后使用多元回归算法建立这 20 个指标关于洪水概率的数学模型，最后使用梯度提升树、随机森林回归模型预测洪水发生的概率，并进行了 K 折交叉验证、模型评估、特征重要性分析以及损失曲线的绘制。</p><h2 id="51-数据预处理"><a class="anchor" href="#51-数据预处理">#</a> <em><strong>*5.1 数据预处理 *</strong></em></h2><p>（1）针对给出的超过 100 万条的洪水事件数据，本文对 train.csv 进行基本的数据清洗，处理缺失值、异常值。</p><p>（2）由于 id 列对于数据分析无直接用处，本文对该列进行清洗处理。</p><h2 id="52-问题一的模型建立与求解"><a class="anchor" href="#52-问题一的模型建立与求解">#</a> <em><strong>*5.2 问题一的模型建立与求解 *</strong></em></h2><h3 id="521用皮尔逊相关系数和斯皮尔曼等级相关系数研究各指标与洪水发生概率的相关性"><a class="anchor" href="#521用皮尔逊相关系数和斯皮尔曼等级相关系数研究各指标与洪水发生概率的相关性">#</a> <em><strong>*5.2.*</strong></em><em><strong>*1 用皮尔逊相关系数和斯皮尔曼等级相关系数研究各指标与洪水发生概率的相关性。*</strong></em></h3><p>​	首先，我们根据式一计算各个指标对于洪水概率的皮尔逊相关系数：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps105.jpg) （1）</p><p>​	斯皮尔曼等级相关系数和！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps106.jpg) 的计算公式如式二三所示：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps107.jpg) （2）</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps108.jpg) （3）</p><p>本文使用.core () 函数，通过皮尔逊相关系数公式和斯皮尔曼等级相关系数来对各个指标和洪水概率的相关系数进行计算，最终得到相关系数表格一：</p><p>表 1 相关系数前 10 的指标</p><table><thead><tr><th>指标</th><th>皮尔逊相关系数</th><th>斯皮尔曼等级相关系数</th></tr></thead><tbody><tr><td>基础设施恶化</td><td>0.190152</td><td>0.1813993896616919</td></tr><tr><td>季风强度</td><td>0.189259</td><td>0.18027659009022504</td></tr><tr><td>大坝质量</td><td>0.187676</td><td>0.17945801481789098</td></tr><tr><td>地形排水</td><td>0.187643</td><td>0.18048274098179304</td></tr><tr><td>河流管理</td><td>0.187202</td><td>0.1789312726571455</td></tr><tr><td>淤积</td><td>0.186914</td><td>0.17873822145951626</td></tr><tr><td>人口得分</td><td>0.186285</td><td>0.1779930987957486</td></tr><tr><td>气候变化</td><td>0.185311</td><td>0.17711933159486049</td></tr><tr><td>滑坡</td><td>0.184926</td><td>0.17663628560684871</td></tr><tr><td>森林砍伐</td><td>0.184420</td><td>0.1770136418593642</td></tr></tbody></table><p>最终可视化图片为：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps109.jpg)![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps110.jpg)</p><p>图 1 不同因素对洪水概率的影响 图 2 不同因素与洪水概率的斯皮尔曼相关系数</p><p>为了便于分析相关性差异，我们将皮尔逊相关系数减去 0.175、斯皮尔曼等级相关系数减去 0.17 后再次可视化，得到结果为：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps111.jpg)![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps112.jpg)</p><p>图 3 调整后的相关系数柱状图</p><h3 id="522通过多元线性回归-递归提升树-随机森林对指标的优劣进行分析"><a class="anchor" href="#522通过多元线性回归-递归提升树-随机森林对指标的优劣进行分析">#</a> <em><strong>*5.2.*</strong></em><em><strong>*2 通过多元线性回归、递归提升树、随机森林对指标的优劣进行分析。*</strong></em></h3><p>先在模型一中，通过多元线性回归拟合模型里各指标的系数，通过计算预测值！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps113.jpg) 和真实值！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps114.jpg) 之间的均方误差！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps115.jpg)、在测试集的分数！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps116.jpg) 以及余弦相似度用以评估模型性能，表达式如下：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps117.jpg) (4)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps118.jpg) (5)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps119.jpg) (6)</p><h4 id="5221-线性回归模型"><a class="anchor" href="#5221-线性回归模型">#</a> <em><strong>*5.2.*</strong></em><em><strong>*2.1 线性回归模型 *</strong></em></h4><p>多元线性回归模型是一种常用的统计分析方法，该模型表达式为：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps120.jpg) (7)</p><p>​	首先初始化回归系数，然后使用梯度下降或正规方程法来迭代更新回归系数，最小化损失函数（均方误差），训练完成后，输出不同特征的回归系数，这些系数反映了每个特征对预测目标的影响程度。</p><p>使用训练数据（X_train 和 y_train）训练线性回归模型，并使用模型对测试集 X_test 进行预测，返回预测值 y_pred，并且得到性能指标如下：</p><p>表 2 线性回归模型性能指标</p><table><thead><tr><th>性能指标</th><th>数值</th></tr></thead><tbody><tr><td>测试集分数</td><td>0.8444528508563269</td></tr><tr><td>均方误差</td><td>0.0004043169737229802</td></tr><tr><td>余弦相似度</td><td>0.9992129608069907</td></tr><tr><td>截距</td><td>-0.053357024379501294</td></tr></tbody></table><p>最终得到公式：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps121.jpg)(8)</p><h4 id="5222-梯度提升树模型"><a class="anchor" href="#5222-梯度提升树模型">#</a> <em><strong>*5.2.*</strong></em><em><strong>*2.2 梯度提升树模型 *</strong></em></h4><p>本文使用梯度提升树（Gradient Boosting Trees, GBT）模型来预测洪水发生概率，并通过一系列步骤来评估模型的性能、特征重要性以及训练过程中的损失变化。该模型通过组合多个弱学习器（决策树）形成一个强学习器，算法通过递归逐步提升优化模型的预测能力，在每一轮迭代中，计算当前模型的残差并基于该残差训练新的决策树，而新训练的决策树会被加入到模型中，与之前的模型加权融合以提高预测能力。</p><p>本文设置迭代次数为 500，即构建 500 棵决策树拟合数据。并使用 5 折交叉验证帮助评估模型在不同数据子集上的性能，估计模型泛化能力，其输出的每折交叉评分为 R2 的值。本文使用完整的训练集（X_train 和 y_train）训练梯度提升树模型，并在测试集上评估模型性能。同时输出特征重要性并使用 matplotlib 库绘制特征重要性的条形图，以直观地展示哪些特征对模型预测的贡献最大。</p><p>本文使用 staged_predict 方法获取模型训练集和测试集上每轮迭代后的预测结果，计算并存储每轮迭代后的均方误差（MSE），以观察训练集和测试集上损失值的变化情况，进而使用 matplotlib 库绘制训练集和测试集上的损失值（MSE）随迭代次数变化的曲线，帮助观察模型是否过拟合（即训练集上的损失持续下降，而测试集上的损失在某个点后开始上升）。</p><p>以下为该模型的性能指标：</p><p>表 3 梯度提升树模型性能指标</p><table><thead><tr><th>性能指标</th><th>数值</th></tr></thead><tbody><tr><td>测试集分数</td><td>0.8367668430250391</td></tr><tr><td>交叉验证平均值</td><td>0.8375024016574082</td></tr><tr><td>均方误差</td><td>0.00042429537540674935</td></tr><tr><td>余弦相似度</td><td>0.9991741505605559</td></tr></tbody></table><p>最终可视化结果如下图：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps122.jpg)</p><p>图 4 梯度提升树各指标的重要性</p><h4 id="5223-随机森林模型"><a class="anchor" href="#5223-随机森林模型">#</a> <em><strong>*5.2.*</strong></em><em><strong>*2.3 随机森林模型 *</strong></em></h4><p>随机森林模型是多个决策树的集成，通过对每棵树的预测结果进行投票或加权平均来得到最终的预测结果。随机森林通过随机选择样本和特征来构建每棵树，从而增加模型的鲁棒性和泛化能力。整个过程线通过自助法（Bootstrap）从训练数据中有放回地抽样，生成多个不同的子集，然后对每个子集，随机选择部分特征，训练一棵决策树，最后将所有树的预测结果进行投票（分类问题）或加权平均（回归问题），得到最终的预测结果，从而输出不同指标的权重（特征重要性），表示每个特征在所有树中的分割点选择过程中所贡献的纯净度提升的总和。</p><p>本文设置随机森林的决策树数量为 200，通过抽取部分训练数据训练模型并使用投票策略以加快模型训练速度并提高模型精度。该模型的性能指标如表 4 所示：</p><p>表 4 随机森林模型性能指标</p><table><thead><tr><th>性能指标</th><th>数值</th></tr></thead><tbody><tr><td>测试集分数</td><td>0.6559609909975268</td></tr><tr><td>预测准确率</td><td>0.000894267826368524</td></tr><tr><td>均方误差</td><td>0.000894267826368524</td></tr><tr><td>余弦相似度</td><td>0.9982658517029762</td></tr></tbody></table><p>随机森林得到的特征重要性如下图：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps123.jpg)</p><p>图 5 随机森林各指标的重要性</p><h3 id="523-结果分析"><a class="anchor" href="#523-结果分析">#</a> <em><strong>*5.2.3*</strong></em> <em><strong>* 结果分析 *</strong></em></h3><p>根据皮尔逊相关系数和斯皮尔曼等级相关系数结果分析，可以发现 **** 基础设施恶化、季风强度、地形排水 *<em><strong> 三个指标与洪水发生概率的相关性最强，而</strong></em> * 海岸脆弱性、排水系统、侵蚀 **** 三个指标与洪水发生概率的相关性最弱。</p><p>针对指标优劣性的分析，本文采用了线性回归模型、递归提升树回归模型和随机森林模型。本文对三种模型得出的指标系数进行降序排列，并记前十项指标得分为 1，后十项指标得分为 - 1，将三种模型的指标得分进行累加，通过比较各指标得分情况判断指标的重要性。最终得到各指标得分条形图。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps124.jpg)</p><p>图 6 各指标得分条形图</p><p>由图可以发现，经过三个模型的综合分析，<strong>森林砍伐</strong>与洪水发生概率的相关性最强，而<strong>淤积、滑坡、城市化、海岸脆弱性、气候变化、侵蚀、人口得分、大坝质量、河流管理、地形排水</strong>十个指标与洪水发生概率的相关性较强，<strong>农业实践、排水系统、规划不足、季风强度、政策因素、流域、基础设施恶化</strong>七个指标与洪水发生概率的相关性较弱 **，湿地损失、无效防灾 ** 两个指标与洪水发生概率的相关性最弱。</p><p>尽管三个模型得出的指标相关性排序结果各有差异，但观察数据可以发现各指标重要性数值差异并不大，因此在考虑洪水发生概率及应对洪水灾害时，<em><strong>* 以上 20 个指标都需要考虑 *</strong></em>。</p><p>综上，本文认为 **** 森林砍伐、淤积、气候变化、人口得分、大坝质量、河流管理、地形排水 *<em><strong> 与洪水发生概率的相关性</strong></em> * 最大 *<em><strong>，而</strong></em> * 无效防灾、湿地损失 *<em><strong> 与洪水发生概率的相关性</strong></em> * 不大 ****。</p><p>对于各指标与洪水发生的相关性的原因，本文作出以下分析：森林通过树木和植被可以有效保持土壤，减少地表径流，并且树木的根系可以固土。而当森林遭到砍伐且暴雨发生时，土壤失去固定易被冲刷，导致泥沙进入河流，削弱河道蓄水能力，导致水量激增进而增加洪水发生的概率。同时砍伐森林会降低蒸腾作用，导致削弱大量水分返回大气的效果，进而导致地表径流增加，增加洪水发生概率。淤积会减少河流、湖泊和水库的蓄水容量，同时改变河道的水流速度和流向，增加洪水发生的复杂性和频率，因此降水量较大时，水体更易溢出河道，导致洪水发生。针对气候变化，现世界范围气候变化多端，导致极端天气事件（如暴雨、飓风）频率增加，降水在短时间内过于集中，导致洪水灾害发生。同时，人口增长和城市化导致土地利用变化，如水泥地面增加减少地表渗透，加剧地表径流，且人口密度高的地区，排水系统和其他基础设施在暴雨时容易超负荷，进而增加洪水发生的概率。大坝在控制水流和调节河流水位方面至关重要，若大坝质量不佳，可能在洪水期间受损坍塌，导致严重泄洪。因此良好的河流管理（如疏浚河道、修建防洪堤坝）可以有效地预防和控制洪水，若河流管理不恰当，即难以预测灾害发生。良好的地形排水设计能够快速排除积水，进而减少洪水风险，且低洼地区和排水不通畅的区域更容易发生洪水。因此地形排水与洪水发生概率的相关性也较大。[2]</p><p>针对低相关性的指标，虽然无效的防灾措施可能会增加洪水的破坏程度，但它们并不会直接增加洪水发生的概率，无效防灾主要影响的是洪水造成的损失，而不是洪水发生的本身。而湿地确实具有调节水流、减缓洪水的功能，但其影响相对于森林和河流管理可能较小，湿地损失更多影响的是水生态系统的健康和水质，而不是洪水发生的直接原因。</p><h2 id="53建议与措施"><a class="anchor" href="#53建议与措施">#</a> <em><strong>*5.3*</strong></em><em><strong>* 建议与措施 *</strong></em></h2><p>结合相关文献，本文针对以下方面给出提前预防洪水的建议和措施：</p><p>（1）加强森林保护和恢复：实施大规模的植树造林项目，恢复被破坏的森林，扩大森林范围，提高土壤的吸水和固土能力。同时制定并严格执行森林保护法律法规，防止非法乱砍和乱伐现象。</p><p>（2）改善河流管理和治理：定期清理河道中的淤积物，保持河道的蓄水和排水能力 [1]。同时需要修建防洪堤坝、水闸等设施，并定期检查和维护，确保其在涨洪期间可以正常运行 [2]。</p><p>（3）提高大坝质量和管理水平：先需要对现有的大坝进行定期检测和维护，确保其结构安全。同时采用先进的技术和材料，科学设计和建造新大坝，提高大坝的防洪能力 [2]。</p><p>（4）应对气候变化：利用气象监测和数据分析技术，建立暴雨早期预警系统，及时发布洪水预警信息，提前做好预防。需要加强对气候变化对区域降水模式和洪水风险影响的研究，为防洪决策提供科学依据 [3]。</p><p>（5）改善城市和农村排水系统：升级和改造城市和农村的排水系统，确保在暴雨期间能迅速排除积水 [4]。除此之外，还需要定期清理排水管道和沟渠，防止淤积和堵塞。</p><p>（6）加强公众教育和应急准备：除了进行生态治理，还需要通过社区活动和媒体宣传，提高公众的防灾意识和应急响应能力。同时制定详细的洪水应急预案，并定期进行演练，确保居民在洪水来临时能够迅速、安全撤离。</p><h1 id="六-问题二的建模与求解"><a class="anchor" href="#六-问题二的建模与求解">#</a> <em><strong>* 六、问题二的建模与求解 *</strong></em></h1><h2 id="61-问题2的分析"><a class="anchor" href="#61-问题2的分析">#</a> <em><strong>*6.1*</strong></em> <em><strong>* 问题 2 的分析 *</strong></em></h2><p>问题 2 要求将附件 train.csv 中的洪水发生概率进行聚类，并分析具有高、中、低风险的洪水事件的指标特征。然后选取合适的指标，计算不同指标的权重，建立不同风险的预警评价模型，最后进行模型的灵敏度分析。</p><h3 id="611-数据预处理"><a class="anchor" href="#611-数据预处理">#</a> <em><strong>*6.1.1*</strong></em> <em><strong>* 数据预处理 *</strong></em></h3><p>在进行聚类分析之前，首先需要对数据进行预处理：首先检查数据中是否存在缺失值，并选择合适的方法进行填补。例如，可以使用均值填充法或者插值法填补缺失值。</p><h3 id="612-聚类方法选择"><a class="anchor" href="#612-聚类方法选择">#</a> <em><strong>*6.1.2*</strong></em> <em><strong>* 聚类方法选择 *</strong></em></h3><p>选用 K 均值聚类（K-means clustering）方法对洪水发生概率进行聚类。K 均值聚类是一种非监督学习方法，通过迭代不断调整聚类中心的位置，使得每个数据点到其最近聚类中心的距离之和最小。本文使用该算法将洪水发生概率分为高、中、低三个风险等级。该算法通过迭代的方式寻找 k 个簇的划分方案，使得聚类结果对应的代价函数最小，代价函数通常定义为各个样本点到其所属簇中心点的误差平方和。该算法利用 N 维空间距离对初始聚类中心进行学习，得到最终聚类中心，步骤如下：</p><p>\1. 首先选择要将数据集分成 3 个簇，然后随机选择 3 个数据点作为初始簇中心（即为高、中、低三个风险等级）。</p><p>\2. 将每个数据点分配到距离其最近的簇中心。</p><p>\3. 根据分配的数据点更新簇中心点，即通过计算属于每个簇的数据点的平均值实现。</p><p>\4. 重复步骤 2 和 3 直至簇中心点不再发生变化。</p><p>\5. 输出 3 个簇和每个簇的中心点。</p><h2 id="62-k-means建模实现与分析"><a class="anchor" href="#62-k-means建模实现与分析">#</a> <em><strong>*6.2*</strong></em> <em><strong>*k-means 建模实现与分析 *</strong></em></h2><h3 id="621-模型建立"><a class="anchor" href="#621-模型建立">#</a> <em><strong>*6.2.1*</strong></em> <em><strong>* 模型建立 *</strong></em></h3><p>首先，我们以公式（9）计算数据点到聚类中心的距离，然后对于每个数据点，用公式（10）找到最近的聚类中心。最后重新计算每个聚类中心的位置，使其等于簇内所有数据点的均值。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps125.jpg) (9)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps126.jpg) (10)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps127.jpg) (11)</p><p>​ 重复上述步骤，直到聚类中心不再发生改变，我们可以得到各风险层级聚类中心表格。</p><p>表 5 各风险层级聚类中心</p><table><thead><tr><th>风险等级</th><th>聚类中心</th></tr></thead><tbody><tr><td>高风险</td><td>0.56840101</td></tr><tr><td>中风险</td><td>0.50701183</td></tr><tr><td>低风险</td><td>0.44612593</td></tr></tbody></table><p>洪水发生概率更靠近哪个聚类中心则被划分为哪个聚类。例如，当洪水发生概率为 0.55 时为高风险，洪水发生概率为 0.51 时为中风险，洪水发生概率为 0.42 时为低风险。</p><p>​	我们将区分开后的 train.csv 分别保存为高风险.csv、中风险.csv 和低风险.csv 以便于后续特征分析。</p><h3 id="622-高中低风险特征分析"><a class="anchor" href="#622-高中低风险特征分析">#</a> <em><strong>*6.2.2*</strong></em> <em><strong>* 高中低风险特征分析 *</strong></em></h3><p>​	对于不同风险等级的数据，我们首先分别计算其 20 个指标的平均值和方差。然后通过式（12）进行标准化数据，即将不同量纲的数据转换到一个相同的尺度上，使其均值为 0，标准差为 1。这样可以消除不同特征之间的量纲差异，使得它们在同一尺度上进行比较。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps128.jpg) (12)</p><p>​	据此公式，将 20<em>3</em>2 个指标的数据标准化和特征可视化于图像。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps129.jpg)</p><p>图 7 不同风险类别的标准化特征平均值条形图</p><p>​	通过对图 7 的分析，我们可以知道高中险聚类中，大坝质量、河流管理两个指标的影响最大，而流域、人口得分、规划不足三个指标的影响最小。在中风险聚类中，河流管理、侵蚀、无效防灾、海洋脆弱性、湿地损失、规划不足六个指标的影响最大，而季风强度、基础设施恶化两个指标的影响最小。在低风险聚类中，河流管理、大坝质量两个指标的影响极为突出，而流域、人口得分、规划不足三个指标的影响最小。由此可以得出，洪水发生高风险地区主要受设施建设影响，而中风险地区主要受地质因素影响，低风险地区也主要受设施建设影响。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps130.jpg)</p><p>图 8 不同风险类别的标准化特征方差条形图</p><p>​	通过对图 8 的分析，我们可以知道在三个聚类中，地形排水、城市化、海岸脆弱性、政策因素的不确定性和波动性都很高，而季风强度、森林砍伐、气候变化、基础设施恶化四个指标的不确定性和波动性较低，也可以看出其对于各聚类洪水发生事件都有影响，进而体现其相关性强。除以上指标外，在高风险聚类中城市化、大坝质量、流域等指标的不确定性和波动性较强，即其在高风险聚类中的相关性并不强。</p><p>​	然后，我们利用差值图，通过计算高风险和低风险类别的特征差异，来直观展示哪些特征在不同风险类别中存在显著差异，为了突出差异，我们将最后的结果均减去 0.8，以此来筛选具有显著差异的因素。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps131.jpg) (13)</p><p>​	![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps132.jpg)</p><p>图 9 高风险与低风险类别特征平均值差异条形图</p><p>​	最后我们使用热力图，通过热力图结合层次聚类，直观展示不同风险类别中各特征的表现，识别特征之间的相似性和差异性，帮助分析洪水风险的主要影响因素。对标准化后的数据进行层次聚类分析，将相似的特征聚合在一起。根据聚类结果对数据进行重新排序，以确保相似的特征相邻。使用热力图将重新排序后的数据可视化，用颜色表示数据值的大小。最终得到该不同风险与指标间的热力图。在该热力图中，颜色越淡则代表该指标与该聚类中的洪水发生相关性越弱，而颜色越浓则表示其相关性越强。因此，可以直观分析得到在 **** 高风险聚类 *<em><strong> 中，</strong></em>* 河流管理、大坝质量、湿地损失、基础设施恶化 *<em><strong> 四个指标的相关性最强，而</strong></em> * 低风险聚类 *<em><strong> 中</strong></em> * 地形排水、淤积、人口得分、流域、滑坡 *<em><strong> 等指标相关性最强，在</strong></em> * 中风险聚类 *<em><strong> 则</strong></em> * 季风强度、气候变化、基础设施恶化 **** 三个指标的相关性最强。通过以上分析也可以更好帮助我们在不同风险聚类更好制定预防措施并分析风险来源。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps133.jpg)</p><p>图 10 各指标层次聚类热力图</p><h3 id="623-建立发生洪水不同风险预警模型"><a class="anchor" href="#623-建立发生洪水不同风险预警模型">#</a> <em><strong>*6.2.*</strong></em><em><strong>*3*</strong></em> <em><strong>* 建立发生洪水不同风险预警模型 *</strong></em></h3><p>​	为了建立一个有效的洪水风险预警评价模型，我们分别尝试了选取 20、问题 1 中前 15 个、前 10 个相关性最大的指标进行建模。结果显示当选取 20 个指标的情况下模型拟合程度 **** 最高 ****。故我们选取全部指标进行模型的建立。</p><p>​	为了构建洪水风险预警评价模型，我们选择了几种常见的机器学习模型，包括逻辑斯特回归、线性回归、梯度提升树、决策树和随机森林。这些模型在处理回归和分类问题时具有较好的表现，并且能够提供特征重要性分析。我们通过问题 1 中训练的模型，得出不同指标的权重，并且在本问题中增加了 **** 决策树 *<em><strong> 做法和逻辑斯特回归，使得预警模型更加的全面。并且我们定义了一个</strong></em> * MyModel**** 类，用于调用预警模型，并且以公式（14）来获取模型预测正确率（Model Score）。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps134.jpg) (14)</p><p>​</p><p>​	我们通过分析建立建立的五个模型 (线性回归:linear、梯度提升树：gb_tree、决策树：ju_tree、随机森林：rf_forest，逻辑斯特回归：logistic)，以 MyModel 调用模型得出结果，最终得到模型的性能可视化结果为：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps135.jpg)</p><p>图 11 各模型性能对比条形图</p><p>对比这三个图，我们可以发现，线性回归模型得到的模型性能最优。</p><h2 id="63-模型灵敏度分析"><a class="anchor" href="#63-模型灵敏度分析">#</a> <em><strong>*6.3*</strong></em> <em><strong>* 模型灵敏度分析 *</strong></em></h2><p>灵敏度分析的目标是评估不同模型对输入特征变化的响应程度。这一过程有助于识别哪些特征对模型的预测结果影响最大。为了深入探究线性回归、梯度提升树、决策树和随机森林四个模型的灵敏度。我们采用两种方法进行灵敏度分析：基于特征变化率的灵敏度计算和基于预测结果熵值的灵敏度计算。</p><p>而逻辑斯特模型不能使用特征变化率的灵敏度计算，因为逻辑斯特模型的输出是通过对线性组合后的特征进行 Sigmoid 函数变换而得：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps136.jpg) (15)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps137.jpg) (16)</p><p>特征变化率假设特征与目标变量之间是线性关系，但逻辑斯蒂回归是非线性的。这使得特征值的简单变化并不能直接线性地反映在输出概率的变化上，并且 Sigmoid 函数具有非线性性质，在极端值处（接近 0 或 1）变化很小，而在中间值（接近 0.5）变化较大。这意味着特征值的相同变化在不同范围内对输出概率的影响不同，导致特征变化率无法简单地反映这种非线性变化。这种非线性会导致特征变化率难以稳定地捕捉特征变化对输出的影响。</p><h3 id="631-基于特征变化率的灵敏度计算"><a class="anchor" href="#631-基于特征变化率的灵敏度计算">#</a> <em><strong>*6.3.1*</strong></em> <em><strong>* 基于特征变化率的灵敏度计算 *</strong></em></h3><p>​	基于特征变化率的灵敏度计算通过评估输入特征变化对模型预测结果的影响，来衡量每个特征对模型预测的敏感度。对每个特征逐个进行变化，从最小值逐步增加到最大值，每次变化后记录模型的预测结果的平均值。然后利用式（17）计算变化率。最后计算所有变化率的平均值，作为该特征的灵敏度度量。变化率越高代表模型的灵敏度越高，效果越好。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps138.jpg) (17)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps139.jpg) (18)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps140.jpg)</p><p>​ 通过公式（18）计算得到四个模型的基于特征变化率的灵敏度。</p><p>​	![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps141.jpg) ![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps142.jpg)</p><p>图 12 线性回归模型属性平均变化率 图 13 递归提升树模型属性平均变化率</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps143.jpg) ![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps144.jpg)</p><p>图 14 决策树模型属性平均变化率 图 15 随机森林模型属性平均变化率</p><p>由上图可以看出，不同模型的特征变化率各有差异。图 12 表明在线性回归模型中，季风强度、地形排水、河流管理等特征的变化率较高，说明这些特征对模型的灵敏度较大。图 13 表明在梯度提升树模型中，季风强度和地形排水的变化率较高，其他特征的影响相对较小。图 14 表明决策树模型中，特征变化率分布较为均匀，无明显突出的特征。图 15 表明随机森林模型中整体变化率较低，说明模型对单个特征的依赖程度较低。</p><h3 id="632-基于预测结果熵值的灵敏度计算"><a class="anchor" href="#632-基于预测结果熵值的灵敏度计算">#</a> <em><strong>*6.3.2*</strong></em> <em><strong>* 基于预测结果熵值的灵敏度计算 *</strong></em></h3><p>​	基于预测结果熵值的灵敏度计算通过评估模型预测结果的分布不确定性，来衡量模型的灵敏度。熵值越高，表示模型的预测结果越不确定，灵敏度越高。我们运用公式（19）得到熵值：</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps145.jpg) (19)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps146.jpg)</p><p>在图中，我们展示了五种不同模型的灵敏度（通过熵值衡量）。熵值越高，表示模型的预测结果越不确定，预测的稳定性越差；熵值越低，表示模型的预测结果越确定，模型的稳定性越高。以下是针对五个模型的具体分析：</p><p>1. 线性回归模型：该模型熵值最高，接近 1.6，表示该模型的预测结果具有极高的不确定性。尽管线性回归模型简单且易于理解，但在处理复杂性较高的非线性关系时，表现不如其他模型。且高熵值意味着线性回归模型在特征变化时，预测结果的波动较大，对输入数据的变化非常敏感。</p><p>2. 梯度提升树模型：该模型熵值较高但低于线性回归模型，这表明该模型在预测结果的确定性方面表现比线性回归模型稍好。梯度提升树模型通过集成多棵决策树的预测结果，可以捕捉到更复杂的特征关系，因此在稳定性上有所提升。</p><p>3. 决策树模型：该模型熵值接近梯度提升树模型，但略高。这表明单棵决策树模型在处理特征变化时的稳定性相对较差，预测结果波动较大。且该模型容易过拟合，特别是在训练数据较少的情况下，熵值较高恰好反映了这点。</p><p>4. 随机森林模型：该模型熵值接近 1.4，略低于梯度提升树和决策树模型。随机森林模型通过集成多棵决策树，增强了预测结果的稳定性。较低的熵值表明随机森林模型在特征变化时的预测结果相对稳定，具有较好的鲁棒性。</p><p>5. 逻辑回归模型：该模型熵值与随机森林模型相近，逻辑回归模型在处理分类问题时具有较好的表现，但在特征变化率计算上存在局限性。熵值反映了逻辑回归模型在预测结果上的确定性，表现出一定的稳定性。</p><p>综上，线性回归模型的预测结果不确定性最高，灵敏度也较高。梯度提升树和决策树模型的熵值稍低于线性回归模型，但仍较高，说明这些模型在处理特征变化时有较大波动。随机森林模型和逻辑回归模型的熵值较低，即这些模型的预测结果稳定性表现较好，灵敏度较低。整体而言，随机森林模型在灵敏度分析中表现最佳，具有较高的预测稳定性和确定性。因此，随机森林模型可能是处理洪水概率预测的最佳选择。</p><h3 id="七-问题三的建模与求解"><a class="anchor" href="#七-问题三的建模与求解">#</a> <strong>七、</strong><em><strong>* 问题三的建模与求解 *</strong></em></h3><p><em><strong>*7.1 指标选取 *</strong></em><br><em><strong>*7.1.1*</strong></em><em><strong>* 初步选取指标 *</strong></em></p><p>本文基于问题 1 的结论，将 20 个指标分成三组逐步分析，最终选取拟合度最高的指标组。首先将 20 个指标全部进行考虑，再根据问题 1 的结论剔除相关性最弱的湿地损失、无效防灾两个指标，最后剔除湿地损失、无效防灾、农业实践、排水系统、规划不足、季风强度、政策因素、流域、基础设施恶化 9 个相关性不强的指标，保留剩余 11 个相关性较强的指标。本文通过以上三组指标选取重新训练已有模型，并对各模型计算测试集分数、均方误差、余弦相似度以评估模型性能。</p><h3 id="712-基于过滤法选取关键指标"><a class="anchor" href="#712-基于过滤法选取关键指标">#</a> <em><strong>*7.1.2*</strong></em> <em><strong>* 基于过滤法选取关键指标 *</strong></em></h3><p>过滤法是一种特征选择的方法，用于选择对模型预测结果影响最大的特征。其主要优点是不依赖于特定的模型，通过评估各个特征与目标变量之间的统计关系来进行特征选择。该方法可以帮助我们仅从题中 20 个指标的角度挖掘分析其价值高低，从而实现特征排序和选择。<br>在本文中，我们基于上述方法，从 11 个相关性较强的指标中筛选出 5 个关键影响指标。这 11 个指标包括森林砍伐、淤积、滑坡、城市化、海岸脆弱性、气候变化、侵蚀、人口得分、大坝质量、河流管理、地形排水。由于森林砍伐与洪水发生概率的相关性最强，因此我们首先保留该指标，并从剩余 10 个指标中筛选出 4 个最具影响力的指标，最终选取森林砍伐、淤积、大坝质量、河流管理、地形排水这 5 个关键指标。</p><p>使用这些关键指标，我们对模型进行重新训练并进行评估，以验证所选指标的有效性。</p><h2 id="72-各模型结果分析"><a class="anchor" href="#72-各模型结果分析">#</a> <em><strong>*7.2 各模型 *</strong></em><em><strong>* 结果分析 *</strong></em></h2><p>以上 4 组指标选取后各模型评估结果如下表：</p><table><thead><tr><th>模型</th><th>性能</th><th>全选指标</th><th>18 个指标</th><th>11 个指标</th><th>5 个指标</th></tr></thead><tbody><tr><td>多元线性回归</td><td>Model score</td><td>0.844453</td><td>0.740756</td><td>0.374618</td><td>0.181427</td></tr><tr><td>均方误差</td><td>0.000404</td><td>0.000674</td><td>0.001626</td><td>0.002128</td><td></td></tr><tr><td>余弦相似度</td><td>0.999213</td><td>0.998687</td><td>0.996832</td><td>0.995851</td><td></td></tr><tr><td>梯度提升树</td><td>Model score</td><td>0.841497</td><td>0.735467</td><td>0.378759</td><td>0.185468</td></tr><tr><td>均方误差</td><td>0.000412</td><td>0.000652</td><td>0.001615</td><td>0.002117</td><td></td></tr><tr><td>余弦相似度</td><td>0.999198</td><td>0.999132</td><td>0.996853</td><td>0.995872</td><td></td></tr><tr><td>决策树</td><td>Model score</td><td>0.053015</td><td>0.047629</td><td>-0.37999</td><td>0.061217</td></tr><tr><td>均方误差</td><td>0.002462</td><td>0.003184</td><td>0.003587</td><td>0.00244</td><td></td></tr><tr><td>余弦相似度</td><td>0.995203</td><td>0.994738</td><td>0.993019</td><td>0.995241</td><td></td></tr><tr><td>随机森林</td><td>Model score</td><td>0.499236</td><td>0.306385</td><td>0.156919</td><td>0.089517</td></tr><tr><td>均方误差</td><td>0.001302</td><td>0.001864</td><td>0.002191</td><td>0.002367</td><td></td></tr><tr><td>余弦相似度</td><td>0.997467</td><td>0.996748</td><td>0.995727</td><td>0.995385</td><td></td></tr></tbody></table><p>表 6 各指标分组下模型的性能评估</p><p>通过横向对比各个模型，我们可以发现预测的准确度随着指标的增加而增加，这表明我们需要选择所有指标作为参数才能得到最优的预测模型。而如果只能选择五个数据作为参数的话，经过实验表明，如果只能选取五个指标，那么通过滤波法选取的指标表现是最优的。</p><h2 id="73-最终模型建立"><a class="anchor" href="#73-最终模型建立">#</a> <em><strong>*7.3 最终模型建立 *</strong></em></h2><p>在本文中，我们采用 Bagging (Bootstrap Aggregating) 技术来建立最终的集成学习模型。Bagging 是一种集成学习方法，通过结合多个模型的预测结果来提高模型的准确性和稳定性。</p><p>Bagging 技术的基本思想是通过对数据进行有放回抽样生成多个不同的训练子集，然后在每个子集上训练一个模型，最后将所有模型的预测结果进行结合（例如通过投票或平均）来得到最终的预测结果。Bagging 的优点在于可以有效减少模型的方差，提高预测的准确性和稳定性。</p><p>在实现集成模型的过程中，我们首先创建了一个包含多个模型并实现了 fit、predict 和 score 方法的类 BaggingModel。在 fit 方法中，我们通过对训练数据进行有放回抽样，生成多个数据子集，并分别在每个子集上训练一个模型。随后，在 step 方法中，我们调整各个模型的权重，使得它们的和为 1。在 predict 方法中，我们使用各个模型对测试数据进行预测，并根据调整后的权重计算最终的加权预测结果。</p><p>最终我们得到了训练的最优解模型 (二十个指标) 和如果只能选取五个参数后优化的集成预测模型。</p><h1 id="八-问题四的建模与求解"><a class="anchor" href="#八-问题四的建模与求解">#</a> <em><strong>* 八、问题四的建模与求解 *</strong></em></h1><h2 id="81-结果预测"><a class="anchor" href="#81-结果预测">#</a> <em><strong>*8.1 结果预测 *</strong></em></h2><p>我们采用 7.3 中建立的集成学习模型，通过对 test.csv 模型里参数的读取和预测，最终得到各个 id 的洪水发生概率，并且保存在 submit.csv 中。</p><h2 id="82-结果可视化"><a class="anchor" href="#82-结果可视化">#</a> <em><strong>*8.2*</strong></em> <em><strong>* 结果可视化 *</strong></em></h2><p>本文使用 matplotlib 库绘制直方图和折线图。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps147.jpg)</p><p>图 16 洪水概率预测直方图 图 17 洪水概率预测 Q-Q 图！[img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps148.jpg)</p><p>图 18 洪水概率预测折线图</p><p><em><strong>*8.3 判断洪水发生概率是否符合正态分布 *</strong></em></p><p>满足正态分布的直方图呈现钟形，即中间高、两边低，且直方图左右对称，左右两侧的频数大致相等，正态分布图像只有一个峰值，数据集集中在某一个值附近。由此可见，可以通过直方图 16 初步判断洪水发生概率符合正态分布。</p><p>但仅凭直方图并不能得出最终结论，因此本文结合其他方法进行综合分析。<em><strong>*8.3.*</strong></em><em><strong>*1*</strong></em> <em><strong>*Quantile-Quantile 图 *</strong></em></p><p>Q-Q 图将样本数据从小到大排列，根据个数 n 计算每个数据点的理论分位数。对于正态分布，该理论分位数根据正态分布的累积分布函数计算得到，第 i 个数据点对应分位数常用方法如式（20）。计算后在图上绘制实际数据与理论分布分位数的点。如图 17 绘制了洪水概率预测 Q-Q 图，数据点偏离了对角线，特别是尾部部分，由此说明数据并不完全服从正态分布且存在重尾现象。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps149.jpg) (20)</p><h3 id="832-shapiro-wilk检验"><a class="anchor" href="#832-shapiro-wilk检验">#</a> <em><strong>*8.3.*</strong></em><em><strong>*2*</strong></em> <em><strong>*Shapiro-Wilk 检验 *</strong></em></h3><p>该方法将样本数据按从小到大排列，按排列后的顺序计算权重系数，这些系数依赖于样本大小 n 和正态分布的期望与方差，再由式（21）计算统计量 W 值，根据 W 值和样本大小 n 计算 p 值。本模型计算得到 p 值远小于 0.05，拒绝原假设，数据不服从正态分布。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps150.jpg) (21)</p><h3 id="833-kolmogorov-smirnov测试"><a class="anchor" href="#833-kolmogorov-smirnov测试">#</a> <em><strong>*8.3.*</strong></em><em><strong>*3*</strong></em> <em><strong>*Kolmogorov-Smirnov 测试 *</strong></em></h3><p>首先从每个数据点中减去平均值并除以标准差，将数据转换为均值为 0，标准差为 1 的分布，进行数据标准化。再使用 Kolmogorov-Smirnov 测试，比较该样本与正态分布的相似性。最终由式（22）返回样本分布与参考分布之间的最大差异 D，并输出 p 值即表示观察到的数据或更极端的数据在零假设下出现的概率。该模型最终预测概率的 p 值为 0.0，小于显著性水平（0.05），故认为数据不服从正态分布。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps151.jpg) (22)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps152.jpg) (23)</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps153.jpg)</p><h3 id="834-anderson-darling检验"><a class="anchor" href="#834-anderson-darling检验">#</a> <em><strong>*8.3.*</strong></em><em><strong>*4*</strong></em> <em><strong>*Anderson-Darling 检验 *</strong></em></h3><p>该方法将样本数据从小到大排序，计算该理论分布在数据点处的累加分布函数，并计算统计量，如式（24）所示。计算所得 A2 值远大于所有显著水平对应的临界值，拒绝原假设，数据不服从正态分布。</p><p>![img](file:///C:\Users\JackDu\AppData\Local\Temp\ksohtml23228\wps154.jpg)(24)</p><p>综上，预测洪水发生概率不服从正态分布。</p><h1 id="九-模型分析"><a class="anchor" href="#九-模型分析">#</a> <em><strong>* 九、*</strong></em><em><strong>* 模型分析 *</strong></em></h1><h2 id="91-模型的优点"><a class="anchor" href="#91-模型的优点">#</a> <em><strong>*9.1*</strong></em> <em><strong>* 模型的优点 *</strong></em></h2><p>​	本模型具有多个显著优点。首先，模型在数据预处理阶段对附件中的数据进行了全面的清理，确保了数据的准确性和完整性。这包括处理缺失值和异常值，从而提升了模型的可靠性。其次，本文通过详细的相关性分析，使用了皮尔逊系数和斯皮尔曼相关系数来评估各指标对洪水发生概率的影响，并可视化结果，使得相关性差异一目了然。此外，梯度提升树模型在拟合优度检验中表现出色，说明其能较为准确地预测各影响指标与洪水发生概率之间的关系。本文使用多个模型（如多元线性回归、梯度提升树、随机森林等）进行对比验证，确保了模型结果的准确性和可靠性。最后，本文在建立最终洪水发生概率预测模型时采用了 Bagging 技术，提高了模型的预测精度和稳定性。</p><h2 id="92-模型的缺点"><a class="anchor" href="#92-模型的缺点">#</a> <em><strong>*9.2*</strong></em> <em><strong>* 模型的缺点 *</strong></em></h2><p>​	尽管本模型在多个方面表现出色，但仍存在一些不足。首先，各模型训练后计算的各指标系数的相关性并不完全一致，表明在某些参数调整上还有提升空间。其次，尽管模型采用了多种技术手段进行优化，但在实际应用中，模型的泛化能力和鲁棒性仍需进一步验证。</p><h2 id="93-模型的推广"><a class="anchor" href="#93-模型的推广">#</a> <em><strong>*9.3*</strong></em> <em><strong>* 模型的推广 *</strong></em></h2><p>​	本模型具备广泛的应用前景。首先，本文所建立的模型经过充分训练，可以随时调用和调整选用的指标重新训练，以适应不同的实际需求。其次，针对洪水指标的影响分析和预防措施的提出，对政府、相关部门和公众采取防洪减灾措施具有积极指导意义。此外，本文使用的多元线性回归模型、梯度提升树模型、随机森林算法和 K-means 聚类算法适用于各领域的问题分析与预测，具有很强的通用性和借鉴意义。本文采用的模型性能评估指标和方法，如余弦相似度、均方误差、K 折交叉验证及测试集分数，可以有效评估其他模型的性能，值得推广使用。该模型可以用于洪水风险评估与管理，将洪水风险评估纳入城市发展计划，识别并减轻潜在的洪水危害，指导关键基础设施的建设和维护，以更好应对洪水灾害。利用该模型评估森林砍伐等指标对洪水风险的影响，可以规划再造林项目及其他相关措施以降低洪水风险。最后，该模型为分析其他自然灾害的影响因素提供了思路和指导，能够帮助预测和分析其他自然灾害的发生概率。</p><h1 id="十-总结"><a class="anchor" href="#十-总结">#</a> <em><strong>* 十、*</strong></em><em><strong>* 总结 *</strong></em></h1><p>本文通过对已知条件中的数据进行分析，研究了与洪水发生概率相关的指标，并通过数据集训练模型来预测洪水发生概率。在模型的构建和分析过程中，本文综合运用了多种方法，如相关性分析、多元线性回归、梯度提升树、随机森林和 Bagging 技术等。通过对比各个模型的性能，发现梯度提升树模型在预测洪水发生概率方面表现最佳。此外，本文筛选出森林砍伐、淤积、大坝质量、河流管理、地形排水五个关键指标，并验证了这些指标在预测洪水发生概率中的重要性。本研究不仅为洪水预测提供了有力工具，还提出了切实可行的防洪措施，能够有效降低洪水灾害带来的损失，为相关领域的研究和实际应用提供了宝贵的参考和借鉴。</p><h1 id="十一-参考文献"><a class="anchor" href="#十一-参考文献">#</a> <em><strong>* 十一、*</strong></em><em><strong>* 参考文献 *</strong></em></h1><p>[1] 耿晓君，杨晓茹，李爱花，等。现代化防洪减灾体系中河道治理思路探讨 [J]. 中国水利，2023 (14):11-14.</p><p>[2] 刘影。水利工程对洪水的影响研究 [J]. 水利技术监督，2016,24 (05):15-16.</p><p>[3] 冯燕。气候变化背景下塔里木河流域洪水灾害的经济社会影响及应对 [J]. 环境社会学，2023 (01):83-105+197.</p><p>[4] 原文林，杨逸凡，赵小棚，等。降雨监测与预报技术在防洪减灾中的应用进展 [J/OL]. 人民长江：1-8 [2024-07-06].<span class="exturl" data-url="aHR0cDovL2tucy5jbmtpLm5ldC9rY21zL2RldGFpbC80Mi4xMjAyLlRWLjIwMjQwNjA3LjEwMzQuMDA0Lmh0bWw=">http://kns.cnki.net/kcms/detail/42.1202.TV.20240607.1034.004.html</span>.</p><p>[5] 杨羽菲。中山市防洪减灾思路与对策研究 [J]. 广东水利水电，2023 (10):60-64.</p><p>[6] 刘欣。水利部将全面增强海河流域防洪减灾能力 [N]. 法治日报，2023-08-25 (005).DOI:10.28241/n.cnki.nfzrb.2023.004825.</p><div class="tags"><a href="/tags/Hexo/" rel="tag"><i class="ic i-tag"></i> Hexo</a> <a href="/tags/Front-Matter/" rel="tag"><i class="ic i-tag"></i> Front Matter</a> <a href="/tags/code/" rel="tag"><i class="ic i-tag"></i> code</a> <a href="/tags/recommendation/" rel="tag"><i class="ic i-tag"></i> recommendation</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">Edited on</span> <time title="Modified: 2024-07-08 00:53:47" itemprop="dateModified" datetime="2024-07-08T00:53:47+08:00">2024-07-08</time> </span><span id="bit/亚太数模——洪水预测/" class="item leancloud_visitors" data-flag-title="亚太数模——洪水" title="Views"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">Views</span> <span class="leancloud-visitors-count"></span> <span class="text">times</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> Donate</button><p>Give me a cup of [coffee]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Jack Du WeChat Pay"><p>WeChat Pay</p></div><div><img data-src="/images/alipay.png" alt="Jack Du Alipay"><p>Alipay</p></div><div><img data-src="/images/paypal.png" alt="Jack Du PayPal"><p>PayPal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>Jack Du <i class="ic i-at"><em>@</em></i>Enjun Du</li><li class="link"><strong>Post link: </strong><a href="https://enjundu.github.io/bit/%E4%BA%9A%E5%A4%AA%E6%95%B0%E6%A8%A1%E2%80%94%E2%80%94%E6%B4%AA%E6%B0%B4%E9%A2%84%E6%B5%8B/" title="亚太数模——洪水">https://enjundu.github.io/bit/亚太数模——洪水预测/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/shudian/%E5%AE%9E%E9%AA%8C%E4%BA%8C%E6%8A%A5%E5%91%8A/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;07&#x2F;04&#x2F;ArOLs3qhENaXcUV.jpg" title="复杂组合逻辑模块设计"><span class="type">Previous Post</span> <span class="category"><i class="ic i-flag"></i> 数字逻辑实验</span><h3>复杂组合逻辑模块设计</h3></a></div><div class="item right"><a href="/bit/%E4%BA%9A%E5%A4%AA%E6%95%B0%E6%A8%A1%E2%80%94%E2%80%94%E6%96%B0%E8%83%BD%E6%BA%90%E6%B1%BD%E8%BD%A6/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2024&#x2F;07&#x2F;04&#x2F;OLqrN6bKDJHW5ek.jpg" title="亚太数模——新能源汽车"><span class="type">Next Post</span> <span class="category"><i class="ic i-flag"></i> BIT-study</span><h3>亚太数模——新能源汽车</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">1.</span> <span class="toc-text">代码解析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#before-problem-%E6%95%B0%E6%8D%AE%E4%BC%A0%E5%85%A5%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86%E5%85%A8%E5%B1%80%E5%87%BD%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.1.</span> <span class="toc-text">Before Problem -- 数据传入与预处理，全局函数设置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E5%BF%85%E8%A6%81%E7%9A%84%E5%BA%93"><span class="toc-number">1.1.1.</span> <span class="toc-text">导入必要的库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%85%A5"><span class="toc-number">1.1.2.</span> <span class="toc-text">数据读入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8Etraincsv%E8%AF%BB%E5%85%A5train%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">从 train.csv 读入 train 数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.1.3.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E6%80%A7%E7%AE%80%E8%A6%81%E5%88%86%E6%9E%90"><span class="toc-number">1.1.4.</span> <span class="toc-text">数据相关性简要分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E4%B8%89%E4%B8%AA%E6%B0%B4%E5%B9%B3%E7%9A%84%E6%8C%87%E6%A0%87%E9%9B%86"><span class="toc-number">1.1.5.</span> <span class="toc-text">建立三个水平的指标集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E8%BF%87%E6%BB%A4%E6%B3%95%E8%8E%B7%E5%8F%96%E6%8C%87%E6%A0%87%E9%9B%86"><span class="toc-number">1.1.5.1.</span> <span class="toc-text">利用过滤法获取指标集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E5%87%BD%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.1.6.</span> <span class="toc-text">全局函数设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%92%E5%88%86%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%8E%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">1.1.7.</span> <span class="toc-text">划分训练集与测试集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#problem-one-%E5%85%A8%E6%8C%87%E6%A0%87%E9%9B%86%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="toc-number">1.2.</span> <span class="toc-text">Problem ONE -- 全指标集数学模型建立与求解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%80-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.1.</span> <span class="toc-text">模型一 线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BA%8C-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91"><span class="toc-number">1.2.2.</span> <span class="toc-text">模型二 梯度提升树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%89-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.2.3.</span> <span class="toc-text">模型三 决策树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%9B%9B-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.2.4.</span> <span class="toc-text">模型四 随机森林</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#problem-two-%E9%A3%8E%E9%99%A9%E8%81%9A%E7%B1%BB"><span class="toc-number">1.3.</span> <span class="toc-text">Problem TWO 风险聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.1.</span> <span class="toc-text">读入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95kmeans"><span class="toc-number">1.3.2.</span> <span class="toc-text">聚类算法：Kmeans</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E9%A2%84%E6%B5%8B%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">聚类预测函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8Bkmeans%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">建立 Kmeans 模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E8%81%9A%E7%B1%BB%E4%B8%AD%E5%BF%83%E5%88%B0%E9%A3%8E%E9%99%A9%E7%9A%84%E6%98%A0%E5%B0%84%E5%AD%97%E5%85%B8"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">建立聚类中心到风险的映射字典</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E9%A3%8E%E9%99%A9%E9%A2%84%E8%AD%A6%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.3.</span> <span class="toc-text">基于风险预警的预测模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B-mymodel"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">模型建立 MyModel</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%85%A5-2"><span class="toc-number">1.3.4.</span> <span class="toc-text">数据读入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E5%85%88%E5%89%8D%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">利用先前模型构建自定义的分类模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">逻辑回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%81%B5%E6%95%8F%E5%BA%A6%E5%88%86%E6%9E%90"><span class="toc-number">1.3.5.</span> <span class="toc-text">模型灵敏度分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E7%86%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0-calculate_entropy"><span class="toc-number">1.3.5.1.</span> <span class="toc-text">设置熵的计算函数 calculate_entropy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E5%9F%BA%E4%BA%8E%E5%B1%9E%E6%80%A7%E5%8F%98%E5%8C%96%E7%8E%87%E7%9A%84%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0-average_change_rate"><span class="toc-number">1.3.5.2.</span> <span class="toc-text">设置基于属性变化率的计算函数 average_change_rate</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E5%9F%BA%E4%BA%8E%E5%B1%9E%E6%80%A7%E5%AF%B9%E6%A6%82%E7%8E%87%E5%8F%98%E5%8C%96%E7%8E%87%E7%9A%84%E7%81%B5%E6%95%8F%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">1.3.5.3.</span> <span class="toc-text">启动基于属性对概率变化率的灵敏度计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E5%9F%BA%E4%BA%8E%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%9A%84%E7%86%B5%E5%80%BC%E7%9A%84%E7%81%B5%E6%95%8F%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">1.3.5.4.</span> <span class="toc-text">启动基于预测结果的熵值的灵敏度计算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#problem-three-%E4%BB%8E%E4%B8%8D%E5%90%8C%E6%8C%87%E6%A0%87%E9%9B%86%E5%BB%BA%E7%AB%8B%E6%B4%AA%E6%B0%B4%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">Problem THREE 从不同指标集建立洪水的预测模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E5%8D%8A%E6%8C%87%E6%A0%87%E9%9B%86%E9%87%8D%E6%96%B0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">利用半指标集重新训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%85%A5%E4%B8%8E%E5%88%92%E5%88%86"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">数据读入与划分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">多元线性回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">梯度提升树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.4.1.5.</span> <span class="toc-text">随机森林</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E6%A0%B8%E5%BF%83%E6%8C%87%E6%A0%87%E9%9B%86%E9%87%8D%E6%96%B0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.2.</span> <span class="toc-text">利用核心指标集重新训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%85%A5-3"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">数据读入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-2"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">多元线性回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91-2"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">梯度提升树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91-2"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-2"><span class="toc-number">1.4.2.5.</span> <span class="toc-text">随机森林</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#problem-four-%E9%A2%84%E6%B5%8B%E6%B4%AA%E6%B0%B4%E5%8F%91%E7%94%9F%E7%9A%84%E6%A6%82%E7%8E%87"><span class="toc-number">1.5.</span> <span class="toc-text">Problem FOUR 预测洪水发生的概率</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">训练集成模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA"><span class="toc-number">1.5.0.2.</span> <span class="toc-text">集成模型输出</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E7%BB%98%E5%88%B6%E5%9B%BE%E5%BD%A2%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%92%8Cqq%E5%9B%BE-%E7%BB%9F%E8%AE%A1%E6%A3%80%E9%AA%8Cshapiro-wilk-%E6%A3%80%E9%AA%8C-kolmogorov-smirnov-%E6%A3%80%E9%AA%8C-anderson-darling-%E6%A3%80%E9%AA%8C-dagostinos-k2-%E6%A3%80%E9%AA%8C-%E6%8F%8F%E8%BF%B0%E6%80%A7%E7%BB%9F%E8%AE%A1%E5%81%8F%E5%BA%A6skewness%E5%92%8C%E5%B3%B0%E5%BA%A6kurtosis"><span class="toc-number">1.5.0.3.</span> <span class="toc-text">通过绘制图形（直方图和 QQ 图）、统计检验（Shapiro-Wilk 检验、Kolmogorov-Smirnov 检验、Anderson-Darling 检验、D&#39;Agostino&#39;s K^2 检验）、描述性统计（偏度（Skewness）和峰度（Kurtosis））</span></a></li></ol></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B4%AA%E6%B0%B4%E7%81%BE%E5%AE%B3%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E9%A2%84%E6%B5%8B%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6"><span class="toc-number">2.</span> <span class="toc-text">洪水灾害的数据分析与预测：基于多模型集成学习的方法研究</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-number">2.2.</span> <span class="toc-text">目录</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#-2"><span class="toc-number">4.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF%E4%B8%8E%E9%97%AE%E9%A2%98%E9%87%8D%E8%BF%B0"><span class="toc-number">5.</span> <span class="toc-text">* 一、问题背景与问题重述 *</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF"><span class="toc-number">5.1.</span> <span class="toc-text">*1.1 问题背景 *</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%B7%B2%E7%9F%A5%E6%9D%A1%E4%BB%B6"><span class="toc-number">5.2.</span> <span class="toc-text">*1.2 已知条件 *</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E9%97%AE%E9%A2%98%E9%87%8D%E8%BF%B0"><span class="toc-number">5.3.</span> <span class="toc-text">*1.3 问题重述 *</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-%E6%A8%A1%E5%9E%8B%E5%81%87%E8%AE%BE"><span class="toc-number">6.</span> <span class="toc-text">二、 * 模型假设 *</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-%E7%AC%A6%E5%8F%B7%E8%AF%B4%E6%98%8E"><span class="toc-number">7.</span> <span class="toc-text">三、 * 符号说明 *</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B-%E7%A0%94%E7%A9%B6%E6%80%9D%E8%B7%AF"><span class="toc-number">8.</span> <span class="toc-text">四、 * 研究思路 *</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94-%E9%97%AE%E9%A2%98%E4%B8%80%E7%9A%84%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="toc-number">9.</span> <span class="toc-text">五、 * 问题一的建模与求解 *</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#51-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">9.1.</span> <span class="toc-text">*5.1 数据预处理 *</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#52-%E9%97%AE%E9%A2%98%E4%B8%80%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="toc-number">9.2.</span> <span class="toc-text">*5.2 问题一的模型建立与求解 *</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#521%E7%94%A8%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E5%92%8C%E6%96%AF%E7%9A%AE%E5%B0%94%E6%9B%BC%E7%AD%89%E7%BA%A7%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E7%A0%94%E7%A9%B6%E5%90%84%E6%8C%87%E6%A0%87%E4%B8%8E%E6%B4%AA%E6%B0%B4%E5%8F%91%E7%94%9F%E6%A6%82%E7%8E%87%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="toc-number">9.2.1.</span> <span class="toc-text">*5.2.**1 用皮尔逊相关系数和斯皮尔曼等级相关系数研究各指标与洪水发生概率的相关性。*</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#522%E9%80%9A%E8%BF%87%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E9%80%92%E5%BD%92%E6%8F%90%E5%8D%87%E6%A0%91-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%AF%B9%E6%8C%87%E6%A0%87%E7%9A%84%E4%BC%98%E5%8A%A3%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90"><span class="toc-number">9.2.2.</span> <span class="toc-text">*5.2.**2 通过多元线性回归、递归提升树、随机森林对指标的优劣进行分析。*</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5221-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.2.2.1.</span> <span class="toc-text">*5.2.**2.1 线性回归模型 *</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5222-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.2.2.2.</span> <span class="toc-text">*5.2.**2.2 梯度提升树模型 *</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5223-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.2.2.3.</span> <span class="toc-text">*5.2.**2.3 随机森林模型 *</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#523-%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">9.2.3.</span> <span class="toc-text">*5.2.3* * 结果分析 *</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#53%E5%BB%BA%E8%AE%AE%E4%B8%8E%E6%8E%AA%E6%96%BD"><span class="toc-number">9.3.</span> <span class="toc-text">*5.3** 建议与措施 *</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD-%E9%97%AE%E9%A2%98%E4%BA%8C%E7%9A%84%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="toc-number">10.</span> <span class="toc-text">* 六、问题二的建模与求解 *</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#61-%E9%97%AE%E9%A2%982%E7%9A%84%E5%88%86%E6%9E%90"><span class="toc-number">10.1.</span> <span class="toc-text">*6.1* * 问题 2 的分析 *</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#611-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">10.1.1.</span> <span class="toc-text">*6.1.1* * 数据预处理 *</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#612-%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E9%80%89%E6%8B%A9"><span class="toc-number">10.1.2.</span> <span class="toc-text">*6.1.2* * 聚类方法选择 *</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#62-k-means%E5%BB%BA%E6%A8%A1%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%88%86%E6%9E%90"><span class="toc-number">10.2.</span> <span class="toc-text">*6.2* *k-means 建模实现与分析 *</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#621-%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B"><span class="toc-number">10.2.1.</span> <span class="toc-text">*6.2.1* * 模型建立 *</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#622-%E9%AB%98%E4%B8%AD%E4%BD%8E%E9%A3%8E%E9%99%A9%E7%89%B9%E5%BE%81%E5%88%86%E6%9E%90"><span class="toc-number">10.2.2.</span> <span class="toc-text">*6.2.2* * 高中低风险特征分析 *</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#623-%E5%BB%BA%E7%AB%8B%E5%8F%91%E7%94%9F%E6%B4%AA%E6%B0%B4%E4%B8%8D%E5%90%8C%E9%A3%8E%E9%99%A9%E9%A2%84%E8%AD%A6%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.2.3.</span> <span class="toc-text">*6.2.**3* * 建立发生洪水不同风险预警模型 *</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#63-%E6%A8%A1%E5%9E%8B%E7%81%B5%E6%95%8F%E5%BA%A6%E5%88%86%E6%9E%90"><span class="toc-number">10.3.</span> <span class="toc-text">*6.3* * 模型灵敏度分析 *</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#631-%E5%9F%BA%E4%BA%8E%E7%89%B9%E5%BE%81%E5%8F%98%E5%8C%96%E7%8E%87%E7%9A%84%E7%81%B5%E6%95%8F%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">10.3.1.</span> <span class="toc-text">*6.3.1* * 基于特征变化率的灵敏度计算 *</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#632-%E5%9F%BA%E4%BA%8E%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%86%B5%E5%80%BC%E7%9A%84%E7%81%B5%E6%95%8F%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">10.3.2.</span> <span class="toc-text">*6.3.2* * 基于预测结果熵值的灵敏度计算 *</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%83-%E9%97%AE%E9%A2%98%E4%B8%89%E7%9A%84%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="toc-number">10.3.3.</span> <span class="toc-text">七、* 问题三的建模与求解 *</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#712-%E5%9F%BA%E4%BA%8E%E8%BF%87%E6%BB%A4%E6%B3%95%E9%80%89%E5%8F%96%E5%85%B3%E9%94%AE%E6%8C%87%E6%A0%87"><span class="toc-number">10.3.4.</span> <span class="toc-text">*7.1.2* * 基于过滤法选取关键指标 *</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#72-%E5%90%84%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">10.4.</span> <span class="toc-text">*7.2 各模型 ** 结果分析 *</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#73-%E6%9C%80%E7%BB%88%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B"><span class="toc-number">10.5.</span> <span class="toc-text">*7.3 最终模型建立 *</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB-%E9%97%AE%E9%A2%98%E5%9B%9B%E7%9A%84%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="toc-number">11.</span> <span class="toc-text">* 八、问题四的建模与求解 *</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#81-%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B"><span class="toc-number">11.1.</span> <span class="toc-text">*8.1 结果预测 *</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#82-%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">11.2.</span> <span class="toc-text">*8.2* * 结果可视化 *</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#832-shapiro-wilk%E6%A3%80%E9%AA%8C"><span class="toc-number">11.2.1.</span> <span class="toc-text">*8.3.**2* *Shapiro-Wilk 检验 *</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#833-kolmogorov-smirnov%E6%B5%8B%E8%AF%95"><span class="toc-number">11.2.2.</span> <span class="toc-text">*8.3.**3* *Kolmogorov-Smirnov 测试 *</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#834-anderson-darling%E6%A3%80%E9%AA%8C"><span class="toc-number">11.2.3.</span> <span class="toc-text">*8.3.**4* *Anderson-Darling 检验 *</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D-%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90"><span class="toc-number">12.</span> <span class="toc-text">* 九、** 模型分析 *</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#91-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">12.1.</span> <span class="toc-text">*9.1* * 模型的优点 *</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#92-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="toc-number">12.2.</span> <span class="toc-text">*9.2* * 模型的缺点 *</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#93-%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E5%B9%BF"><span class="toc-number">12.3.</span> <span class="toc-text">*9.3* * 模型的推广 *</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81-%E6%80%BB%E7%BB%93"><span class="toc-number">13.</span> <span class="toc-text">* 十、** 总结 *</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%B8%80-%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">14.</span> <span class="toc-text">* 十一、** 参考文献 *</span></a></li></div><div class="related panel pjax" data-title="Related"><ul><li><a href="/bit/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AE%9E%E9%AA%8C/" rel="bookmark" title="数据结构实验">数据结构实验</a></li><li><a href="/bit/%E7%BE%8E%E8%B5%9B%E2%80%94%E2%80%94%E4%B8%83%E9%B3%83%E9%B3%97/" rel="bookmark" title="美赛——七鳃鳗">美赛——七鳃鳗</a></li><li><a href="/bit/%E4%BA%9A%E5%A4%AA%E6%95%B0%E6%A8%A1%E2%80%94%E2%80%94%E6%96%B0%E8%83%BD%E6%BA%90%E6%B1%BD%E8%BD%A6/" rel="bookmark" title="亚太数模——新能源汽车">亚太数模——新能源汽车</a></li><li class="active"><a href="/bit/%E4%BA%9A%E5%A4%AA%E6%95%B0%E6%A8%A1%E2%80%94%E2%80%94%E6%B4%AA%E6%B0%B4%E9%A2%84%E6%B5%8B/" rel="bookmark" title="亚太数模——洪水">亚太数模——洪水</a></li><li><a href="/bit/%E5%AF%86%E7%A0%81%E5%B7%A5%E7%A8%8B/" rel="bookmark" title="密码工程大作业">密码工程大作业</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Jack Du" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Jack Du</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">83</span> <span class="name">posts</span></a></div><div class="item categories"><a href="/categories/"><span class="count">12</span> <span class="name">categories</span></a></div><div class="item tags"><a href="/tags/"><span class="count">13</span> <span class="name">tags</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0VuanVuRHU=" title="https:&#x2F;&#x2F;github.com&#x2F;EnjunDu"><i class="ic i-github"></i></span> <a href="/enjundu.cs@gmail.com" title="enjundu.cs@gmail.com" class="item email"><i class="ic i-envelope"></i></a></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>Home</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/shudian/%E5%AE%9E%E9%AA%8C%E4%BA%8C%E6%8A%A5%E5%91%8A/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/bit/%E4%BA%9A%E5%A4%AA%E6%95%B0%E6%A8%A1%E2%80%94%E2%80%94%E6%96%B0%E8%83%BD%E6%BA%90%E6%B1%BD%E8%BD%A6/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>Random Posts</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/ECEBC/" title="In 嵩天的爱-ECE-BC">嵩天的爱-ECE-BC</a></div><span><a href="/ECEBC/redis-dict.c%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/" title="DICT.C源码阅读报告">DICT.C源码阅读报告</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/IntroductiontoNetSecurityLab/" title="In 网安导论实验">网安导论实验</a></div><span><a href="/IntroductiontoNetSecurityLab/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E5%AE%89%E5%85%A8_%E5%90%8E%E9%97%A8%E9%98%B2%E5%BE%A1_%E9%80%89%E5%81%9A/" title="人工智能算法安全_后门防御_选座">人工智能算法安全_后门防御_选座</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEA/" title="In 单片机小学期ECEA">单片机小学期ECEA</a></div><span><a href="/ECEA/ReadMe.zh/" title="001_Readme Me">001_Readme Me</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/HKUST-RA/" title="In HKUST_RA">HKUST_RA</a></div><span><a href="/HKUST-RA/example/" title="example">example</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/deep-learning/" title="In deep-learning">deep-learning</a></div><span><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="深度学习_1">深度学习_1</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEA/" title="In 单片机小学期ECEA">单片机小学期ECEA</a></div><span><a href="/ECEA/1%E7%82%B9%E4%BA%AE%E7%81%AF/" title="LED lighting program">LED lighting program</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/deep-learning/" title="In deep-learning">deep-learning</a></div><span><a href="/deep-learning/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_8_%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="深度学习_8_图神经网络">深度学习_8_图神经网络</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEA/" title="In 单片机小学期ECEA">单片机小学期ECEA</a></div><span><a href="/ECEA/4%E4%BA%A7%E7%94%9F%E4%B8%83%E8%89%B2%E7%81%AF/" title="Generate colorful lights">Generate colorful lights</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/LLMstestoracle/" title="In LLM的test oracle生成">LLM的test oracle生成</a></div><span><a href="/LLMstestoracle/%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C/" title="a_统计结果">a_统计结果</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ECEBC/" title="In 嵩天的爱-ECE-BC">嵩天的爱-ECE-BC</a></div><span><a href="/ECEBC/Request%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/" title="Request阅读报告">Request阅读报告</a></span></li></ul></div><div><h2>Recent Comments</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Jack Du @ 蓝天の网站</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="Symbols count total">1.2m words</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="Reading time total">17:39</span></div><div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"bit/亚太数模——洪水预测/",favicon:{show:"（●´3｀●）Goooood",hide:"(´Д｀)Booooom"},search:{placeholder:"Search for Posts",empty:"We didn't find any results for the search: ${query}",stats:"${hits} results found in ${time} ms"},valine:!0,fancybox:!0,copyright:'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->